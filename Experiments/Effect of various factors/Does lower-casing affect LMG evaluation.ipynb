{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "b1bf495c",
   "metadata": {},
   "outputs": [],
   "source": [
    "import csv\n",
    "import json\n",
    "\n",
    "preds=[]\n",
    "refs=[]\n",
    "auth_1 =[]\n",
    "auth_2 = []\n",
    "auth_3 =[]\n",
    "with open('human_annotations.csv') as csvfile:\n",
    "    ader = csv.reader(csvfile)\n",
    "    for row in ader:\n",
    "        refs.append(row[1])\n",
    "        preds.append(row[0])\n",
    "        auth_1.append(row[2])\n",
    "        auth_2.append(row[3])\n",
    "        auth_3.append(row[4])\n",
    "        \n",
    "refs = refs[:100]\n",
    "preds = preds[:100]\n",
    "\n",
    "auth_1 = auth_1[:100]\n",
    "for i in range(0, len(auth_1)):\n",
    "    auth_1[i] = int(auth_1[i])\n",
    "#print(auth_1)\n",
    "\n",
    "auth_2 = auth_2[:100]\n",
    "for i in range(0, len(auth_2)):\n",
    "    auth_2[i] = int(auth_2[i])\n",
    "#print(auth_2)\n",
    "\n",
    "auth_3 = auth_3[:100]\n",
    "for i in range(0, len(auth_3)):\n",
    "    auth_3[i] = int(auth_3[i])\n",
    "#print(auth_3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "88afe9d2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['noting', 'polish', 'add known user', ', refactor jdbcxabackenddatasourcefactory', 'kryo updated', 'update changes', 'update issue_template', 'change name select_order_by_with_table_star_table_name for parser', 'update changes', 'remove unused property']\n"
     ]
    }
   ],
   "source": [
    "#reference in lower case\n",
    "refs_lower = []\n",
    "for ref in refs:\n",
    "    refs_lower.append(ref.lower())\n",
    "print(refs_lower[:10])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "4e4a8574",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['noting', 'polish', 'add known user', ', refactor jdbcxabackenddatasourcefactory', 'kryo updated', 'update changes', 'update issue_template', 'add select_order_by_with_table_star_table_name', 'updated changes', 'remove unused logger']\n"
     ]
    }
   ],
   "source": [
    "#predicted in lower case\n",
    "preds_lower = []\n",
    "for pred in preds:\n",
    "    preds_lower.append(pred.lower())\n",
    "print(preds_lower[:10])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "b90a3820",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.5, 1.0, 0.75, 0.75, 0.75, 0.75, 1.0, 1.0, 0.5, 0.5, 0.75, 0.75, 0.5, 0.5, 0.5, 0.75, 0.25, 0.75, 0.75, 0.5, 0.5, 0.75, 0.75, 0.75, 0.75, 0.25, 0.75, 1.0, 0.25, 1.0, 0.75, 0.75, 0.75, 0.25, 0.75, 1.0, 0.75, 0.5, 0.75, 0.25, 0.0, 0.75, 0.5, 0.25, 1.0, 0.25, 0.5, 0.5, 0.0, 0.5, 0.5, 0.25, 0.5, 0.25, 0.25, 0.25, 0.25, 0.25, 0.5, 0.5, 0.5, 0.5, 0.5, 0.5, 0.25, 0.25, 0.5, 0.75, 0.5, 0.5, 0.25, 0.25, 0.75, 0.5, 0.5, 0.75, 0.5, 0.25, 0.25, 0.25, 0.25, 0.25, 0.5, 0.5, 0.5, 0.75, 0.25, 0.5, 0.75, 0.5, 0.0, 0.0, 0.25]\n"
     ]
    }
   ],
   "source": [
    "#normalised author 1 scores\n",
    "norm_auth_1 =[]\n",
    "max1 = max(auth_1)\n",
    "min1 = min(auth_1)\n",
    "\n",
    "for a in range(len(auth_1)):\n",
    "    norm_auth_1.append((auth_1[a])/(max1))\n",
    "print(norm_auth_1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "deda4dc3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.25, 1.0, 0.75, 0.75, 0.75, 0.75, 1.0, 1.0, 0.25, 0.5, 0.5, 0.5, 0.25, 0.5, 0.5, 0.5, 0.0, 0.75, 0.75, 0.25, 0.25, 0.75, 0.5, 0.75, 0.5, 0.25, 0.75, 1.0, 0.0, 0.75, 0.75, 0.75, 0.75, 0.25, 0.5, 1.0, 0.5, 0.5, 0.5, 0.25, 0.0, 0.5, 0.5, 0.25, 1.0, 0.25, 0.25, 0.25, 0.0, 0.5, 0.25, 0.5, 0.25, 0.25, 0.25, 0.25, 0.25, 0.25, 0.25, 0.25, 0.5, 0.5, 0.25, 0.25, 0.25, 0.25, 0.25, 0.75, 0.25, 0.5, 0.25, 0.25, 0.5, 0.5, 0.25, 0.5, 0.5, 0.25, 0.25, 0.25, 0.25, 0.25, 0.25, 0.25, 0.5, 0.5, 0.25, 0.25, 0.5, 0.25, 0.0, 0.0, 0.5]\n"
     ]
    }
   ],
   "source": [
    "#normalised author 2 scores\n",
    "norm_auth_2 =[]\n",
    "max1 = max(auth_2)\n",
    "min1 = min(auth_2)\n",
    "\n",
    "for a in range(len(auth_2)):\n",
    "    norm_auth_2.append((auth_2[a])/(max1))\n",
    "print(norm_auth_2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "1faac0a5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.5, 1.0, 0.75, 0.75, 0.75, 0.75, 1.0, 1.0, 0.5, 0.5, 0.75, 0.75, 0.5, 0.75, 0.5, 0.75, 0.25, 0.75, 0.75, 0.25, 0.5, 0.75, 0.75, 0.75, 0.75, 0.25, 0.75, 1.0, 0.0, 1.0, 0.75, 1.0, 0.75, 0.25, 0.75, 1.0, 0.75, 0.5, 0.75, 0.5, 0.0, 0.75, 0.5, 0.25, 1.0, 0.25, 0.5, 0.5, 0.0, 0.75, 0.5, 0.5, 0.5, 0.25, 0.25, 0.5, 0.5, 0.25, 0.5, 0.25, 0.5, 0.5, 0.5, 0.25, 0.25, 0.25, 0.5, 0.75, 0.5, 0.75, 0.25, 0.25, 0.75, 0.75, 0.5, 0.75, 0.5, 0.25, 0.25, 0.25, 0.25, 0.25, 0.25, 0.5, 0.5, 0.5, 0.25, 0.5, 0.75, 0.5, 0.0, 0.0, 0.25]\n"
     ]
    }
   ],
   "source": [
    "#normalised author 3 scores\n",
    "norm_auth_3 =[]\n",
    "max1 = max(auth_3)\n",
    "min1 = min(auth_3)\n",
    "\n",
    "for a in range(len(auth_3)):\n",
    "    norm_auth_3.append((auth_3[a])/(max1))\n",
    "print(norm_auth_3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "878d56a3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.42, 1.0, 0.75, 0.75, 0.75, 0.75, 1.0, 1.0, 0.42, 0.5, 0.67, 0.67, 0.42, 0.58, 0.5, 0.67, 0.17, 0.75, 0.75, 0.33, 0.42, 0.75, 0.67, 0.75, 0.67, 0.25, 0.75, 1.0, 0.08, 0.92, 0.75, 0.83, 0.75, 0.25, 0.67, 1.0, 0.67, 0.5, 0.67, 0.33, 0.0, 0.67, 0.5, 0.25, 1.0, 0.25, 0.42, 0.42, 0.0, 0.58, 0.42, 0.42, 0.42, 0.25, 0.25, 0.33, 0.33, 0.25, 0.42, 0.33, 0.5, 0.5, 0.42, 0.33, 0.25, 0.25, 0.42, 0.75, 0.42, 0.58, 0.25, 0.25, 0.67, 0.58, 0.42, 0.67, 0.5, 0.25, 0.25, 0.25, 0.25, 0.25, 0.33, 0.42, 0.5, 0.58, 0.25, 0.42, 0.67, 0.42, 0.0, 0.0, 0.33]\n"
     ]
    }
   ],
   "source": [
    "#Average normalised author scores\n",
    "avg_norm_score = []\n",
    "for i in range(len(auth_1)):\n",
    "    avg_norm_score.append(round((norm_auth_1[i]+norm_auth_2[i]+norm_auth_3[i])/3,2))\n",
    "print(avg_norm_score)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "d1f210ee",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from nltk.translate.bleu_score import sentence_bleu\n",
    "from nltk.translate.bleu_score import SmoothingFunction\n",
    "from nltk.translate.meteor_score import meteor_score"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "13cb9126",
   "metadata": {},
   "source": [
    "# BLEU4 with lower case"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "9726fcc5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.62, 0.8, 0.65, 0.71, 0.69, 0.62, 0.61, 0.63, 0.35, 0.24, 0.5, 0.5, 0.39, 0.13, 0.52, 0.41, 0.28, 0.7, 0.49, 0.16, 0.04, 0.32, 0.54, 0.37, 0.36, 0.48, 0.27, 0.71, 0.48, 0.66, 0.54, 0.52, 0.55, 0.26, 0.37, 1.0, 0.15, 0.45, 0.68, 0.52, 0.0, 0.52, 0.37, 0.43, 0.92, 0.14, 0.12, 0.27, 0.17, 0.27, 0.21, 0.39, 0.49, 0.31, 0.27, 0.48, 0.31, 0.11, 0.31, 0.13, 0.4, 0.32, 0.22, 0.1, 0.35, 0.14, 0.16, 0.52, 0.13, 0.49, 0.33, 0.26, 0.23, 0.44, 0.13, 0.54, 0.54, 0.28, 0.25, 0.32, 0.35, 0.26, 0.21, 0.27, 0.33, 0.24, 0.27, 0.17, 0.44, 0.18, 0.0, 0.0, 0.24]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\samri\\anaconda3\\lib\\site-packages\\nltk\\translate\\bleu_score.py:516: UserWarning: \n",
      "The hypothesis contains 0 counts of 3-gram overlaps.\n",
      "Therefore the BLEU score evaluates to 0, independently of\n",
      "how many N-gram overlaps of lower order it contains.\n",
      "Consider using lower n-gram order or use SmoothingFunction()\n",
      "  warnings.warn(_msg)\n",
      "C:\\Users\\samri\\anaconda3\\lib\\site-packages\\nltk\\translate\\bleu_score.py:516: UserWarning: \n",
      "The hypothesis contains 0 counts of 4-gram overlaps.\n",
      "Therefore the BLEU score evaluates to 0, independently of\n",
      "how many N-gram overlaps of lower order it contains.\n",
      "Consider using lower n-gram order or use SmoothingFunction()\n",
      "  warnings.warn(_msg)\n"
     ]
    }
   ],
   "source": [
    "bleu4_l=[]\n",
    "for i in range(len(auth_1)):\n",
    "    bleu4_l.append(round(sentence_bleu([refs_lower[i]], preds_lower[i]),2))\n",
    "print(bleu4_l)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "2c9de0bc",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.62, 0.8, 0.65, 0.71, 0.69, 0.62, 0.61, 0.63, 0.35, 0.24, 0.5, 0.5, 0.39, 0.13, 0.52, 0.41, 0.28, 0.7, 0.49, 0.16, 0.04, 0.32, 0.54, 0.37, 0.36, 0.48, 0.27, 0.71, 0.48, 0.66, 0.54, 0.52, 0.55, 0.26, 0.37, 1.0, 0.15, 0.45, 0.68, 0.52, 0.0, 0.52, 0.37, 0.43, 0.92, 0.14, 0.12, 0.27, 0.17, 0.27, 0.21, 0.39, 0.49, 0.31, 0.27, 0.48, 0.31, 0.11, 0.31, 0.13, 0.4, 0.32, 0.22, 0.1, 0.35, 0.14, 0.16, 0.52, 0.13, 0.49, 0.33, 0.26, 0.23, 0.44, 0.13, 0.54, 0.54, 0.28, 0.25, 0.32, 0.35, 0.26, 0.21, 0.27, 0.33, 0.24, 0.27, 0.17, 0.44, 0.18, 0.0, 0.0, 0.24]\n"
     ]
    }
   ],
   "source": [
    "norm_bleu4_l = []\n",
    "\n",
    "max_b4=max(bleu4_l)\n",
    "min_b4=min(bleu4_l)\n",
    "\n",
    "for a in range(len(bleu4_l)):\n",
    "    norm_bleu4_l.append(round(((bleu4_l[a])/(max_b4)),2))\n",
    "print(norm_bleu4_l)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "2eb80626",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Spearmans correlation coefficient: 0.717\n",
      "correlated (reject H0) p=0.000\n"
     ]
    }
   ],
   "source": [
    "from scipy.stats import spearmanr\n",
    "# calculate spearman's correlation\n",
    "coef, p = spearmanr(norm_bleu4_l, avg_norm_score)\n",
    "print('Spearmans correlation coefficient: %.3f' % coef)\n",
    "# interpret the significance\n",
    "alpha = 0.05\n",
    "if p > alpha:\n",
    "\tprint('uncorrelated (fail to reject H0) p=%.3f' % p)\n",
    "else:\n",
    "\tprint('correlated (reject H0) p=%.3f' % p)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "00eb5c51",
   "metadata": {},
   "source": [
    "# BLEU4 without lower case "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "f3854744",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.62, 0.72, 0.61, 0.71, 0.69, 0.56, 0.61, 0.63, 0.31, 0.24, 0.5, 0.5, 0.39, 0.13, 0.34, 0.29, 0.26, 0.7, 0.49, 0.15, 0.04, 0.32, 0.54, 0.34, 0.32, 0.43, 0.27, 0.71, 0.47, 0.61, 0.49, 0.52, 0.55, 0.26, 0.37, 0.92, 0.08, 0.42, 0.57, 0.52, 0.0, 0.52, 0.36, 0.42, 0.79, 0.13, 0.1, 0.13, 0.17, 0.27, 0.21, 0.39, 0.18, 0.29, 0.26, 0.38, 0.31, 0.11, 0.31, 0.13, 0.35, 0.32, 0.22, 0.1, 0.35, 0.13, 0.16, 0.44, 0.0, 0.46, 0.27, 0.25, 0.23, 0.44, 0.13, 0.49, 0.53, 0.28, 0.25, 0.3, 0.35, 0.26, 0.21, 0.16, 0.32, 0.24, 0.27, 0.17, 0.44, 0.18, 0.0, 0.0, 0.24]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\samri\\anaconda3\\lib\\site-packages\\nltk\\translate\\bleu_score.py:516: UserWarning: \n",
      "The hypothesis contains 0 counts of 2-gram overlaps.\n",
      "Therefore the BLEU score evaluates to 0, independently of\n",
      "how many N-gram overlaps of lower order it contains.\n",
      "Consider using lower n-gram order or use SmoothingFunction()\n",
      "  warnings.warn(_msg)\n"
     ]
    }
   ],
   "source": [
    "bleu4=[]\n",
    "for i in range(len(auth_1)):\n",
    "    bleu4.append(round(sentence_bleu([refs[i]], preds[i]),2))\n",
    "print(bleu4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "2b0d3841",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.62, 0.72, 0.61, 0.71, 0.69, 0.56, 0.61, 0.63, 0.31, 0.24, 0.5, 0.5, 0.39, 0.13, 0.34, 0.29, 0.26, 0.7, 0.49, 0.15, 0.04, 0.32, 0.54, 0.34, 0.32, 0.43, 0.27, 0.71, 0.47, 0.61, 0.49, 0.52, 0.55, 0.26, 0.37, 0.92, 0.08, 0.42, 0.57, 0.52, 0.0, 0.52, 0.36, 0.42, 0.79, 0.13, 0.1, 0.13, 0.17, 0.27, 0.21, 0.39, 0.18, 0.29, 0.26, 0.38, 0.31, 0.11, 0.31, 0.13, 0.35, 0.32, 0.22, 0.1, 0.35, 0.13, 0.16, 0.44, 0.0, 0.46, 0.27, 0.25, 0.23, 0.44, 0.13, 0.49, 0.53, 0.28, 0.25, 0.3, 0.35, 0.26, 0.21, 0.16, 0.32, 0.24, 0.27, 0.17, 0.44, 0.18, 0.0, 0.0, 0.24]\n"
     ]
    }
   ],
   "source": [
    "norm_bleu4 = []\n",
    "\n",
    "max_b4=max(bleu4)\n",
    "min_b4=min(bleu4)\n",
    "\n",
    "for a in range(len(bleu4)):\n",
    "    norm_bleu4.append(round(((bleu4[a])/(max_b4)),2))\n",
    "print(norm_bleu4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "19cccbf7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Spearmans correlation coefficient: 0.705\n",
      "correlated (reject H0) p=0.000\n"
     ]
    }
   ],
   "source": [
    "from scipy.stats import spearmanr\n",
    "# calculate spearman's correlation\n",
    "coef, p = spearmanr(norm_bleu4, avg_norm_score)\n",
    "print('Spearmans correlation coefficient: %.3f' % coef)\n",
    "# interpret the significance\n",
    "alpha = 0.05\n",
    "if p > alpha:\n",
    "\tprint('uncorrelated (fail to reject H0) p=%.3f' % p)\n",
    "else:\n",
    "\tprint('correlated (reject H0) p=%.3f' % p)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "49fe0e00",
   "metadata": {},
   "source": [
    "# BLEUNorm with lower case"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "601e6cfa",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.62, 0.81, 0.66, 0.72, 0.7, 0.62, 0.61, 0.64, 0.39, 0.24, 0.51, 0.51, 0.42, 0.16, 0.54, 0.43, 0.3, 0.71, 0.51, 0.19, 0.05, 0.32, 0.56, 0.37, 0.36, 0.49, 0.27, 0.73, 0.48, 0.66, 0.55, 0.53, 0.56, 0.29, 0.39, 1.0, 0.15, 0.47, 0.68, 0.53, 0.11, 0.53, 0.38, 0.44, 0.92, 0.15, 0.16, 0.31, 0.21, 0.27, 0.25, 0.39, 0.5, 0.33, 0.28, 0.49, 0.31, 0.14, 0.33, 0.17, 0.41, 0.33, 0.24, 0.14, 0.38, 0.19, 0.19, 0.53, 0.15, 0.5, 0.34, 0.27, 0.25, 0.45, 0.15, 0.54, 0.55, 0.29, 0.26, 0.33, 0.37, 0.28, 0.23, 0.29, 0.35, 0.25, 0.28, 0.18, 0.46, 0.2, 0.11, 0.05, 0.27]\n"
     ]
    }
   ],
   "source": [
    "bleun_l=[]\n",
    "for i in range(len(auth_1)):\n",
    "    bleun_l.append(round(sentence_bleu([refs_lower[i]], preds_lower[i],smoothing_function=SmoothingFunction().method2),2))\n",
    "print(bleun_l)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "f87a3823",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.62, 0.81, 0.66, 0.72, 0.7, 0.62, 0.61, 0.64, 0.39, 0.24, 0.51, 0.51, 0.42, 0.16, 0.54, 0.43, 0.3, 0.71, 0.51, 0.19, 0.05, 0.32, 0.56, 0.37, 0.36, 0.49, 0.27, 0.73, 0.48, 0.66, 0.55, 0.53, 0.56, 0.29, 0.39, 1.0, 0.15, 0.47, 0.68, 0.53, 0.11, 0.53, 0.38, 0.44, 0.92, 0.15, 0.16, 0.31, 0.21, 0.27, 0.25, 0.39, 0.5, 0.33, 0.28, 0.49, 0.31, 0.14, 0.33, 0.17, 0.41, 0.33, 0.24, 0.14, 0.38, 0.19, 0.19, 0.53, 0.15, 0.5, 0.34, 0.27, 0.25, 0.45, 0.15, 0.54, 0.55, 0.29, 0.26, 0.33, 0.37, 0.28, 0.23, 0.29, 0.35, 0.25, 0.28, 0.18, 0.46, 0.2, 0.11, 0.05, 0.27]\n"
     ]
    }
   ],
   "source": [
    "norm_bleun_l = []\n",
    "\n",
    "max_bn=max(bleun_l)\n",
    "min_bn=min(bleun_l)\n",
    "\n",
    "for a in range(len(bleun_l)):\n",
    "    norm_bleun_l.append(round(((bleun_l[a])/(max_bn)),2))\n",
    "print(norm_bleun_l)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "aca80fe6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Spearmans correlation coefficient: 0.703\n",
      "correlated (reject H0) p=0.000\n"
     ]
    }
   ],
   "source": [
    "from scipy.stats import spearmanr\n",
    "# calculate spearman's correlation\n",
    "coef, p = spearmanr(norm_bleun_l, avg_norm_score)\n",
    "print('Spearmans correlation coefficient: %.3f' % coef)\n",
    "# interpret the significance\n",
    "alpha = 0.05\n",
    "if p > alpha:\n",
    "\tprint('uncorrelated (fail to reject H0) p=%.3f' % p)\n",
    "else:\n",
    "\tprint('correlated (reject H0) p=%.3f' % p)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "73fdfecd",
   "metadata": {},
   "source": [
    "# BLEUNorm without lower case"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "5111ea43",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.62, 0.74, 0.63, 0.72, 0.7, 0.56, 0.61, 0.64, 0.35, 0.24, 0.51, 0.51, 0.42, 0.16, 0.38, 0.32, 0.29, 0.71, 0.51, 0.18, 0.05, 0.32, 0.56, 0.34, 0.32, 0.45, 0.27, 0.73, 0.48, 0.61, 0.5, 0.53, 0.55, 0.29, 0.39, 0.92, 0.09, 0.44, 0.58, 0.53, 0.11, 0.53, 0.38, 0.43, 0.8, 0.13, 0.13, 0.18, 0.21, 0.27, 0.25, 0.39, 0.2, 0.31, 0.28, 0.39, 0.31, 0.14, 0.33, 0.17, 0.36, 0.32, 0.24, 0.14, 0.37, 0.18, 0.19, 0.45, 0.03, 0.47, 0.28, 0.27, 0.25, 0.45, 0.15, 0.5, 0.54, 0.29, 0.26, 0.31, 0.36, 0.28, 0.23, 0.18, 0.33, 0.25, 0.28, 0.18, 0.46, 0.2, 0.1, 0.05, 0.27]\n"
     ]
    }
   ],
   "source": [
    "bleun=[]\n",
    "for i in range(len(auth_1)):\n",
    "    bleun.append(round(sentence_bleu([refs[i]], preds[i],smoothing_function=SmoothingFunction().method2),2))\n",
    "print(bleun)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "0fbf918f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.62, 0.74, 0.63, 0.72, 0.7, 0.56, 0.61, 0.64, 0.35, 0.24, 0.51, 0.51, 0.42, 0.16, 0.38, 0.32, 0.29, 0.71, 0.51, 0.18, 0.05, 0.32, 0.56, 0.34, 0.32, 0.45, 0.27, 0.73, 0.48, 0.61, 0.5, 0.53, 0.55, 0.29, 0.39, 0.92, 0.09, 0.44, 0.58, 0.53, 0.11, 0.53, 0.38, 0.43, 0.8, 0.13, 0.13, 0.18, 0.21, 0.27, 0.25, 0.39, 0.2, 0.31, 0.28, 0.39, 0.31, 0.14, 0.33, 0.17, 0.36, 0.32, 0.24, 0.14, 0.37, 0.18, 0.19, 0.45, 0.03, 0.47, 0.28, 0.27, 0.25, 0.45, 0.15, 0.5, 0.54, 0.29, 0.26, 0.31, 0.36, 0.28, 0.23, 0.18, 0.33, 0.25, 0.28, 0.18, 0.46, 0.2, 0.1, 0.05, 0.27]\n"
     ]
    }
   ],
   "source": [
    "norm_bleun = []\n",
    "\n",
    "max_bn=max(bleun)\n",
    "min_bn=min(bleun)\n",
    "\n",
    "for a in range(len(bleun)):\n",
    "    norm_bleun.append(round(((bleun[a])/(max_bn)),2))\n",
    "print(norm_bleun)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "a24359f1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Spearmans correlation coefficient: 0.691\n",
      "correlated (reject H0) p=0.000\n"
     ]
    }
   ],
   "source": [
    "from scipy.stats import spearmanr\n",
    "# calculate spearman's correlation\n",
    "coef, p = spearmanr(norm_bleun, avg_norm_score)\n",
    "print('Spearmans correlation coefficient: %.3f' % coef)\n",
    "# interpret the significance\n",
    "alpha = 0.05\n",
    "if p > alpha:\n",
    "\tprint('uncorrelated (fail to reject H0) p=%.3f' % p)\n",
    "else:\n",
    "\tprint('correlated (reject H0) p=%.3f' % p)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "43b10c11",
   "metadata": {},
   "source": [
    "# BLEUCC with lower case"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "aa972fff",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[1.12, 1.12, 1.12, 1.12, 1.12, 1.12, 1.12, 0.7, 0.89, 0.75, 0.82, 0.79, 0.69, 0.61, 0.69, 0.43, 0.26, 0.61, 0.61, 0.47, 0.21, 0.61, 0.49, 0.36, 0.79, 0.59, 0.2, 0.05, 0.35, 0.64, 0.41, 0.4, 0.57, 0.3, 0.81, 0.58, 0.74, 0.62, 0.61, 0.62, 0.33, 0.46, 1.12, 0.17, 0.54, 0.76, 0.61, 0.14, 0.63, 0.46, 0.52, 1.03, 0.17, 0.21, 0.36, 0.27, 0.37, 0.29, 0.44, 0.56, 0.41, 0.36, 0.57, 0.34, 0.16, 0.38, 0.22, 0.49, 0.38, 0.31, 0.18, 0.45, 0.22, 0.24, 0.59, 0.18, 0.59, 0.38, 0.32, 0.31, 0.54, 0.18, 0.63, 0.62, 0.36, 0.34, 0.38, 0.45, 0.35, 0.28, 0.35, 0.4, 0.27, 0.31, 0.25, 0.52, 0.27, 0.16, 0.09, 0.33]\n"
     ]
    }
   ],
   "source": [
    "bleucc_l=[]\n",
    "for i in range(len(auth_1)):\n",
    "    bleucc_l.append(round(sentence_bleu([refs_lower[i]], preds_lower[i],smoothing_function=SmoothingFunction().method5),2))\n",
    "print(bleucc_l)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "c161d887",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.62, 0.79, 0.67, 0.73, 0.71, 0.62, 0.54, 0.62, 0.38, 0.23, 0.54, 0.54, 0.42, 0.19, 0.54, 0.44, 0.32, 0.71, 0.53, 0.18, 0.04, 0.31, 0.57, 0.37, 0.36, 0.51, 0.27, 0.72, 0.52, 0.66, 0.55, 0.54, 0.55, 0.29, 0.41, 1.0, 0.15, 0.48, 0.68, 0.54, 0.12, 0.56, 0.41, 0.46, 0.92, 0.15, 0.19, 0.32, 0.24, 0.33, 0.26, 0.39, 0.5, 0.37, 0.32, 0.51, 0.3, 0.14, 0.34, 0.2, 0.44, 0.34, 0.28, 0.16, 0.4, 0.2, 0.21, 0.53, 0.16, 0.53, 0.34, 0.29, 0.28, 0.48, 0.16, 0.56, 0.55, 0.32, 0.3, 0.34, 0.4, 0.31, 0.25, 0.31, 0.36, 0.24, 0.28, 0.22, 0.46, 0.24, 0.14, 0.08, 0.29]\n"
     ]
    }
   ],
   "source": [
    "norm_bleucc_l = []\n",
    "\n",
    "max_bcc=max(bleucc_l)\n",
    "min_bcc=min(bleucc_l)\n",
    "\n",
    "for a in range(len(bleucc_l)):\n",
    "    norm_bleucc_l.append(round(((bleucc_l[a])/(max_bcc)),2))\n",
    "print(norm_bleucc_l)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "14143322",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Spearmans correlation coefficient: 0.691\n",
      "correlated (reject H0) p=0.000\n"
     ]
    }
   ],
   "source": [
    "from scipy.stats import spearmanr\n",
    "# calculate spearman's correlation\n",
    "coef, p = spearmanr(norm_bleucc_l, avg_norm_score)\n",
    "print('Spearmans correlation coefficient: %.3f' % coef)\n",
    "# interpret the significance\n",
    "alpha = 0.05\n",
    "if p > alpha:\n",
    "\tprint('uncorrelated (fail to reject H0) p=%.3f' % p)\n",
    "else:\n",
    "\tprint('correlated (reject H0) p=%.3f' % p)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2940049e",
   "metadata": {},
   "source": [
    "# BLEUCC without lower case"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "099d30a0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[1.12, 1.12, 1.12, 1.12, 1.12, 1.12, 1.12, 0.7, 0.81, 0.71, 0.82, 0.79, 0.63, 0.61, 0.69, 0.39, 0.26, 0.61, 0.61, 0.47, 0.21, 0.41, 0.38, 0.34, 0.79, 0.59, 0.19, 0.05, 0.35, 0.64, 0.38, 0.36, 0.52, 0.3, 0.81, 0.57, 0.68, 0.58, 0.61, 0.62, 0.33, 0.46, 1.03, 0.1, 0.51, 0.65, 0.61, 0.12, 0.62, 0.45, 0.51, 0.89, 0.15, 0.19, 0.21, 0.27, 0.37, 0.29, 0.44, 0.23, 0.38, 0.35, 0.47, 0.34, 0.16, 0.38, 0.22, 0.44, 0.37, 0.31, 0.18, 0.44, 0.2, 0.24, 0.52, 0.05, 0.56, 0.32, 0.32, 0.31, 0.54, 0.18, 0.59, 0.62, 0.36, 0.34, 0.36, 0.45, 0.35, 0.28, 0.23, 0.38, 0.27, 0.31, 0.25, 0.52, 0.27, 0.15, 0.09, 0.33]\n"
     ]
    }
   ],
   "source": [
    "bleucc=[]\n",
    "for i in range(len(auth_1)):\n",
    "    bleucc.append(round(sentence_bleu([refs[i]], preds[i],smoothing_function=SmoothingFunction().method5),2))\n",
    "print(bleucc)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "e4f7ffdb",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.62, 0.72, 0.63, 0.73, 0.71, 0.56, 0.54, 0.62, 0.35, 0.23, 0.54, 0.54, 0.42, 0.19, 0.37, 0.34, 0.3, 0.71, 0.53, 0.17, 0.04, 0.31, 0.57, 0.34, 0.32, 0.46, 0.27, 0.72, 0.51, 0.61, 0.52, 0.54, 0.55, 0.29, 0.41, 0.92, 0.09, 0.46, 0.58, 0.54, 0.11, 0.55, 0.4, 0.46, 0.79, 0.13, 0.17, 0.19, 0.24, 0.33, 0.26, 0.39, 0.21, 0.34, 0.31, 0.42, 0.3, 0.14, 0.34, 0.2, 0.39, 0.33, 0.28, 0.16, 0.39, 0.18, 0.21, 0.46, 0.04, 0.5, 0.29, 0.29, 0.28, 0.48, 0.16, 0.53, 0.55, 0.32, 0.3, 0.32, 0.4, 0.31, 0.25, 0.21, 0.34, 0.24, 0.28, 0.22, 0.46, 0.24, 0.13, 0.08, 0.29]\n"
     ]
    }
   ],
   "source": [
    "norm_bleucc = []\n",
    "\n",
    "max_bcc=max(bleucc)\n",
    "min_bcc=min(bleucc)\n",
    "\n",
    "for a in range(len(bleucc)):\n",
    "    norm_bleucc.append(round(((bleucc[a])/(max_bcc)),2))\n",
    "print(norm_bleucc)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "39552254",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Spearmans correlation coefficient: 0.681\n",
      "correlated (reject H0) p=0.000\n"
     ]
    }
   ],
   "source": [
    "from scipy.stats import spearmanr\n",
    "# calculate spearman's correlation\n",
    "coef, p = spearmanr(norm_bleucc, avg_norm_score)\n",
    "print('Spearmans correlation coefficient: %.3f' % coef)\n",
    "# interpret the significance\n",
    "alpha = 0.05\n",
    "if p > alpha:\n",
    "\tprint('uncorrelated (fail to reject H0) p=%.3f' % p)\n",
    "else:\n",
    "\tprint('correlated (reject H0) p=%.3f' % p)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "33e8f71e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: rouge in c:\\users\\samri\\anaconda3\\lib\\site-packages (1.0.1)\n",
      "Requirement already satisfied: six in c:\\users\\samri\\anaconda3\\lib\\site-packages (from rouge) (1.15.0)\n"
     ]
    }
   ],
   "source": [
    "!pip install rouge\n",
    "from rouge import Rouge \n",
    "rouge = Rouge()\n",
    "\n",
    "def rouge1_scores(reference, hypothesis):\n",
    "    rouge = Rouge()\n",
    "    scores = rouge.get_scores(hypothesis, reference)\n",
    "    dict=scores[0]\n",
    "    dict.values()\n",
    "    temp0 =list(dict.values())[0]\n",
    "    temp1= list(dict.values())[1]\n",
    "    temp2= list(dict.values())[2]\n",
    "    ROGUE_1 = temp0['f']\n",
    "    #ROGUE_2 = temp1['f']\n",
    "    #ROGUE_L = temp2['f']\n",
    "    return ROGUE_1\n",
    "\n",
    "def rouge2_scores(reference, hypothesis):\n",
    "    rouge = Rouge()\n",
    "    scores = rouge.get_scores(hypothesis, reference)\n",
    "    dict=scores[0]\n",
    "    dict.values()\n",
    "    temp0 =list(dict.values())[0]\n",
    "    temp1= list(dict.values())[1]\n",
    "    temp2= list(dict.values())[2]\n",
    "    #ROGUE_1 = temp0['f']\n",
    "    ROGUE_2 = temp1['f']\n",
    "    #ROGUE_L = temp2['f']\n",
    "    return ROGUE_2\n",
    "\n",
    "def rougeL_scores(reference, hypothesis):\n",
    "    rouge = Rouge()\n",
    "    scores = rouge.get_scores(hypothesis, reference)\n",
    "    dict=scores[0]\n",
    "    dict.values()\n",
    "    temp0 =list(dict.values())[0]\n",
    "    temp1= list(dict.values())[1]\n",
    "    temp2= list(dict.values())[2]\n",
    "    #ROGUE_1 = temp0['f']\n",
    "    #ROGUE_2 = temp1['f']\n",
    "    ROGUE_L = temp2['f']\n",
    "    return ROGUE_L"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "74576285",
   "metadata": {},
   "source": [
    "# ROUGE-1 without lower case"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "a12ed363",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.29, 0.5, 0.33, 0.67, 0.75, 0.6, 1.0, 0.8, 0.0, 0.67, 0.86, 0.86, 0.33, 0.75, 0.25, 0.5, 0.55, 0.67, 0.6, 0.29, 0.33, 0.73, 0.67, 0.29, 0.5, 0.0, 0.67, 0.89, 0.17, 0.74, 0.4, 0.62, 0.6, 0.25, 0.57, 0.86, 0.25, 0.25, 0.53, 0.4, 0.0, 0.36, 0.47, 0.4, 0.67, 0.22, 0.25, 0.29, 0.0, 0.5, 0.25, 0.43, 0.15, 0.33, 0.44, 0.29, 0.57, 0.36, 0.33, 0.18, 0.42, 0.45, 0.22, 0.25, 0.22, 0.25, 0.22, 0.2, 0.0, 0.0, 0.36, 0.29, 0.5, 0.36, 0.36, 0.62, 0.43, 0.29, 0.32, 0.29, 0.27, 0.38, 0.29, 0.0, 0.27, 0.43, 0.33, 0.36, 0.67, 0.25, 0.0, 0.0, 0.2]\n"
     ]
    }
   ],
   "source": [
    "rouge1=[]\n",
    "for i in range(len(auth_1)):\n",
    "    rouge1.append(round(rouge1_scores(preds[i],refs[i]),2))\n",
    "print(rouge1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "1eb3986c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.29, 0.5, 0.33, 0.67, 0.75, 0.6, 1.0, 0.8, 0.0, 0.67, 0.86, 0.86, 0.33, 0.75, 0.25, 0.5, 0.55, 0.67, 0.6, 0.29, 0.33, 0.73, 0.67, 0.29, 0.5, 0.0, 0.67, 0.89, 0.17, 0.74, 0.4, 0.62, 0.6, 0.25, 0.57, 0.86, 0.25, 0.25, 0.53, 0.4, 0.0, 0.36, 0.47, 0.4, 0.67, 0.22, 0.25, 0.29, 0.0, 0.5, 0.25, 0.43, 0.15, 0.33, 0.44, 0.29, 0.57, 0.36, 0.33, 0.18, 0.42, 0.45, 0.22, 0.25, 0.22, 0.25, 0.22, 0.2, 0.0, 0.0, 0.36, 0.29, 0.5, 0.36, 0.36, 0.62, 0.43, 0.29, 0.32, 0.29, 0.27, 0.38, 0.29, 0.0, 0.27, 0.43, 0.33, 0.36, 0.67, 0.25, 0.0, 0.0, 0.2]\n"
     ]
    }
   ],
   "source": [
    "norm_r1 = []\n",
    "\n",
    "max_r1=max(rouge1)\n",
    "min_r1=min(rouge1)\n",
    "\n",
    "for a in range(len(rouge1)):\n",
    "    norm_r1.append(round(((rouge1[a])/(max_r1)),2))\n",
    "print(norm_r1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "a8fff0ef",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Spearmans correlation coefficient: 0.723\n",
      "correlated (reject H0) p=0.000\n"
     ]
    }
   ],
   "source": [
    "from scipy.stats import spearmanr\n",
    "# calculate spearman's correlation\n",
    "coef, p = spearmanr(norm_r1, avg_norm_score)\n",
    "print('Spearmans correlation coefficient: %.3f' % coef)\n",
    "# interpret the significance\n",
    "alpha = 0.05\n",
    "if p > alpha:\n",
    "\tprint('uncorrelated (fail to reject H0) p=%.3f' % p)\n",
    "else:\n",
    "\tprint('correlated (reject H0) p=%.3f' % p)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "416706cb",
   "metadata": {},
   "source": [
    "# ROUGE-1 with lower case"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "79e28b95",
   "metadata": {},
   "outputs": [],
   "source": [
    "rouge1_l=[]\n",
    "for i in range(len(auth_1)):\n",
    "    rouge1_l.append(round(rouge1_scores(preds_lower[i],refs_lower[i]),2))\n",
    "print(rouge1_l)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e6a88c9e",
   "metadata": {},
   "outputs": [],
   "source": [
    "norm_r1_l = []\n",
    "\n",
    "max_r1=max(rouge1_l)\n",
    "min_r1=min(rouge1_l)\n",
    "\n",
    "for a in range(len(rouge1_l)):\n",
    "    norm_r1_l.append(round(((rouge1_l[a])/(max_r1_l)),2))\n",
    "print(norm_r1_l)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1054bc8d",
   "metadata": {},
   "outputs": [],
   "source": [
    "from scipy.stats import spearmanr\n",
    "# calculate spearman's correlation\n",
    "coef, p = spearmanr(norm_r1_l, avg_norm_score)\n",
    "print('Spearmans correlation coefficient: %.3f' % coef)\n",
    "# interpret the significance\n",
    "alpha = 0.05\n",
    "if p > alpha:\n",
    "\tprint('uncorrelated (fail to reject H0) p=%.3f' % p)\n",
    "else:\n",
    "\tprint('correlated (reject H0) p=%.3f' % p)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "47f97d66",
   "metadata": {},
   "source": [
    "# ROUGE-2 without lower case"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "fc8729c9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 0.0, 0.5, 0.5, 0.67, 0.75, 0.0, 0.0, 0.0, 0.5, 0.67, 0.67, 0.0, 0.33, 0.33, 0.33, 0.44, 0.0, 0.5, 0.0, 0.0, 0.44, 0.4, 0.0, 0.55, 0.0, 0.29, 0.57, 0.0, 0.47, 0.25, 0.36, 0.25, 0.0, 0.4, 1.0, 0.33, 0.0, 0.29, 0.25, 0.0, 0.22, 0.24, 0.22, 0.77, 0.0, 0.0, 0.0, 0.0, 0.4, 0.0, 0.31, 0.0, 0.24, 0.12, 0.21, 0.32, 0.0, 0.0, 0.0, 0.22, 0.29, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.2, 0.0, 0.0, 0.18, 0.0, 0.25, 0.0, 0.18, 0.19, 0.21, 0.15, 0.08, 0.0, 0.0, 0.0, 0.17, 0.18, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0]\n"
     ]
    }
   ],
   "source": [
    "rouge2=[]\n",
    "for i in range(len(auth_1)):\n",
    "    rouge2.append(round(rouge2_scores(preds[i],refs[i]),2))\n",
    "print(rouge2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "b66ee79b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 0.0, 0.5, 0.5, 0.67, 0.75, 0.0, 0.0, 0.0, 0.5, 0.67, 0.67, 0.0, 0.33, 0.33, 0.33, 0.44, 0.0, 0.5, 0.0, 0.0, 0.44, 0.4, 0.0, 0.55, 0.0, 0.29, 0.57, 0.0, 0.47, 0.25, 0.36, 0.25, 0.0, 0.4, 1.0, 0.33, 0.0, 0.29, 0.25, 0.0, 0.22, 0.24, 0.22, 0.77, 0.0, 0.0, 0.0, 0.0, 0.4, 0.0, 0.31, 0.0, 0.24, 0.12, 0.21, 0.32, 0.0, 0.0, 0.0, 0.22, 0.29, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.2, 0.0, 0.0, 0.18, 0.0, 0.25, 0.0, 0.18, 0.19, 0.21, 0.15, 0.08, 0.0, 0.0, 0.0, 0.17, 0.18, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0]\n"
     ]
    }
   ],
   "source": [
    "norm_r2 = []\n",
    "\n",
    "max_r2=max(rouge2)\n",
    "min_r2=min(rouge2)\n",
    "\n",
    "for a in range(len(rouge2)):\n",
    "    norm_r2.append(round(((rouge2[a])/(max_r2)),2))\n",
    "print(norm_r2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "6ece9d73",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Spearmans correlation coefficient: 0.485\n",
      "correlated (reject H0) p=0.000\n"
     ]
    }
   ],
   "source": [
    "from scipy.stats import spearmanr\n",
    "# calculate spearman's correlation\n",
    "coef, p = spearmanr(norm_r2, avg_norm_score)\n",
    "print('Spearmans correlation coefficient: %.3f' % coef)\n",
    "# interpret the significance\n",
    "alpha = 0.05\n",
    "if p > alpha:\n",
    "\tprint('uncorrelated (fail to reject H0) p=%.3f' % p)\n",
    "else:\n",
    "\tprint('correlated (reject H0) p=%.3f' % p)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "48e52a8a",
   "metadata": {},
   "source": [
    "# ROUGE-2 with lower case"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8d7a1861",
   "metadata": {},
   "outputs": [],
   "source": [
    "rouge2_l=[]\n",
    "for i in range(len(auth_1)):\n",
    "    rouge2_l.append(round(rouge2_scores(preds_lower[i],refs_lower[i]),2))\n",
    "print(rouge2_l)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ab7d0c19",
   "metadata": {},
   "outputs": [],
   "source": [
    "norm_r2_l = []\n",
    "\n",
    "max_r2=max(rouge2_l)\n",
    "min_r2=min(rouge2_l)\n",
    "\n",
    "for a in range(len(rouge2_l)):\n",
    "    norm_r2_l.append(round(((rouge2_l[a])/(max_r2)),2))\n",
    "print(norm_r2_l)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4b2ee5a0",
   "metadata": {},
   "outputs": [],
   "source": [
    "from scipy.stats import spearmanr\n",
    "# calculate spearman's correlation\n",
    "coef, p = spearmanr(norm_r2_l, avg_norm_score)\n",
    "print('Spearmans correlation coefficient: %.3f' % coef)\n",
    "# interpret the significance\n",
    "alpha = 0.05\n",
    "if p > alpha:\n",
    "\tprint('uncorrelated (fail to reject H0) p=%.3f' % p)\n",
    "else:\n",
    "\tprint('correlated (reject H0) p=%.3f' % p)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2eced254",
   "metadata": {},
   "source": [
    "# ROUGE-L without lower case"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "20658507",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.29, 0.5, 0.67, 0.67, 0.75, 0.8, 1.0, 0.8, 0.33, 0.67, 0.86, 0.86, 0.33, 0.75, 0.5, 0.75, 0.55, 0.67, 0.6, 0.29, 0.33, 0.73, 0.67, 0.57, 0.67, 0.33, 0.67, 0.89, 0.17, 0.78, 0.4, 0.67, 0.6, 0.25, 0.57, 1.0, 0.5, 0.5, 0.67, 0.4, 0.0, 0.36, 0.47, 0.4, 0.93, 0.22, 0.25, 0.29, 0.0, 0.5, 0.25, 0.43, 0.31, 0.35, 0.33, 0.38, 0.57, 0.36, 0.33, 0.18, 0.52, 0.45, 0.22, 0.25, 0.22, 0.25, 0.22, 0.2, 0.18, 0.29, 0.48, 0.29, 0.5, 0.36, 0.36, 0.75, 0.43, 0.29, 0.32, 0.29, 0.27, 0.38, 0.29, 0.29, 0.27, 0.43, 0.33, 0.36, 0.67, 0.25, 0.0, 0.0, 0.2]\n"
     ]
    }
   ],
   "source": [
    "rougel=[]\n",
    "for i in range(len(auth_1)):\n",
    "    rougel.append(round(rougeL_scores(preds[i],refs[i]),2))\n",
    "print(rougel)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "50b17e5d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.29, 0.5, 0.67, 0.67, 0.75, 0.8, 1.0, 0.8, 0.33, 0.67, 0.86, 0.86, 0.33, 0.75, 0.5, 0.75, 0.55, 0.67, 0.6, 0.29, 0.33, 0.73, 0.67, 0.57, 0.67, 0.33, 0.67, 0.89, 0.17, 0.78, 0.4, 0.67, 0.6, 0.25, 0.57, 1.0, 0.5, 0.5, 0.67, 0.4, 0.0, 0.36, 0.47, 0.4, 0.93, 0.22, 0.25, 0.29, 0.0, 0.5, 0.25, 0.43, 0.31, 0.35, 0.33, 0.38, 0.57, 0.36, 0.33, 0.18, 0.52, 0.45, 0.22, 0.25, 0.22, 0.25, 0.22, 0.2, 0.18, 0.29, 0.48, 0.29, 0.5, 0.36, 0.36, 0.75, 0.43, 0.29, 0.32, 0.29, 0.27, 0.38, 0.29, 0.29, 0.27, 0.43, 0.33, 0.36, 0.67, 0.25, 0.0, 0.0, 0.2]\n"
     ]
    }
   ],
   "source": [
    "norm_rl = []\n",
    "\n",
    "max_rl=max(rougel)\n",
    "min_rl=min(rougel)\n",
    "\n",
    "for a in range(len(rougel)):\n",
    "    norm_rl.append(round(((rougel[a])/(max_rl)),2))\n",
    "print(norm_rl)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "667c92ca",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Spearmans correlation coefficient: 0.799\n",
      "correlated (reject H0) p=0.000\n"
     ]
    }
   ],
   "source": [
    "from scipy.stats import spearmanr\n",
    "# calculate spearman's correlation\n",
    "coef, p = spearmanr(norm_rl, avg_norm_score)\n",
    "print('Spearmans correlation coefficient: %.3f' % coef)\n",
    "# interpret the significance\n",
    "alpha = 0.05\n",
    "if p > alpha:\n",
    "\tprint('uncorrelated (fail to reject H0) p=%.3f' % p)\n",
    "else:\n",
    "\tprint('correlated (reject H0) p=%.3f' % p)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ab65029f",
   "metadata": {},
   "source": [
    "# ROUGE-L with lower case"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "03a238a9",
   "metadata": {},
   "outputs": [],
   "source": [
    "rougel_l=[]\n",
    "for i in range(len(auth_1)):\n",
    "    rougel_l.append(round(rougeL_scores(preds_lower[i],refs_lower[i]),2))\n",
    "print(rougel_l"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e0a34089",
   "metadata": {},
   "outputs": [],
   "source": [
    "norm_rl_l = []\n",
    "\n",
    "max_rl=max(rougel_l)\n",
    "min_rl=min(rougel_l)\n",
    "\n",
    "for a in range(len(rougel_l)):\n",
    "    norm_rl_l.append(round(((rougel_l[a])/(max_rl)),2))\n",
    "print(norm_rl_l)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fb50918f",
   "metadata": {},
   "outputs": [],
   "source": [
    "from scipy.stats import spearmanr\n",
    "# calculate spearman's correlation\n",
    "coef, p = spearmanr(norm_rl_l, avg_norm_score)\n",
    "print('Spearmans correlation coefficient: %.3f' % coef)\n",
    "# interpret the significance\n",
    "alpha = 0.05\n",
    "if p > alpha:\n",
    "\tprint('uncorrelated (fail to reject H0) p=%.3f' % p)\n",
    "else:\n",
    "\tprint('correlated (reject H0) p=%.3f' % p)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "308137da",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "f9b10d9b",
   "metadata": {},
   "outputs": [],
   "source": [
    "def wer_score(hyp, ref, print_matrix=False):\n",
    "    N = len(hyp)\n",
    "    M = len(ref)\n",
    "    L = np.zeros((N,M))\n",
    "    for i in range(0, N):\n",
    "        for j in range(0, M):\n",
    "            if min(i,j) == 0:\n",
    "                L[i,j] = max(i,j)\n",
    "            else:\n",
    "                deletion = L[i-1,j] + 1\n",
    "                insertion = L[i,j-1] + 1\n",
    "                sub = 1 if hyp[i] != ref[j] else 0\n",
    "                substitution = L[i-1,j-1] + sub\n",
    "                L[i,j] = min(deletion, min(insertion, substitution))\n",
    "                #print(\"{} - {}: del {} ins {} sub {} s {}\".format(hyp[i], ref[j], deletion, insertion, substitution, sub))\n",
    "    if print_matrix:\n",
    "        print(\"WER matrix ({}x{}): \".format(N, M))\n",
    "        print(L)\n",
    "    return int(L[N-1, M-1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "id": "9633b284",
   "metadata": {},
   "outputs": [],
   "source": [
    "def ter(hyp, ref):\n",
    "    ref = str(ref).split()\n",
    "    r=len(ref)\n",
    "    hyp=str(hyp).split()\n",
    "    wer=wer_score(hyp, ref, print_matrix = False)\n",
    "    return wer/r"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f4f3c99b",
   "metadata": {},
   "source": [
    "# TER without lower case"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "id": "58a481b2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.6, 0.0, 0.33, 0.33, 0.25, 0.33, 0.5, 0.33, 0.67, 0.5, 0.67, 0.67, 0.67, 0.25, 0.5, 0.5, 0.5, 0.75, 0.75, 0.75, 0.75, 0.43, 0.5, 0.5, 0.56, 1.0, 0.5, 0.5, 2.0, 0.36, 0.6, 0.67, 0.5, 0.8, 1.0, 0.0, 0.67, 1.0, 0.45, 0.8, 0.6, 1.25, 0.56, 0.67, 0.11, 0.67, 1.33, 1.5, 0.6, 1.86, 1.0, 0.56, 0.71, 0.89, 1.0, 0.75, 0.53, 0.71, 0.71, 0.83, 0.79, 0.71, 1.25, 1.67, 1.25, 1.33, 1.67, 0.83, 0.86, 1.67, 0.62, 0.86, 2.5, 1.0, 0.62, 0.88, 0.86, 1.0, 1.0, 0.77, 1.0, 0.69, 0.88, 0.78, 0.86, 0.7, 0.75, 1.5, 1.75, 3.0, 1.0, 2.5, 1.25]\n"
     ]
    }
   ],
   "source": [
    "ter_=[]\n",
    "for i in range(len(auth_1)):\n",
    "    ter_.append(round(ter(preds[i],refs[i]),2))\n",
    "print(ter_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "id": "f9966aa3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.2, 0.0, 0.11, 0.11, 0.08, 0.11, 0.17, 0.11, 0.22, 0.17, 0.22, 0.22, 0.22, 0.08, 0.17, 0.17, 0.17, 0.25, 0.25, 0.25, 0.25, 0.14, 0.17, 0.17, 0.19, 0.33, 0.17, 0.17, 0.67, 0.12, 0.2, 0.22, 0.17, 0.27, 0.33, 0.0, 0.22, 0.33, 0.15, 0.27, 0.2, 0.42, 0.19, 0.22, 0.04, 0.22, 0.44, 0.5, 0.2, 0.62, 0.33, 0.19, 0.24, 0.3, 0.33, 0.25, 0.18, 0.24, 0.24, 0.28, 0.26, 0.24, 0.42, 0.56, 0.42, 0.44, 0.56, 0.28, 0.29, 0.56, 0.21, 0.29, 0.83, 0.33, 0.21, 0.29, 0.29, 0.33, 0.33, 0.26, 0.33, 0.23, 0.29, 0.26, 0.29, 0.23, 0.25, 0.5, 0.58, 1.0, 0.33, 0.83, 0.42]\n"
     ]
    }
   ],
   "source": [
    "norm_t = []\n",
    "\n",
    "max_t=max(ter_)\n",
    "min_t=min(ter_)\n",
    "\n",
    "for a in range(len(ter_)):\n",
    "    norm_t.append(round(((ter_[a])/(max_t)),2))\n",
    "print(norm_t)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "id": "f3405ef6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.8, 1.0, 0.89, 0.89, 0.92, 0.89, 0.83, 0.89, 0.78, 0.83, 0.78, 0.78, 0.78, 0.92, 0.83, 0.83, 0.83, 0.75, 0.75, 0.75, 0.75, 0.86, 0.83, 0.83, 0.81, 0.67, 0.83, 0.83, 0.33, 0.88, 0.8, 0.78, 0.83, 0.73, 0.67, 1.0, 0.78, 0.67, 0.85, 0.73, 0.8, 0.58, 0.81, 0.78, 0.96, 0.78, 0.56, 0.5, 0.8, 0.38, 0.67, 0.81, 0.76, 0.7, 0.67, 0.75, 0.82, 0.76, 0.76, 0.72, 0.74, 0.76, 0.58, 0.44, 0.58, 0.56, 0.44, 0.72, 0.71, 0.44, 0.79, 0.71, 0.17, 0.67, 0.79, 0.71, 0.71, 0.67, 0.67, 0.74, 0.67, 0.77, 0.71, 0.74, 0.71, 0.77, 0.75, 0.5, 0.42, 0.0, 0.67, 0.17, 0.58]\n"
     ]
    }
   ],
   "source": [
    "t_sim = []\n",
    "for i in range(len(norm_t)):\n",
    "    t_sim.append(round((1-norm_t[i]),2))\n",
    "print(t_sim)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "id": "0c3b36ef",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Spearmans correlation coefficient: 0.583\n",
      "correlated (reject H0) p=0.000\n"
     ]
    }
   ],
   "source": [
    "from scipy.stats import spearmanr\n",
    "# calculate spearman's correlation\n",
    "coef, p = spearmanr(t_sim, avg_norm_score)\n",
    "print('Spearmans correlation coefficient: %.3f' % coef)\n",
    "# interpret the significance\n",
    "alpha = 0.05\n",
    "if p > alpha:\n",
    "\tprint('uncorrelated (fail to reject H0) p=%.3f' % p)\n",
    "else:\n",
    "\tprint('correlated (reject H0) p=%.3f' % p)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2a91e81a",
   "metadata": {},
   "source": [
    "# TER with lower case"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0241ee0c",
   "metadata": {},
   "outputs": [],
   "source": [
    "ter_l=[]\n",
    "for i in range(len(auth_1)):\n",
    "    ter_l.append(round(ter(preds_lower[i],refs_lower[i]),2))\n",
    "print(ter_l)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a4b3ac62",
   "metadata": {},
   "outputs": [],
   "source": [
    "norm_t_l = []\n",
    "\n",
    "max_t=max(ter_l)\n",
    "min_t=min(ter_l)\n",
    "\n",
    "for a in range(len(ter_l)):\n",
    "    norm_t_l.append(round(((ter_l[a])/(max_t)),2))\n",
    "print(norm_t_l)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "35b3af83",
   "metadata": {},
   "outputs": [],
   "source": [
    "t_sim_l = []\n",
    "for i in range(len(norm_t_l)):\n",
    "    t_sim_l.append(round((1-norm_t_l[i]),2))\n",
    "print(t_sim_l)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fe28f329",
   "metadata": {},
   "outputs": [],
   "source": [
    "from scipy.stats import spearmanr\n",
    "# calculate spearman's correlation\n",
    "coef, p = spearmanr(t_sim_l, avg_norm_score)\n",
    "print('Spearmans correlation coefficient: %.3f' % coef)\n",
    "# interpret the significance\n",
    "alpha = 0.05\n",
    "if p > alpha:\n",
    "\tprint('uncorrelated (fail to reject H0) p=%.3f' % p)\n",
    "else:\n",
    "\tprint('correlated (reject H0) p=%.3f' % p)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "913c6b46",
   "metadata": {},
   "source": [
    "# METEOR without lower case"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "2ca24783",
   "metadata": {},
   "outputs": [],
   "source": [
    "from nltk.stem.porter import PorterStemmer\n",
    "from nltk.corpus import wordnet\n",
    "from itertools import chain, product\n",
    "\n",
    "\n",
    "def _generate_enums(hypothesis, reference):\n",
    "    \"\"\"\n",
    "    Takes in string inputs for hypothesis and reference and returns\n",
    "    enumerated word lists for each of them\n",
    "\n",
    "    :param hypothesis: hypothesis string\n",
    "    :type hypothesis: str\n",
    "    :param reference: reference string\n",
    "    :type reference: str\n",
    "    :preprocess: preprocessing method (default str.lower)\n",
    "    :type preprocess: method\n",
    "    :return: enumerated words list\n",
    "    :rtype: list of 2D tuples, list of 2D tuples\n",
    "    \"\"\"\n",
    "    hypothesis_list = list(enumerate(hypothesis.split()))\n",
    "    reference_list = list(enumerate(reference.split()))\n",
    "    return hypothesis_list, reference_list\n",
    "\n",
    "\n",
    "def exact_match(hypothesis, reference):\n",
    "    \"\"\"\n",
    "    matches exact words in hypothesis and reference\n",
    "    and returns a word mapping based on the enumerated\n",
    "    word id between hypothesis and reference\n",
    "\n",
    "    :param hypothesis: hypothesis string\n",
    "    :type hypothesis: str\n",
    "    :param reference: reference string\n",
    "    :type reference: str\n",
    "    :return: enumerated matched tuples, enumerated unmatched hypothesis tuples,\n",
    "             enumerated unmatched reference tuples\n",
    "    :rtype: list of 2D tuples, list of 2D tuples,  list of 2D tuples\n",
    "    \"\"\"\n",
    "    hypothesis_list, reference_list = _generate_enums(hypothesis, reference)\n",
    "    return _match_enums(hypothesis_list, reference_list)\n",
    "\n",
    "\n",
    "\n",
    "def _match_enums(enum_hypothesis_list, enum_reference_list):\n",
    "    \"\"\"\n",
    "    matches exact words in hypothesis and reference and returns\n",
    "    a word mapping between enum_hypothesis_list and enum_reference_list\n",
    "    based on the enumerated word id.\n",
    "\n",
    "    :param enum_hypothesis_list: enumerated hypothesis list\n",
    "    :type enum_hypothesis_list: list of tuples\n",
    "    :param enum_reference_list: enumerated reference list\n",
    "    :type enum_reference_list: list of 2D tuples\n",
    "    :return: enumerated matched tuples, enumerated unmatched hypothesis tuples,\n",
    "             enumerated unmatched reference tuples\n",
    "    :rtype: list of 2D tuples, list of 2D tuples,  list of 2D tuples\n",
    "    \"\"\"\n",
    "    word_match = []\n",
    "    for i in range(len(enum_hypothesis_list))[::-1]:\n",
    "        for j in range(len(enum_reference_list))[::-1]:\n",
    "            if enum_hypothesis_list[i][1] == enum_reference_list[j][1]:\n",
    "                word_match.append(\n",
    "                    (enum_hypothesis_list[i][0], enum_reference_list[j][0])\n",
    "                )\n",
    "                (enum_hypothesis_list.pop(i)[1], enum_reference_list.pop(j)[1])\n",
    "                break\n",
    "    return word_match, enum_hypothesis_list, enum_reference_list\n",
    "\n",
    "\n",
    "def _enum_stem_match(\n",
    "    enum_hypothesis_list, enum_reference_list, stemmer=PorterStemmer()\n",
    "):\n",
    "    \"\"\"\n",
    "    Stems each word and matches them in hypothesis and reference\n",
    "    and returns a word mapping between enum_hypothesis_list and\n",
    "    enum_reference_list based on the enumerated word id. The function also\n",
    "    returns a enumerated list of unmatched words for hypothesis and reference.\n",
    "\n",
    "    :param enum_hypothesis_list:\n",
    "    :type enum_hypothesis_list:\n",
    "    :param enum_reference_list:\n",
    "    :type enum_reference_list:\n",
    "    :param stemmer: nltk.stem.api.StemmerI object (default PorterStemmer())\n",
    "    :type stemmer: nltk.stem.api.StemmerI or any class that implements a stem method\n",
    "    :return: enumerated matched tuples, enumerated unmatched hypothesis tuples,\n",
    "             enumerated unmatched reference tuples\n",
    "    :rtype: list of 2D tuples, list of 2D tuples,  list of 2D tuples\n",
    "    \"\"\"\n",
    "    stemmed_enum_list1 = [\n",
    "        (word_pair[0], stemmer.stem(word_pair[1])) for word_pair in enum_hypothesis_list\n",
    "    ]\n",
    "\n",
    "    stemmed_enum_list2 = [\n",
    "        (word_pair[0], stemmer.stem(word_pair[1])) for word_pair in enum_reference_list\n",
    "    ]\n",
    "\n",
    "    word_match, enum_unmat_hypo_list, enum_unmat_ref_list = _match_enums(\n",
    "        stemmed_enum_list1, stemmed_enum_list2\n",
    "    )\n",
    "\n",
    "    enum_unmat_hypo_list = (\n",
    "        list(zip(*enum_unmat_hypo_list)) if len(enum_unmat_hypo_list) > 0 else []\n",
    "    )\n",
    "\n",
    "    enum_unmat_ref_list = (\n",
    "        list(zip(*enum_unmat_ref_list)) if len(enum_unmat_ref_list) > 0 else []\n",
    "    )\n",
    "\n",
    "    enum_hypothesis_list = list(\n",
    "        filter(lambda x: x[0] not in enum_unmat_hypo_list, enum_hypothesis_list)\n",
    "    )\n",
    "\n",
    "    enum_reference_list = list(\n",
    "        filter(lambda x: x[0] not in enum_unmat_ref_list, enum_reference_list)\n",
    "    )\n",
    "\n",
    "    return word_match, enum_hypothesis_list, enum_reference_list\n",
    "\n",
    "\n",
    "def stem_match(hypothesis, reference, stemmer=PorterStemmer()):\n",
    "    \"\"\"\n",
    "    Stems each word and matches them in hypothesis and reference\n",
    "    and returns a word mapping between hypothesis and reference\n",
    "\n",
    "    :param hypothesis:\n",
    "    :type hypothesis:\n",
    "    :param reference:\n",
    "    :type reference:\n",
    "    :param stemmer: nltk.stem.api.StemmerI object (default PorterStemmer())\n",
    "    :type stemmer: nltk.stem.api.StemmerI or any class that\n",
    "                   implements a stem method\n",
    "    :return: enumerated matched tuples, enumerated unmatched hypothesis tuples,\n",
    "             enumerated unmatched reference tuples\n",
    "    :rtype: list of 2D tuples, list of 2D tuples,  list of 2D tuples\n",
    "    \"\"\"\n",
    "    enum_hypothesis_list, enum_reference_list = _generate_enums(hypothesis, reference)\n",
    "    return _enum_stem_match(enum_hypothesis_list, enum_reference_list, stemmer=stemmer)\n",
    "\n",
    "\n",
    "\n",
    "def _enum_wordnetsyn_match(enum_hypothesis_list, enum_reference_list, wordnet=wordnet):\n",
    "    \"\"\"\n",
    "    Matches each word in reference to a word in hypothesis\n",
    "    if any synonym of a hypothesis word is the exact match\n",
    "    to the reference word.\n",
    "\n",
    "    :param enum_hypothesis_list: enumerated hypothesis list\n",
    "    :param enum_reference_list: enumerated reference list\n",
    "    :param wordnet: a wordnet corpus reader object (default nltk.corpus.wordnet)\n",
    "    :type wordnet: WordNetCorpusReader\n",
    "    :return: list of matched tuples, unmatched hypothesis list, unmatched reference list\n",
    "    :rtype:  list of tuples, list of tuples, list of tuples\n",
    "\n",
    "    \"\"\"\n",
    "    word_match = []\n",
    "    for i in range(len(enum_hypothesis_list))[::-1]:\n",
    "        hypothesis_syns = set(\n",
    "            chain.from_iterable(\n",
    "                (\n",
    "                    lemma.name()\n",
    "                    for lemma in synset.lemmas()\n",
    "                    if lemma.name().find(\"_\") < 0\n",
    "                )\n",
    "                for synset in wordnet.synsets(enum_hypothesis_list[i][1])\n",
    "            )\n",
    "        ).union({enum_hypothesis_list[i][1]})\n",
    "        for j in range(len(enum_reference_list))[::-1]:\n",
    "            if enum_reference_list[j][1] in hypothesis_syns:\n",
    "                word_match.append(\n",
    "                    (enum_hypothesis_list[i][0], enum_reference_list[j][0])\n",
    "                )\n",
    "                enum_hypothesis_list.pop(i), enum_reference_list.pop(j)\n",
    "                break\n",
    "    return word_match, enum_hypothesis_list, enum_reference_list\n",
    "\n",
    "\n",
    "def wordnetsyn_match(hypothesis, reference, wordnet=wordnet):\n",
    "    \"\"\"\n",
    "    Matches each word in reference to a word in hypothesis if any synonym\n",
    "    of a hypothesis word is the exact match to the reference word.\n",
    "\n",
    "    :param hypothesis: hypothesis string\n",
    "    :param reference: reference string\n",
    "    :param wordnet: a wordnet corpus reader object (default nltk.corpus.wordnet)\n",
    "    :type wordnet: WordNetCorpusReader\n",
    "    :return: list of mapped tuples\n",
    "    :rtype: list of tuples\n",
    "    \"\"\"\n",
    "    enum_hypothesis_list, enum_reference_list = _generate_enums(hypothesis, reference)\n",
    "    return _enum_wordnetsyn_match(\n",
    "        enum_hypothesis_list, enum_reference_list, wordnet=wordnet\n",
    "    )\n",
    "\n",
    "\n",
    "\n",
    "def _enum_allign_words(\n",
    "    enum_hypothesis_list, enum_reference_list, stemmer=PorterStemmer(), wordnet=wordnet\n",
    "):\n",
    "    \"\"\"\n",
    "    Aligns/matches words in the hypothesis to reference by sequentially\n",
    "    applying exact match, stemmed match and wordnet based synonym match.\n",
    "    in case there are multiple matches the match which has the least number\n",
    "    of crossing is chosen. Takes enumerated list as input instead of\n",
    "    string input\n",
    "\n",
    "    :param enum_hypothesis_list: enumerated hypothesis list\n",
    "    :param enum_reference_list: enumerated reference list\n",
    "    :param stemmer: nltk.stem.api.StemmerI object (default PorterStemmer())\n",
    "    :type stemmer: nltk.stem.api.StemmerI or any class that implements a stem method\n",
    "    :param wordnet: a wordnet corpus reader object (default nltk.corpus.wordnet)\n",
    "    :type wordnet: WordNetCorpusReader\n",
    "    :return: sorted list of matched tuples, unmatched hypothesis list,\n",
    "             unmatched reference list\n",
    "    :rtype: list of tuples, list of tuples, list of tuples\n",
    "    \"\"\"\n",
    "    exact_matches, enum_hypothesis_list, enum_reference_list = _match_enums(\n",
    "        enum_hypothesis_list, enum_reference_list\n",
    "    )\n",
    "\n",
    "    stem_matches, enum_hypothesis_list, enum_reference_list = _enum_stem_match(\n",
    "        enum_hypothesis_list, enum_reference_list, stemmer=stemmer\n",
    "    )\n",
    "\n",
    "    wns_matches, enum_hypothesis_list, enum_reference_list = _enum_wordnetsyn_match(\n",
    "        enum_hypothesis_list, enum_reference_list, wordnet=wordnet\n",
    "    )\n",
    "\n",
    "    return (\n",
    "        sorted(\n",
    "            exact_matches + stem_matches + wns_matches, key=lambda wordpair: wordpair[0]\n",
    "        ),\n",
    "        enum_hypothesis_list,\n",
    "        enum_reference_list,\n",
    "    )\n",
    "\n",
    "\n",
    "def allign_words(hypothesis, reference, stemmer=PorterStemmer(), wordnet=wordnet):\n",
    "    \"\"\"\n",
    "    Aligns/matches words in the hypothesis to reference by sequentially\n",
    "    applying exact match, stemmed match and wordnet based synonym match.\n",
    "    In case there are multiple matches the match which has the least number\n",
    "    of crossing is chosen.\n",
    "\n",
    "    :param hypothesis: hypothesis string\n",
    "    :param reference: reference string\n",
    "    :param stemmer: nltk.stem.api.StemmerI object (default PorterStemmer())\n",
    "    :type stemmer: nltk.stem.api.StemmerI or any class that implements a stem method\n",
    "    :param wordnet: a wordnet corpus reader object (default nltk.corpus.wordnet)\n",
    "    :type wordnet: WordNetCorpusReader\n",
    "    :return: sorted list of matched tuples, unmatched hypothesis list, unmatched reference list\n",
    "    :rtype: list of tuples, list of tuples, list of tuples\n",
    "    \"\"\"\n",
    "    enum_hypothesis_list, enum_reference_list = _generate_enums(hypothesis, reference)\n",
    "    return _enum_allign_words(\n",
    "        enum_hypothesis_list, enum_reference_list, stemmer=stemmer, wordnet=wordnet\n",
    "    )\n",
    "\n",
    "\n",
    "\n",
    "def _count_chunks(matches):\n",
    "    \"\"\"\n",
    "    Counts the fewest possible number of chunks such that matched unigrams\n",
    "    of each chunk are adjacent to each other. This is used to caluclate the\n",
    "    fragmentation part of the metric.\n",
    "\n",
    "    :param matches: list containing a mapping of matched words (output of allign_words)\n",
    "    :return: Number of chunks a sentence is divided into post allignment\n",
    "    :rtype: int\n",
    "    \"\"\"\n",
    "    i = 0\n",
    "    chunks = 1\n",
    "    while i < len(matches) - 1:\n",
    "        if (matches[i + 1][0] == matches[i][0] + 1) and (\n",
    "            matches[i + 1][1] == matches[i][1] + 1\n",
    "        ):\n",
    "            i += 1\n",
    "            continue\n",
    "        i += 1\n",
    "        chunks += 1\n",
    "    return chunks\n",
    "\n",
    "\n",
    "def single_meteor_score(\n",
    "    reference,\n",
    "    hypothesis,\n",
    "    stemmer=PorterStemmer(),\n",
    "    wordnet=wordnet,\n",
    "    alpha=0.9,\n",
    "    beta=3,\n",
    "    gamma=0.5,\n",
    "):\n",
    "    \"\"\"\n",
    "    Calculates METEOR score for single hypothesis and reference as per\n",
    "    \"Meteor: An Automatic Metric for MT Evaluation with HighLevels of\n",
    "    Correlation with Human Judgments\" by Alon Lavie and Abhaya Agarwal,\n",
    "    in Proceedings of ACL.\n",
    "    http://www.cs.cmu.edu/~alavie/METEOR/pdf/Lavie-Agarwal-2007-METEOR.pdf\n",
    "\n",
    "\n",
    "    >>> hypothesis1 = 'It is a guide to action which ensures that the military always obeys the commands of the party'\n",
    "\n",
    "    >>> reference1 = 'It is a guide to action that ensures that the military will forever heed Party commands'\n",
    "\n",
    "\n",
    "    >>> round(single_meteor_score(reference1, hypothesis1),4)\n",
    "    0.7398\n",
    "\n",
    "        If there is no words match during the alignment the method returns the\n",
    "        score as 0. We can safely  return a zero instead of raising a\n",
    "        division by zero error as no match usually implies a bad translation.\n",
    "\n",
    "    >>> round(meteor_score('this is a cat', 'non matching hypothesis'),4)\n",
    "    0.0\n",
    "\n",
    "    :param references: reference sentences\n",
    "    :type references: list(str)\n",
    "    :param hypothesis: a hypothesis sentence\n",
    "    :type hypothesis: str\n",
    "    :param preprocess: preprocessing function (default str.lower)\n",
    "    :type preprocess: method\n",
    "    :param stemmer: nltk.stem.api.StemmerI object (default PorterStemmer())\n",
    "    :type stemmer: nltk.stem.api.StemmerI or any class that implements a stem method\n",
    "    :param wordnet: a wordnet corpus reader object (default nltk.corpus.wordnet)\n",
    "    :type wordnet: WordNetCorpusReader\n",
    "    :param alpha: parameter for controlling relative weights of precision and recall.\n",
    "    :type alpha: float\n",
    "    :param beta: parameter for controlling shape of penalty as a\n",
    "                 function of as a function of fragmentation.\n",
    "    :type beta: float\n",
    "    :param gamma: relative weight assigned to fragmentation penality.\n",
    "    :type gamma: float\n",
    "    :return: The sentence-level METEOR score.\n",
    "    :rtype: float\n",
    "    \"\"\"\n",
    "    enum_hypothesis, enum_reference = _generate_enums(\n",
    "        hypothesis, reference)\n",
    "    translation_length = len(enum_hypothesis)\n",
    "    reference_length = len(enum_reference)\n",
    "    matches, _, _ = _enum_allign_words(enum_hypothesis, enum_reference, stemmer=stemmer)\n",
    "    matches_count = len(matches)\n",
    "    try:\n",
    "        precision = float(matches_count) / translation_length\n",
    "        recall = float(matches_count) / reference_length\n",
    "        fmean = (precision * recall) / (alpha * precision + (1 - alpha) * recall)\n",
    "        chunk_count = float(_count_chunks(matches))\n",
    "        frag_frac = chunk_count / matches_count\n",
    "    except ZeroDivisionError:\n",
    "        return 0.0\n",
    "    penalty = gamma * frag_frac ** beta\n",
    "    return (1 - penalty) * fmean\n",
    "\n",
    "\n",
    "\n",
    "def meteor_score_wolc(\n",
    "    references,\n",
    "    hypothesis,\n",
    "    stemmer=PorterStemmer(),\n",
    "    wordnet=wordnet,\n",
    "    alpha=0.9,\n",
    "    beta=3,\n",
    "    gamma=0.5,\n",
    "):\n",
    "    \"\"\"\n",
    "    Calculates METEOR score for hypothesis with multiple references as\n",
    "    described in \"Meteor: An Automatic Metric for MT Evaluation with\n",
    "    HighLevels of Correlation with Human Judgments\" by Alon Lavie and\n",
    "    Abhaya Agarwal, in Proceedings of ACL.\n",
    "    http://www.cs.cmu.edu/~alavie/METEOR/pdf/Lavie-Agarwal-2007-METEOR.pdf\n",
    "\n",
    "\n",
    "    In case of multiple references the best score is chosen. This method\n",
    "    iterates over single_meteor_score and picks the best pair among all\n",
    "    the references for a given hypothesis\n",
    "\n",
    "    >>> hypothesis1 = 'It is a guide to action which ensures that the military always obeys the commands of the party'\n",
    "    >>> hypothesis2 = 'It is to insure the troops forever hearing the activity guidebook that party direct'\n",
    "\n",
    "    >>> reference1 = 'It is a guide to action that ensures that the military will forever heed Party commands'\n",
    "    >>> reference2 = 'It is the guiding principle which guarantees the military forces always being under the command of the Party'\n",
    "    >>> reference3 = 'It is the practical guide for the army always to heed the directions of the party'\n",
    "\n",
    "    >>> round(meteor_score([reference1, reference2, reference3], hypothesis1),4)\n",
    "    0.7398\n",
    "\n",
    "        If there is no words match during the alignment the method returns the\n",
    "        score as 0. We can safely  return a zero instead of raising a\n",
    "        division by zero error as no match usually implies a bad translation.\n",
    "\n",
    "    >>> round(meteor_score(['this is a cat'], 'non matching hypothesis'),4)\n",
    "    0.0\n",
    "\n",
    "    :param references: reference sentences\n",
    "    :type references: list(str)\n",
    "    :param hypothesis: a hypothesis sentence\n",
    "    :type hypothesis: str\n",
    "    :param preprocess: preprocessing function (default str.lower)\n",
    "    :type preprocess: method\n",
    "    :param stemmer: nltk.stem.api.StemmerI object (default PorterStemmer())\n",
    "    :type stemmer: nltk.stem.api.StemmerI or any class that implements a stem method\n",
    "    :param wordnet: a wordnet corpus reader object (default nltk.corpus.wordnet)\n",
    "    :type wordnet: WordNetCorpusReader\n",
    "    :param alpha: parameter for controlling relative weights of precision and recall.\n",
    "    :type alpha: float\n",
    "    :param beta: parameter for controlling shape of penalty as a function\n",
    "                 of as a function of fragmentation.\n",
    "    :type beta: float\n",
    "    :param gamma: relative weight assigned to fragmentation penality.\n",
    "    :type gamma: float\n",
    "    :return: The sentence-level METEOR score.\n",
    "    :rtype: float\n",
    "    \"\"\"\n",
    "    return max(\n",
    "        [\n",
    "            single_meteor_score(\n",
    "                reference,\n",
    "                hypothesis,\n",
    "                stemmer=stemmer,\n",
    "                wordnet=wordnet,\n",
    "                alpha=alpha,\n",
    "                beta=beta,\n",
    "                gamma=gamma,\n",
    "            )\n",
    "            for reference in references\n",
    "        ]\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "a8c5f34a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0.5, 0.5, 0.98, 0.98, 0.94, 0.94, 0.94, 0.11, 0.94, 0.62, 0.62, 0.74, 0.68, 0.26, 0.34, 0.17, 0.49, 0.8, 0.8, 0.17, 0.64, 0.64, 0.89, 0.5, 0.38, 0.7, 0.13, 0.13, 0.56, 0.56, 0.45, 0.47, 0.32, 0.45, 0.89, 0.11, 0.73, 0.63, 0.8, 0.43, 0.1, 0.77, 1.0, 0.33, 0.31, 0.53, 0.37, 0.1, 0.65, 0.41, 0.42, 0.98, 0.09, 0.16, 0.22, 0.1, 0.69, 0.24, 0.34, 0.29, 0.48, 0.52, 0.38, 0.43, 0.15, 0.15, 0.08, 0.44, 0.34, 0.12, 0.15, 0.12, 0.15, 0.15, 0.17, 0.14, 0.29, 0.2, 0.14, 0.4, 0.28, 0.19, 0.68, 0.21, 0.18, 0.13, 0.23, 0.26, 0.28, 0.13, 0.22, 0.14, 0.27, 0.13, 0.33, 0.33, 0.2, 0.1, 0.2, 0.12]\n"
     ]
    }
   ],
   "source": [
    "meteor_wolc=[]\n",
    "for i in range(len(auth_1)):\n",
    "    meteor_wolc.append(round(meteor_score_wolc([refs[i]], preds[i]),2))\n",
    "print(meteor_wolc)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "999e8fe4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0.5, 0.5, 0.98, 0.98, 0.94, 0.94, 0.94, 0.11, 0.94, 0.62, 0.62, 0.74, 0.68, 0.26, 0.34, 0.17, 0.49, 0.8, 0.8, 0.17, 0.64, 0.64, 0.89, 0.5, 0.38, 0.7, 0.13, 0.13, 0.56, 0.56, 0.45, 0.47, 0.32, 0.45, 0.89, 0.11, 0.73, 0.63, 0.8, 0.43, 0.1, 0.77, 1.0, 0.33, 0.31, 0.53, 0.37, 0.1, 0.65, 0.41, 0.42, 0.98, 0.09, 0.16, 0.22, 0.1, 0.69, 0.24, 0.34, 0.29, 0.48, 0.52, 0.38, 0.43, 0.15, 0.15, 0.08, 0.44, 0.34, 0.12, 0.15, 0.12, 0.15, 0.15, 0.17, 0.14, 0.29, 0.2, 0.14, 0.4, 0.28, 0.19, 0.68, 0.21, 0.18, 0.13, 0.23, 0.26, 0.28, 0.13, 0.22, 0.14, 0.27, 0.13, 0.33, 0.33, 0.2, 0.1, 0.2, 0.12]\n"
     ]
    }
   ],
   "source": [
    "norm_m_wolc = []\n",
    "\n",
    "max_m=max(meteor_wolc)\n",
    "min_m=min(meteor_wolc)\n",
    "\n",
    "for a in range(len(meteor_wolc)):\n",
    "    norm_m_wolc.append(round(((meteor_wolc[a])/(max_m)),2))\n",
    "print(norm_m_wolc)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "eaf654f6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Spearmans correlation coefficient: 0.740\n",
      "correlated (reject H0) p=0.000\n"
     ]
    }
   ],
   "source": [
    "from scipy.stats import spearmanr\n",
    "# calculate spearman's correlation\n",
    "coef, p = spearmanr(norm_m_wolc, avg_norm_score)\n",
    "print('Spearmans correlation coefficient: %.3f' % coef)\n",
    "# interpret the significance\n",
    "alpha = 0.05\n",
    "if p > alpha:\n",
    "\tprint('uncorrelated (fail to reject H0) p=%.3f' % p)\n",
    "else:\n",
    "\tprint('correlated (reject H0) p=%.3f' % p)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "839c1593",
   "metadata": {},
   "source": [
    "# METEOR with lower case"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "8c641509",
   "metadata": {},
   "outputs": [],
   "source": [
    "from nltk.translate.meteor_score import meteor_score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "3bc79fc3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0.5, 0.5, 0.98, 0.98, 0.94, 0.94, 0.94, 0.11, 1.28, 0.62, 0.62, 0.74, 0.68, 0.26, 0.34, 0.17, 0.49, 0.8, 0.8, 0.17, 0.64, 0.47, 0.89, 0.5, 0.38, 0.7, 0.13, 0.13, 0.56, 0.56, 0.45, 0.47, 0.16, 0.45, 0.89, 0.11, 0.69, 0.63, 0.72, 0.43, 0.1, 0.77, 1.0, 0.33, 0.31, 0.53, 0.37, 0.1, 0.65, 0.41, 0.42, 0.98, 0.09, 0.16, 0.22, 0.1, 0.69, 0.24, 0.34, 0.22, 0.27, 0.52, 0.38, 0.43, 0.15, 0.15, 0.08, 0.37, 0.34, 0.12, 0.15, 0.12, 0.15, 0.15, 0.17, 0.07, 0.29, 0.2, 0.21, 0.4, 0.28, 0.19, 0.61, 0.21, 0.18, 0.13, 0.23, 0.26, 0.28, 0.13, 0.11, 0.14, 0.27, 0.13, 0.33, 0.33, 0.2, 0.1, 0.2, 0.12]\n"
     ]
    }
   ],
   "source": [
    "meteor_wlc=[]\n",
    "for i in range(len(auth_1)):\n",
    "    meteor_wlc.append(round(meteor_score([refs[i]], preds[i]),2))\n",
    "print(meteor_wlc)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "0316ca5a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0.39, 0.39, 0.77, 0.77, 0.73, 0.73, 0.73, 0.09, 1.0, 0.48, 0.48, 0.58, 0.53, 0.2, 0.27, 0.13, 0.38, 0.62, 0.62, 0.13, 0.5, 0.37, 0.7, 0.39, 0.3, 0.55, 0.1, 0.1, 0.44, 0.44, 0.35, 0.37, 0.12, 0.35, 0.7, 0.09, 0.54, 0.49, 0.56, 0.34, 0.08, 0.6, 0.78, 0.26, 0.24, 0.41, 0.29, 0.08, 0.51, 0.32, 0.33, 0.77, 0.07, 0.12, 0.17, 0.08, 0.54, 0.19, 0.27, 0.17, 0.21, 0.41, 0.3, 0.34, 0.12, 0.12, 0.06, 0.29, 0.27, 0.09, 0.12, 0.09, 0.12, 0.12, 0.13, 0.05, 0.23, 0.16, 0.16, 0.31, 0.22, 0.15, 0.48, 0.16, 0.14, 0.1, 0.18, 0.2, 0.22, 0.1, 0.09, 0.11, 0.21, 0.1, 0.26, 0.26, 0.16, 0.08, 0.16, 0.09]\n"
     ]
    }
   ],
   "source": [
    "norm_m_wlc = []\n",
    "\n",
    "max_m=max(meteor_wlc)\n",
    "min_m=min(meteor_wlc)\n",
    "\n",
    "for a in range(len(meteor_wlc)):\n",
    "    norm_m_wlc.append(round(((meteor_wlc[a])/(max_m)),2))\n",
    "print(norm_m_wlc)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "770037a6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Spearmans correlation coefficient: 0.748\n",
      "correlated (reject H0) p=0.000\n"
     ]
    }
   ],
   "source": [
    "from scipy.stats import spearmanr\n",
    "# calculate spearman's correlation\n",
    "coef, p = spearmanr(norm_m_wlc, avg_norm_score)\n",
    "print('Spearmans correlation coefficient: %.3f' % coef)\n",
    "# interpret the significance\n",
    "alpha = 0.05\n",
    "if p > alpha:\n",
    "\tprint('uncorrelated (fail to reject H0) p=%.3f' % p)\n",
    "else:\n",
    "\tprint('correlated (reject H0) p=%.3f' % p)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "830b2062",
   "metadata": {},
   "source": [
    "# METEOR-NEXT without lower case"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "edf6b049",
   "metadata": {},
   "outputs": [],
   "source": [
    "from nltk.stem.porter import PorterStemmer\n",
    "from nltk.corpus import wordnet\n",
    "from itertools import chain, product\n",
    "\n",
    "\n",
    "def _generate_enums(hypothesis, reference):\n",
    "    \"\"\"\n",
    "    Takes in string inputs for hypothesis and reference and returns\n",
    "    enumerated word lists for each of them\n",
    "\n",
    "    :param hypothesis: hypothesis string\n",
    "    :type hypothesis: str\n",
    "    :param reference: reference string\n",
    "    :type reference: str\n",
    "    :preprocess: preprocessing method (default str.lower)\n",
    "    :type preprocess: method\n",
    "    :return: enumerated words list\n",
    "    :rtype: list of 2D tuples, list of 2D tuples\n",
    "    \"\"\"\n",
    "    hypothesis_list = list(enumerate(hypothesis.split()))\n",
    "    reference_list = list(enumerate(reference.split()))\n",
    "    return hypothesis_list, reference_list\n",
    "\n",
    "\n",
    "def exact_match(hypothesis, reference):\n",
    "    \"\"\"\n",
    "    matches exact words in hypothesis and reference\n",
    "    and returns a word mapping based on the enumerated\n",
    "    word id between hypothesis and reference\n",
    "\n",
    "    :param hypothesis: hypothesis string\n",
    "    :type hypothesis: str\n",
    "    :param reference: reference string\n",
    "    :type reference: str\n",
    "    :return: enumerated matched tuples, enumerated unmatched hypothesis tuples,\n",
    "             enumerated unmatched reference tuples\n",
    "    :rtype: list of 2D tuples, list of 2D tuples,  list of 2D tuples\n",
    "    \"\"\"\n",
    "    hypothesis_list, reference_list = _generate_enums(hypothesis, reference)\n",
    "    return _match_enums(hypothesis_list, reference_list)\n",
    "\n",
    "\n",
    "\n",
    "def _match_enums(enum_hypothesis_list, enum_reference_list):\n",
    "    \"\"\"\n",
    "    matches exact words in hypothesis and reference and returns\n",
    "    a word mapping between enum_hypothesis_list and enum_reference_list\n",
    "    based on the enumerated word id.\n",
    "\n",
    "    :param enum_hypothesis_list: enumerated hypothesis list\n",
    "    :type enum_hypothesis_list: list of tuples\n",
    "    :param enum_reference_list: enumerated reference list\n",
    "    :type enum_reference_list: list of 2D tuples\n",
    "    :return: enumerated matched tuples, enumerated unmatched hypothesis tuples,\n",
    "             enumerated unmatched reference tuples\n",
    "    :rtype: list of 2D tuples, list of 2D tuples,  list of 2D tuples\n",
    "    \"\"\"\n",
    "    word_match = []\n",
    "    for i in range(len(enum_hypothesis_list))[::-1]:\n",
    "        for j in range(len(enum_reference_list))[::-1]:\n",
    "            if enum_hypothesis_list[i][1] == enum_reference_list[j][1]:\n",
    "                word_match.append(\n",
    "                    (enum_hypothesis_list[i][0], enum_reference_list[j][0])\n",
    "                )\n",
    "                (enum_hypothesis_list.pop(i)[1], enum_reference_list.pop(j)[1])\n",
    "                break\n",
    "    return word_match, enum_hypothesis_list, enum_reference_list\n",
    "\n",
    "\n",
    "def _enum_stem_match(\n",
    "    enum_hypothesis_list, enum_reference_list, stemmer=PorterStemmer()\n",
    "):\n",
    "    \"\"\"\n",
    "    Stems each word and matches them in hypothesis and reference\n",
    "    and returns a word mapping between enum_hypothesis_list and\n",
    "    enum_reference_list based on the enumerated word id. The function also\n",
    "    returns a enumerated list of unmatched words for hypothesis and reference.\n",
    "\n",
    "    :param enum_hypothesis_list:\n",
    "    :type enum_hypothesis_list:\n",
    "    :param enum_reference_list:\n",
    "    :type enum_reference_list:\n",
    "    :param stemmer: nltk.stem.api.StemmerI object (default PorterStemmer())\n",
    "    :type stemmer: nltk.stem.api.StemmerI or any class that implements a stem method\n",
    "    :return: enumerated matched tuples, enumerated unmatched hypothesis tuples,\n",
    "             enumerated unmatched reference tuples\n",
    "    :rtype: list of 2D tuples, list of 2D tuples,  list of 2D tuples\n",
    "    \"\"\"\n",
    "    stemmed_enum_list1 = [\n",
    "        (word_pair[0], stemmer.stem(word_pair[1])) for word_pair in enum_hypothesis_list\n",
    "    ]\n",
    "\n",
    "    stemmed_enum_list2 = [\n",
    "        (word_pair[0], stemmer.stem(word_pair[1])) for word_pair in enum_reference_list\n",
    "    ]\n",
    "\n",
    "    word_match, enum_unmat_hypo_list, enum_unmat_ref_list = _match_enums(\n",
    "        stemmed_enum_list1, stemmed_enum_list2\n",
    "    )\n",
    "\n",
    "    enum_unmat_hypo_list = (\n",
    "        list(zip(*enum_unmat_hypo_list)) if len(enum_unmat_hypo_list) > 0 else []\n",
    "    )\n",
    "\n",
    "    enum_unmat_ref_list = (\n",
    "        list(zip(*enum_unmat_ref_list)) if len(enum_unmat_ref_list) > 0 else []\n",
    "    )\n",
    "\n",
    "    enum_hypothesis_list = list(\n",
    "        filter(lambda x: x[0] not in enum_unmat_hypo_list, enum_hypothesis_list)\n",
    "    )\n",
    "\n",
    "    enum_reference_list = list(\n",
    "        filter(lambda x: x[0] not in enum_unmat_ref_list, enum_reference_list)\n",
    "    )\n",
    "\n",
    "    return word_match, enum_hypothesis_list, enum_reference_list\n",
    "\n",
    "\n",
    "def stem_match(hypothesis, reference, stemmer=PorterStemmer()):\n",
    "    \"\"\"\n",
    "    Stems each word and matches them in hypothesis and reference\n",
    "    and returns a word mapping between hypothesis and reference\n",
    "\n",
    "    :param hypothesis:\n",
    "    :type hypothesis:\n",
    "    :param reference:\n",
    "    :type reference:\n",
    "    :param stemmer: nltk.stem.api.StemmerI object (default PorterStemmer())\n",
    "    :type stemmer: nltk.stem.api.StemmerI or any class that\n",
    "                   implements a stem method\n",
    "    :return: enumerated matched tuples, enumerated unmatched hypothesis tuples,\n",
    "             enumerated unmatched reference tuples\n",
    "    :rtype: list of 2D tuples, list of 2D tuples,  list of 2D tuples\n",
    "    \"\"\"\n",
    "    enum_hypothesis_list, enum_reference_list = _generate_enums(hypothesis, reference)\n",
    "    return _enum_stem_match(enum_hypothesis_list, enum_reference_list, stemmer=stemmer)\n",
    "\n",
    "\n",
    "\n",
    "def _enum_wordnetsyn_match(enum_hypothesis_list, enum_reference_list, wordnet=wordnet):\n",
    "    \"\"\"\n",
    "    Matches each word in reference to a word in hypothesis\n",
    "    if any synonym of a hypothesis word is the exact match\n",
    "    to the reference word.\n",
    "\n",
    "    :param enum_hypothesis_list: enumerated hypothesis list\n",
    "    :param enum_reference_list: enumerated reference list\n",
    "    :param wordnet: a wordnet corpus reader object (default nltk.corpus.wordnet)\n",
    "    :type wordnet: WordNetCorpusReader\n",
    "    :return: list of matched tuples, unmatched hypothesis list, unmatched reference list\n",
    "    :rtype:  list of tuples, list of tuples, list of tuples\n",
    "\n",
    "    \"\"\"\n",
    "    word_match = []\n",
    "    for i in range(len(enum_hypothesis_list))[::-1]:\n",
    "        hypothesis_syns = set(\n",
    "            chain.from_iterable(\n",
    "                (\n",
    "                    lemma.name()\n",
    "                    for lemma in synset.lemmas()\n",
    "                    if lemma.name().find(\"_\") < 0\n",
    "                )\n",
    "                for synset in wordnet.synsets(enum_hypothesis_list[i][1])\n",
    "            )\n",
    "        ).union({enum_hypothesis_list[i][1]})\n",
    "        for j in range(len(enum_reference_list))[::-1]:\n",
    "            if enum_reference_list[j][1] in hypothesis_syns:\n",
    "                word_match.append(\n",
    "                    (enum_hypothesis_list[i][0], enum_reference_list[j][0])\n",
    "                )\n",
    "                enum_hypothesis_list.pop(i), enum_reference_list.pop(j)\n",
    "                break\n",
    "    return word_match, enum_hypothesis_list, enum_reference_list\n",
    "\n",
    "\n",
    "def wordnetsyn_match(hypothesis, reference, wordnet=wordnet):\n",
    "    \"\"\"\n",
    "    Matches each word in reference to a word in hypothesis if any synonym\n",
    "    of a hypothesis word is the exact match to the reference word.\n",
    "\n",
    "    :param hypothesis: hypothesis string\n",
    "    :param reference: reference string\n",
    "    :param wordnet: a wordnet corpus reader object (default nltk.corpus.wordnet)\n",
    "    :type wordnet: WordNetCorpusReader\n",
    "    :return: list of mapped tuples\n",
    "    :rtype: list of tuples\n",
    "    \"\"\"\n",
    "    enum_hypothesis_list, enum_reference_list = _generate_enums(hypothesis, reference)\n",
    "    return _enum_wordnetsyn_match(\n",
    "        enum_hypothesis_list, enum_reference_list, wordnet=wordnet\n",
    "    )\n",
    "\n",
    "\n",
    "\n",
    "def _enum_allign_words(\n",
    "    enum_hypothesis_list, enum_reference_list, stemmer=PorterStemmer(), wordnet=wordnet\n",
    "):\n",
    "    \"\"\"\n",
    "    Aligns/matches words in the hypothesis to reference by sequentially\n",
    "    applying exact match, stemmed match and wordnet based synonym match.\n",
    "    in case there are multiple matches the match which has the least number\n",
    "    of crossing is chosen. Takes enumerated list as input instead of\n",
    "    string input\n",
    "\n",
    "    :param enum_hypothesis_list: enumerated hypothesis list\n",
    "    :param enum_reference_list: enumerated reference list\n",
    "    :param stemmer: nltk.stem.api.StemmerI object (default PorterStemmer())\n",
    "    :type stemmer: nltk.stem.api.StemmerI or any class that implements a stem method\n",
    "    :param wordnet: a wordnet corpus reader object (default nltk.corpus.wordnet)\n",
    "    :type wordnet: WordNetCorpusReader\n",
    "    :return: sorted list of matched tuples, unmatched hypothesis list,\n",
    "             unmatched reference list\n",
    "    :rtype: list of tuples, list of tuples, list of tuples\n",
    "    \"\"\"\n",
    "    exact_matches, enum_hypothesis_list, enum_reference_list = _match_enums(\n",
    "        enum_hypothesis_list, enum_reference_list\n",
    "    )\n",
    "\n",
    "    stem_matches, enum_hypothesis_list, enum_reference_list = _enum_stem_match(\n",
    "        enum_hypothesis_list, enum_reference_list, stemmer=stemmer\n",
    "    )\n",
    "\n",
    "    wns_matches, enum_hypothesis_list, enum_reference_list = _enum_wordnetsyn_match(\n",
    "        enum_hypothesis_list, enum_reference_list, wordnet=wordnet\n",
    "    )\n",
    "\n",
    "    return (\n",
    "        sorted(\n",
    "            exact_matches + stem_matches + wns_matches, key=lambda wordpair: wordpair[0]\n",
    "        ),\n",
    "        enum_hypothesis_list,\n",
    "        enum_reference_list,\n",
    "    )\n",
    "\n",
    "\n",
    "def allign_words(hypothesis, reference, stemmer=PorterStemmer(), wordnet=wordnet):\n",
    "    \"\"\"\n",
    "    Aligns/matches words in the hypothesis to reference by sequentially\n",
    "    applying exact match, stemmed match and wordnet based synonym match.\n",
    "    In case there are multiple matches the match which has the least number\n",
    "    of crossing is chosen.\n",
    "\n",
    "    :param hypothesis: hypothesis string\n",
    "    :param reference: reference string\n",
    "    :param stemmer: nltk.stem.api.StemmerI object (default PorterStemmer())\n",
    "    :type stemmer: nltk.stem.api.StemmerI or any class that implements a stem method\n",
    "    :param wordnet: a wordnet corpus reader object (default nltk.corpus.wordnet)\n",
    "    :type wordnet: WordNetCorpusReader\n",
    "    :return: sorted list of matched tuples, unmatched hypothesis list, unmatched reference list\n",
    "    :rtype: list of tuples, list of tuples, list of tuples\n",
    "    \"\"\"\n",
    "    enum_hypothesis_list, enum_reference_list = _generate_enums(hypothesis, reference)\n",
    "    return _enum_allign_words(\n",
    "        enum_hypothesis_list, enum_reference_list, stemmer=stemmer, wordnet=wordnet\n",
    "    )\n",
    "\n",
    "\n",
    "\n",
    "def _count_chunks(matches):\n",
    "    \"\"\"\n",
    "    Counts the fewest possible number of chunks such that matched unigrams\n",
    "    of each chunk are adjacent to each other. This is used to caluclate the\n",
    "    fragmentation part of the metric.\n",
    "\n",
    "    :param matches: list containing a mapping of matched words (output of allign_words)\n",
    "    :return: Number of chunks a sentence is divided into post allignment\n",
    "    :rtype: int\n",
    "    \"\"\"\n",
    "    i = 0\n",
    "    chunks = 1\n",
    "    while i < len(matches) - 1:\n",
    "        if (matches[i + 1][0] == matches[i][0] + 1) and (\n",
    "            matches[i + 1][1] == matches[i][1] + 1\n",
    "        ):\n",
    "            i += 1\n",
    "            continue\n",
    "        i += 1\n",
    "        chunks += 1\n",
    "    return chunks\n",
    "\n",
    "\n",
    "def single_meteor_score(\n",
    "    reference,\n",
    "    hypothesis,\n",
    "    \n",
    "    stemmer=PorterStemmer(),\n",
    "    wordnet=wordnet,\n",
    "    alpha=0.9,\n",
    "    beta=3,\n",
    "    gamma=0.5,\n",
    "):\n",
    "    \"\"\"\n",
    "    Calculates METEOR score for single hypothesis and reference as per\n",
    "    \"Meteor: An Automatic Metric for MT Evaluation with HighLevels of\n",
    "    Correlation with Human Judgments\" by Alon Lavie and Abhaya Agarwal,\n",
    "    in Proceedings of ACL.\n",
    "    http://www.cs.cmu.edu/~alavie/METEOR/pdf/Lavie-Agarwal-2007-METEOR.pdf\n",
    "\n",
    "\n",
    "    >>> hypothesis1 = 'It is a guide to action which ensures that the military always obeys the commands of the party'\n",
    "\n",
    "    >>> reference1 = 'It is a guide to action that ensures that the military will forever heed Party commands'\n",
    "\n",
    "\n",
    "    >>> round(single_meteor_score(reference1, hypothesis1),4)\n",
    "    0.7398\n",
    "\n",
    "        If there is no words match during the alignment the method returns the\n",
    "        score as 0. We can safely  return a zero instead of raising a\n",
    "        division by zero error as no match usually implies a bad translation.\n",
    "\n",
    "    >>> round(meteor_score('this is a cat', 'non matching hypothesis'),4)\n",
    "    0.0\n",
    "\n",
    "    :param references: reference sentences\n",
    "    :type references: list(str)\n",
    "    :param hypothesis: a hypothesis sentence\n",
    "    :type hypothesis: str\n",
    "    :param preprocess: preprocessing function (default str.lower)\n",
    "    :type preprocess: method\n",
    "    :param stemmer: nltk.stem.api.StemmerI object (default PorterStemmer())\n",
    "    :type stemmer: nltk.stem.api.StemmerI or any class that implements a stem method\n",
    "    :param wordnet: a wordnet corpus reader object (default nltk.corpus.wordnet)\n",
    "    :type wordnet: WordNetCorpusReader\n",
    "    :param alpha: parameter for controlling relative weights of precision and recall.\n",
    "    :type alpha: float\n",
    "    :param beta: parameter for controlling shape of penalty as a\n",
    "                 function of as a function of fragmentation.\n",
    "    :type beta: float\n",
    "    :param gamma: relative weight assigned to fragmentation penality.\n",
    "    :type gamma: float\n",
    "    :return: The sentence-level METEOR score.\n",
    "    :rtype: float\n",
    "    \"\"\"\n",
    "    enum_hypothesis, enum_reference = _generate_enums(\n",
    "        hypothesis, reference\n",
    "    )\n",
    "    translation_length = len(enum_hypothesis)\n",
    "    reference_length = len(enum_reference)\n",
    "    \n",
    "    exact_m,_,_ = exact_match(hypothesis, reference)\n",
    "    stem_m,_,_ = _enum_stem_match(enum_hypothesis, enum_reference, stemmer=PorterStemmer())\n",
    "    syn_m,_,_ = _enum_wordnetsyn_match(enum_hypothesis, enum_reference, wordnet=wordnet)\n",
    "    \n",
    "\n",
    "    exact_count = len(list(exact_m))\n",
    "    stem_count = len(list(set(stem_m).difference(exact_m)))\n",
    "    syn_count = len(list(set(syn_m).difference(stem_m)))\n",
    "    matches_count = exact_count+stem_count+syn_count\n",
    "    try:\n",
    "        precision = float(matches_count) / translation_length\n",
    "        recall = float(matches_count) / reference_length\n",
    "        fmean = (precision * recall) / (alpha * precision + (1 - alpha) * recall)\n",
    "        chunk_count = float(_count_chunks(syn_m))\n",
    "        frag_frac = chunk_count / matches_count\n",
    "    except ZeroDivisionError:\n",
    "        return 0.0\n",
    "    penalty = gamma * frag_frac ** beta\n",
    "    return fmean*(1-penalty)\n",
    "\n",
    "\n",
    "\n",
    "def meteor_score_wolc(\n",
    "    references,\n",
    "    hypothesis,\n",
    "    \n",
    "    stemmer=PorterStemmer(),\n",
    "    wordnet=wordnet,\n",
    "    alpha=0.9,\n",
    "    beta=3,\n",
    "    gamma=0.5,\n",
    "):\n",
    "    \"\"\"\n",
    "    Calculates METEOR score for hypothesis with multiple references as\n",
    "    described in \"Meteor: An Automatic Metric for MT Evaluation with\n",
    "    HighLevels of Correlation with Human Judgments\" by Alon Lavie and\n",
    "    Abhaya Agarwal, in Proceedings of ACL.\n",
    "    http://www.cs.cmu.edu/~alavie/METEOR/pdf/Lavie-Agarwal-2007-METEOR.pdf\n",
    "\n",
    "\n",
    "    In case of multiple references the best score is chosen. This method\n",
    "    iterates over single_meteor_score and picks the best pair among all\n",
    "    the references for a given hypothesis\n",
    "\n",
    "    >>> hypothesis1 = 'It is a guide to action which ensures that the military always obeys the commands of the party'\n",
    "    >>> hypothesis2 = 'It is to insure the troops forever hearing the activity guidebook that party direct'\n",
    "\n",
    "    >>> reference1 = 'It is a guide to action that ensures that the military will forever heed Party commands'\n",
    "    >>> reference2 = 'It is the guiding principle which guarantees the military forces always being under the command of the Party'\n",
    "    >>> reference3 = 'It is the practical guide for the army always to heed the directions of the party'\n",
    "\n",
    "    >>> round(meteor_score([reference1, reference2, reference3], hypothesis1),4)\n",
    "    0.7398\n",
    "\n",
    "        If there is no words match during the alignment the method returns the\n",
    "        score as 0. We can safely  return a zero instead of raising a\n",
    "        division by zero error as no match usually implies a bad translation.\n",
    "\n",
    "    >>> round(meteor_score(['this is a cat'], 'non matching hypothesis'),4)\n",
    "    0.0\n",
    "\n",
    "    :param references: reference sentences\n",
    "    :type references: list(str)\n",
    "    :param hypothesis: a hypothesis sentence\n",
    "    :type hypothesis: str\n",
    "    :param preprocess: preprocessing function (default str.lower)\n",
    "    :type preprocess: method\n",
    "    :param stemmer: nltk.stem.api.StemmerI object (default PorterStemmer())\n",
    "    :type stemmer: nltk.stem.api.StemmerI or any class that implements a stem method\n",
    "    :param wordnet: a wordnet corpus reader object (default nltk.corpus.wordnet)\n",
    "    :type wordnet: WordNetCorpusReader\n",
    "    :param alpha: parameter for controlling relative weights of precision and recall.\n",
    "    :type alpha: float\n",
    "    :param beta: parameter for controlling shape of penalty as a function\n",
    "                 of as a function of fragmentation.\n",
    "    :type beta: float\n",
    "    :param gamma: relative weight assigned to fragmentation penality.\n",
    "    :type gamma: float\n",
    "    :return: The sentence-level METEOR score.\n",
    "    :rtype: float\n",
    "    \"\"\"\n",
    "    return max(\n",
    "        [\n",
    "            single_meteor_score(\n",
    "                reference,\n",
    "                hypothesis,\n",
    "                \n",
    "                stemmer=stemmer,\n",
    "                wordnet=wordnet,\n",
    "                alpha=alpha,\n",
    "                beta=beta,\n",
    "                gamma=gamma,\n",
    "            )\n",
    "            for reference in references\n",
    "        ]\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "b4264a0d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0.5, 0.5, 0.5, 0.5, 0.5, 0.5, 0.5, 0.11, 0.94, 0.62, 0.33, 0.38, 0.54, 0.26, 0.34, 0.17, 0.26, 0.47, 0.47, 0.17, 0.38, 0.25, 0.75, 0.25, 0.38, 0.36, 0.13, 0.49, 0.3, 0.32, 0.45, 0.37, 0.16, 0.26, 0.48, 0.11, 0.77, 0.3, 1.02, 0.25, 0.1, 0.45, 0.64, 0.33, 0.59, 0.44, 0.2, 0.1, 0.33, 0.22, 0.25, 0.76, 0.09, 0.16, 0.22, 0.1, 0.42, 0.24, 0.17, 0.37, 0.22, 0.29, 0.38, 0.25, 0.15, 0.15, 0.08, 0.58, 0.18, 0.12, 0.15, 0.12, 0.15, 0.15, 0.31, 0.07, 0.55, 0.3, 0.14, 0.4, 0.28, 0.4, 0.36, 0.21, 0.5, 0.13, 0.12, 0.14, 0.16, 0.13, 0.11, 0.14, 0.16, 0.13, 0.33, 0.33, 0.2, 0.1, 0.2, 0.12]\n"
     ]
    }
   ],
   "source": [
    "meteor_wolc=[]\n",
    "for i in range(len(auth_1)):\n",
    "    meteor_wolc.append(round(meteor_score_wolc([refs[i]], preds[i]),2))\n",
    "print(meteor_wolc)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "1d25219f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0.49, 0.49, 0.49, 0.49, 0.49, 0.49, 0.49, 0.11, 0.92, 0.61, 0.32, 0.37, 0.53, 0.25, 0.33, 0.17, 0.25, 0.46, 0.46, 0.17, 0.37, 0.25, 0.74, 0.25, 0.37, 0.35, 0.13, 0.48, 0.29, 0.31, 0.44, 0.36, 0.16, 0.25, 0.47, 0.11, 0.75, 0.29, 1.0, 0.25, 0.1, 0.44, 0.63, 0.32, 0.58, 0.43, 0.2, 0.1, 0.32, 0.22, 0.25, 0.75, 0.09, 0.16, 0.22, 0.1, 0.41, 0.24, 0.17, 0.36, 0.22, 0.28, 0.37, 0.25, 0.15, 0.15, 0.08, 0.57, 0.18, 0.12, 0.15, 0.12, 0.15, 0.15, 0.3, 0.07, 0.54, 0.29, 0.14, 0.39, 0.27, 0.39, 0.35, 0.21, 0.49, 0.13, 0.12, 0.14, 0.16, 0.13, 0.11, 0.14, 0.16, 0.13, 0.32, 0.32, 0.2, 0.1, 0.2, 0.12]\n"
     ]
    }
   ],
   "source": [
    "norm_m_wolc = []\n",
    "\n",
    "max_m=max(meteor_wolc)\n",
    "min_m=min(meteor_wolc)\n",
    "\n",
    "for a in range(len(meteor_wolc)):\n",
    "    norm_m_wolc.append(round(((meteor_wolc[a])/(max_m)),2))\n",
    "print(norm_m_wolc)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "c71f2987",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Spearmans correlation coefficient: 0.736\n",
      "correlated (reject H0) p=0.000\n"
     ]
    }
   ],
   "source": [
    "from scipy.stats import spearmanr\n",
    "# calculate spearman's correlation\n",
    "coef, p = spearmanr(norm_m_woa, avg_norm_score)\n",
    "print('Spearmans correlation coefficient: %.3f' % coef)\n",
    "# interpret the significance\n",
    "alpha = 0.05\n",
    "if p > alpha:\n",
    "\tprint('uncorrelated (fail to reject H0) p=%.3f' % p)\n",
    "else:\n",
    "\tprint('correlated (reject H0) p=%.3f' % p)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b5ee8d8f",
   "metadata": {},
   "source": [
    "# METEOR-NEXT with lower case"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a5216617",
   "metadata": {},
   "outputs": [],
   "source": [
    "from nltk.stem.porter import PorterStemmer\n",
    "from nltk.corpus import wordnet\n",
    "from itertools import chain, product\n",
    "\n",
    "\n",
    "def _generate_enums(hypothesis, reference, preprocess=str.lower):\n",
    "    \"\"\"\n",
    "    Takes in string inputs for hypothesis and reference and returns\n",
    "    enumerated word lists for each of them\n",
    "\n",
    "    :param hypothesis: hypothesis string\n",
    "    :type hypothesis: str\n",
    "    :param reference: reference string\n",
    "    :type reference: str\n",
    "    :preprocess: preprocessing method (default str.lower)\n",
    "    :type preprocess: method\n",
    "    :return: enumerated words list\n",
    "    :rtype: list of 2D tuples, list of 2D tuples\n",
    "    \"\"\"\n",
    "    hypothesis_list = list(enumerate(preprocess(hypothesis).split()))\n",
    "    reference_list = list(enumerate(preprocess(reference).split()))\n",
    "    return hypothesis_list, reference_list\n",
    "\n",
    "\n",
    "def exact_match(hypothesis, reference):\n",
    "    \"\"\"\n",
    "    matches exact words in hypothesis and reference\n",
    "    and returns a word mapping based on the enumerated\n",
    "    word id between hypothesis and reference\n",
    "\n",
    "    :param hypothesis: hypothesis string\n",
    "    :type hypothesis: str\n",
    "    :param reference: reference string\n",
    "    :type reference: str\n",
    "    :return: enumerated matched tuples, enumerated unmatched hypothesis tuples,\n",
    "             enumerated unmatched reference tuples\n",
    "    :rtype: list of 2D tuples, list of 2D tuples,  list of 2D tuples\n",
    "    \"\"\"\n",
    "    hypothesis_list, reference_list = _generate_enums(hypothesis, reference)\n",
    "    return _match_enums(hypothesis_list, reference_list)\n",
    "\n",
    "\n",
    "\n",
    "def _match_enums(enum_hypothesis_list, enum_reference_list):\n",
    "    \"\"\"\n",
    "    matches exact words in hypothesis and reference and returns\n",
    "    a word mapping between enum_hypothesis_list and enum_reference_list\n",
    "    based on the enumerated word id.\n",
    "\n",
    "    :param enum_hypothesis_list: enumerated hypothesis list\n",
    "    :type enum_hypothesis_list: list of tuples\n",
    "    :param enum_reference_list: enumerated reference list\n",
    "    :type enum_reference_list: list of 2D tuples\n",
    "    :return: enumerated matched tuples, enumerated unmatched hypothesis tuples,\n",
    "             enumerated unmatched reference tuples\n",
    "    :rtype: list of 2D tuples, list of 2D tuples,  list of 2D tuples\n",
    "    \"\"\"\n",
    "    word_match = []\n",
    "    for i in range(len(enum_hypothesis_list))[::-1]:\n",
    "        for j in range(len(enum_reference_list))[::-1]:\n",
    "            if enum_hypothesis_list[i][1] == enum_reference_list[j][1]:\n",
    "                word_match.append(\n",
    "                    (enum_hypothesis_list[i][0], enum_reference_list[j][0])\n",
    "                )\n",
    "                (enum_hypothesis_list.pop(i)[1], enum_reference_list.pop(j)[1])\n",
    "                break\n",
    "    return word_match, enum_hypothesis_list, enum_reference_list\n",
    "\n",
    "\n",
    "def _enum_stem_match(\n",
    "    enum_hypothesis_list, enum_reference_list, stemmer=PorterStemmer()\n",
    "):\n",
    "    \"\"\"\n",
    "    Stems each word and matches them in hypothesis and reference\n",
    "    and returns a word mapping between enum_hypothesis_list and\n",
    "    enum_reference_list based on the enumerated word id. The function also\n",
    "    returns a enumerated list of unmatched words for hypothesis and reference.\n",
    "\n",
    "    :param enum_hypothesis_list:\n",
    "    :type enum_hypothesis_list:\n",
    "    :param enum_reference_list:\n",
    "    :type enum_reference_list:\n",
    "    :param stemmer: nltk.stem.api.StemmerI object (default PorterStemmer())\n",
    "    :type stemmer: nltk.stem.api.StemmerI or any class that implements a stem method\n",
    "    :return: enumerated matched tuples, enumerated unmatched hypothesis tuples,\n",
    "             enumerated unmatched reference tuples\n",
    "    :rtype: list of 2D tuples, list of 2D tuples,  list of 2D tuples\n",
    "    \"\"\"\n",
    "    stemmed_enum_list1 = [\n",
    "        (word_pair[0], stemmer.stem(word_pair[1])) for word_pair in enum_hypothesis_list\n",
    "    ]\n",
    "\n",
    "    stemmed_enum_list2 = [\n",
    "        (word_pair[0], stemmer.stem(word_pair[1])) for word_pair in enum_reference_list\n",
    "    ]\n",
    "\n",
    "    word_match, enum_unmat_hypo_list, enum_unmat_ref_list = _match_enums(\n",
    "        stemmed_enum_list1, stemmed_enum_list2\n",
    "    )\n",
    "\n",
    "    enum_unmat_hypo_list = (\n",
    "        list(zip(*enum_unmat_hypo_list)) if len(enum_unmat_hypo_list) > 0 else []\n",
    "    )\n",
    "\n",
    "    enum_unmat_ref_list = (\n",
    "        list(zip(*enum_unmat_ref_list)) if len(enum_unmat_ref_list) > 0 else []\n",
    "    )\n",
    "\n",
    "    enum_hypothesis_list = list(\n",
    "        filter(lambda x: x[0] not in enum_unmat_hypo_list, enum_hypothesis_list)\n",
    "    )\n",
    "\n",
    "    enum_reference_list = list(\n",
    "        filter(lambda x: x[0] not in enum_unmat_ref_list, enum_reference_list)\n",
    "    )\n",
    "\n",
    "    return word_match, enum_hypothesis_list, enum_reference_list\n",
    "\n",
    "\n",
    "def stem_match(hypothesis, reference, stemmer=PorterStemmer()):\n",
    "    \"\"\"\n",
    "    Stems each word and matches them in hypothesis and reference\n",
    "    and returns a word mapping between hypothesis and reference\n",
    "\n",
    "    :param hypothesis:\n",
    "    :type hypothesis:\n",
    "    :param reference:\n",
    "    :type reference:\n",
    "    :param stemmer: nltk.stem.api.StemmerI object (default PorterStemmer())\n",
    "    :type stemmer: nltk.stem.api.StemmerI or any class that\n",
    "                   implements a stem method\n",
    "    :return: enumerated matched tuples, enumerated unmatched hypothesis tuples,\n",
    "             enumerated unmatched reference tuples\n",
    "    :rtype: list of 2D tuples, list of 2D tuples,  list of 2D tuples\n",
    "    \"\"\"\n",
    "    enum_hypothesis_list, enum_reference_list = _generate_enums(hypothesis, reference)\n",
    "    return _enum_stem_match(enum_hypothesis_list, enum_reference_list, stemmer=stemmer)\n",
    "\n",
    "\n",
    "\n",
    "def _enum_wordnetsyn_match(enum_hypothesis_list, enum_reference_list, wordnet=wordnet):\n",
    "    \"\"\"\n",
    "    Matches each word in reference to a word in hypothesis\n",
    "    if any synonym of a hypothesis word is the exact match\n",
    "    to the reference word.\n",
    "\n",
    "    :param enum_hypothesis_list: enumerated hypothesis list\n",
    "    :param enum_reference_list: enumerated reference list\n",
    "    :param wordnet: a wordnet corpus reader object (default nltk.corpus.wordnet)\n",
    "    :type wordnet: WordNetCorpusReader\n",
    "    :return: list of matched tuples, unmatched hypothesis list, unmatched reference list\n",
    "    :rtype:  list of tuples, list of tuples, list of tuples\n",
    "\n",
    "    \"\"\"\n",
    "    word_match = []\n",
    "    for i in range(len(enum_hypothesis_list))[::-1]:\n",
    "        hypothesis_syns = set(\n",
    "            chain.from_iterable(\n",
    "                (\n",
    "                    lemma.name()\n",
    "                    for lemma in synset.lemmas()\n",
    "                    if lemma.name().find(\"_\") < 0\n",
    "                )\n",
    "                for synset in wordnet.synsets(enum_hypothesis_list[i][1])\n",
    "            )\n",
    "        ).union({enum_hypothesis_list[i][1]})\n",
    "        for j in range(len(enum_reference_list))[::-1]:\n",
    "            if enum_reference_list[j][1] in hypothesis_syns:\n",
    "                word_match.append(\n",
    "                    (enum_hypothesis_list[i][0], enum_reference_list[j][0])\n",
    "                )\n",
    "                enum_hypothesis_list.pop(i), enum_reference_list.pop(j)\n",
    "                break\n",
    "    return word_match, enum_hypothesis_list, enum_reference_list\n",
    "\n",
    "\n",
    "def wordnetsyn_match(hypothesis, reference, wordnet=wordnet):\n",
    "    \"\"\"\n",
    "    Matches each word in reference to a word in hypothesis if any synonym\n",
    "    of a hypothesis word is the exact match to the reference word.\n",
    "\n",
    "    :param hypothesis: hypothesis string\n",
    "    :param reference: reference string\n",
    "    :param wordnet: a wordnet corpus reader object (default nltk.corpus.wordnet)\n",
    "    :type wordnet: WordNetCorpusReader\n",
    "    :return: list of mapped tuples\n",
    "    :rtype: list of tuples\n",
    "    \"\"\"\n",
    "    enum_hypothesis_list, enum_reference_list = _generate_enums(hypothesis, reference)\n",
    "    return _enum_wordnetsyn_match(\n",
    "        enum_hypothesis_list, enum_reference_list, wordnet=wordnet\n",
    "    )\n",
    "\n",
    "\n",
    "\n",
    "def _enum_allign_words(\n",
    "    enum_hypothesis_list, enum_reference_list, stemmer=PorterStemmer(), wordnet=wordnet\n",
    "):\n",
    "    \"\"\"\n",
    "    Aligns/matches words in the hypothesis to reference by sequentially\n",
    "    applying exact match, stemmed match and wordnet based synonym match.\n",
    "    in case there are multiple matches the match which has the least number\n",
    "    of crossing is chosen. Takes enumerated list as input instead of\n",
    "    string input\n",
    "\n",
    "    :param enum_hypothesis_list: enumerated hypothesis list\n",
    "    :param enum_reference_list: enumerated reference list\n",
    "    :param stemmer: nltk.stem.api.StemmerI object (default PorterStemmer())\n",
    "    :type stemmer: nltk.stem.api.StemmerI or any class that implements a stem method\n",
    "    :param wordnet: a wordnet corpus reader object (default nltk.corpus.wordnet)\n",
    "    :type wordnet: WordNetCorpusReader\n",
    "    :return: sorted list of matched tuples, unmatched hypothesis list,\n",
    "             unmatched reference list\n",
    "    :rtype: list of tuples, list of tuples, list of tuples\n",
    "    \"\"\"\n",
    "    exact_matches, enum_hypothesis_list, enum_reference_list = _match_enums(\n",
    "        enum_hypothesis_list, enum_reference_list\n",
    "    )\n",
    "\n",
    "    stem_matches, enum_hypothesis_list, enum_reference_list = _enum_stem_match(\n",
    "        enum_hypothesis_list, enum_reference_list, stemmer=stemmer\n",
    "    )\n",
    "\n",
    "    wns_matches, enum_hypothesis_list, enum_reference_list = _enum_wordnetsyn_match(\n",
    "        enum_hypothesis_list, enum_reference_list, wordnet=wordnet\n",
    "    )\n",
    "\n",
    "    return (\n",
    "        sorted(\n",
    "            exact_matches + stem_matches + wns_matches, key=lambda wordpair: wordpair[0]\n",
    "        ),\n",
    "        enum_hypothesis_list,\n",
    "        enum_reference_list,\n",
    "    )\n",
    "\n",
    "\n",
    "def allign_words(hypothesis, reference, stemmer=PorterStemmer(), wordnet=wordnet):\n",
    "    \"\"\"\n",
    "    Aligns/matches words in the hypothesis to reference by sequentially\n",
    "    applying exact match, stemmed match and wordnet based synonym match.\n",
    "    In case there are multiple matches the match which has the least number\n",
    "    of crossing is chosen.\n",
    "\n",
    "    :param hypothesis: hypothesis string\n",
    "    :param reference: reference string\n",
    "    :param stemmer: nltk.stem.api.StemmerI object (default PorterStemmer())\n",
    "    :type stemmer: nltk.stem.api.StemmerI or any class that implements a stem method\n",
    "    :param wordnet: a wordnet corpus reader object (default nltk.corpus.wordnet)\n",
    "    :type wordnet: WordNetCorpusReader\n",
    "    :return: sorted list of matched tuples, unmatched hypothesis list, unmatched reference list\n",
    "    :rtype: list of tuples, list of tuples, list of tuples\n",
    "    \"\"\"\n",
    "    enum_hypothesis_list, enum_reference_list = _generate_enums(hypothesis, reference)\n",
    "    return _enum_allign_words(\n",
    "        enum_hypothesis_list, enum_reference_list, stemmer=stemmer, wordnet=wordnet\n",
    "    )\n",
    "\n",
    "\n",
    "\n",
    "def _count_chunks(matches):\n",
    "    \"\"\"\n",
    "    Counts the fewest possible number of chunks such that matched unigrams\n",
    "    of each chunk are adjacent to each other. This is used to caluclate the\n",
    "    fragmentation part of the metric.\n",
    "\n",
    "    :param matches: list containing a mapping of matched words (output of allign_words)\n",
    "    :return: Number of chunks a sentence is divided into post allignment\n",
    "    :rtype: int\n",
    "    \"\"\"\n",
    "    i = 0\n",
    "    chunks = 1\n",
    "    while i < len(matches) - 1:\n",
    "        if (matches[i + 1][0] == matches[i][0] + 1) and (\n",
    "            matches[i + 1][1] == matches[i][1] + 1\n",
    "        ):\n",
    "            i += 1\n",
    "            continue\n",
    "        i += 1\n",
    "        chunks += 1\n",
    "    return chunks\n",
    "\n",
    "\n",
    "def single_meteor_score(\n",
    "    reference,\n",
    "    hypothesis,\n",
    "    preprocess=str.lower,\n",
    "    stemmer=PorterStemmer(),\n",
    "    wordnet=wordnet,\n",
    "    alpha=0.85,\n",
    "    beta=2.35,\n",
    "    gamma=0.45,\n",
    "    w_1=1,\n",
    "    w_2=0.8,\n",
    "    w_3=0.6\n",
    "):\n",
    "    \"\"\"\n",
    "    Calculates METEOR score for single hypothesis and reference as per\n",
    "    \"Meteor: An Automatic Metric for MT Evaluation with HighLevels of\n",
    "    Correlation with Human Judgments\" by Alon Lavie and Abhaya Agarwal,\n",
    "    in Proceedings of ACL.\n",
    "    http://www.cs.cmu.edu/~alavie/METEOR/pdf/Lavie-Agarwal-2007-METEOR.pdf\n",
    "\n",
    "\n",
    "    >>> hypothesis1 = 'It is a guide to action which ensures that the military always obeys the commands of the party'\n",
    "\n",
    "    >>> reference1 = 'It is a guide to action that ensures that the military will forever heed Party commands'\n",
    "\n",
    "\n",
    "    >>> round(single_meteor_score(reference1, hypothesis1),4)\n",
    "    0.7398\n",
    "\n",
    "        If there is no words match during the alignment the method returns the\n",
    "        score as 0. We can safely  return a zero instead of raising a\n",
    "        division by zero error as no match usually implies a bad translation.\n",
    "\n",
    "    >>> round(meteor_score('this is a cat', 'non matching hypothesis'),4)\n",
    "    0.0\n",
    "\n",
    "    :param references: reference sentences\n",
    "    :type references: list(str)\n",
    "    :param hypothesis: a hypothesis sentence\n",
    "    :type hypothesis: str\n",
    "    :param preprocess: preprocessing function (default str.lower)\n",
    "    :type preprocess: method\n",
    "    :param stemmer: nltk.stem.api.StemmerI object (default PorterStemmer())\n",
    "    :type stemmer: nltk.stem.api.StemmerI or any class that implements a stem method\n",
    "    :param wordnet: a wordnet corpus reader object (default nltk.corpus.wordnet)\n",
    "    :type wordnet: WordNetCorpusReader\n",
    "    :param alpha: parameter for controlling relative weights of precision and recall.\n",
    "    :type alpha: float\n",
    "    :param beta: parameter for controlling shape of penalty as a\n",
    "                 function of as a function of fragmentation.\n",
    "    :type beta: float\n",
    "    :param gamma: relative weight assigned to fragmentation penality.\n",
    "    :type gamma: float\n",
    "    :return: The sentence-level METEOR score.\n",
    "    :rtype: float\n",
    "    \"\"\"\n",
    "    enum_hypothesis, enum_reference = _generate_enums(\n",
    "        hypothesis, reference, preprocess=preprocess\n",
    "    )\n",
    "    translation_length = len(enum_hypothesis)\n",
    "    reference_length = len(enum_reference)\n",
    "    matches, _, _ = _enum_allign_words(enum_hypothesis, enum_reference, stemmer=stemmer)\n",
    "    print(matches)\n",
    "    exact_m,_,_ = exact_match(hypothesis, reference)\n",
    "    print(exact_m)\n",
    "    stem_m,_,_ = _enum_stem_match(enum_hypothesis, enum_reference, stemmer=PorterStemmer())\n",
    "    print(stem_m)\n",
    "    syn_m,_,_ = _enum_wordnetsyn_match(enum_hypothesis, enum_reference, wordnet=wordnet)\n",
    "    print(syn_m)\n",
    "    \n",
    "    \n",
    "    \n",
    "    exact_count = len(exact_m)\n",
    "    stem_count = len((set(stem_m).difference(exact_m)))\n",
    "    syn_count = len((set(syn_m).difference(stem_m)))\n",
    "    print(exact_count)\n",
    "    matches_count = len(matches)\n",
    "    print(matches_count)\n",
    "    try:\n",
    "        precision = float(w_1*exact_count + w_2*stem_count + w_3*syn_count) / translation_length\n",
    "        recall = float(w_1*exact_count + w_2*stem_count + w_3*syn_count) / reference_length\n",
    "        #precision = float(matches_count) / translation_length\n",
    "        #recall = float(matches_count) / reference_length\n",
    "        fmean = (precision * recall) / (alpha * precision + (1 - alpha) * recall)\n",
    "        chunk_count = float(_count_chunks(matches))\n",
    "        frag_frac = chunk_count / matches_count\n",
    "    except ZeroDivisionError:\n",
    "        return 0.0\n",
    "    penalty = gamma * frag_frac ** beta\n",
    "    return (1 - penalty) * fmean\n",
    "\n",
    "\n",
    "\n",
    "def meteorn_score_wlc(\n",
    "    references,\n",
    "    hypothesis,\n",
    "    preprocess=str.lower,\n",
    "    stemmer=PorterStemmer(),\n",
    "    wordnet=wordnet,\n",
    "    alpha=0.85,\n",
    "    beta=2.35,\n",
    "    gamma=0.45,\n",
    "    w_1=1,\n",
    "    w_2=0.8,\n",
    "    w_3=0.6\n",
    "):\n",
    "    \"\"\"\n",
    "    Calculates METEOR score for hypothesis with multiple references as\n",
    "    described in \"Meteor: An Automatic Metric for MT Evaluation with\n",
    "    HighLevels of Correlation with Human Judgments\" by Alon Lavie and\n",
    "    Abhaya Agarwal, in Proceedings of ACL.\n",
    "    http://www.cs.cmu.edu/~alavie/METEOR/pdf/Lavie-Agarwal-2007-METEOR.pdf\n",
    "\n",
    "\n",
    "    In case of multiple references the best score is chosen. This method\n",
    "    iterates over single_meteor_score and picks the best pair among all\n",
    "    the references for a given hypothesis\n",
    "\n",
    "    >>> hypothesis1 = 'It is a guide to action which ensures that the military always obeys the commands of the party'\n",
    "    >>> hypothesis2 = 'It is to insure the troops forever hearing the activity guidebook that party direct'\n",
    "\n",
    "    >>> reference1 = 'It is a guide to action that ensures that the military will forever heed Party commands'\n",
    "    >>> reference2 = 'It is the guiding principle which guarantees the military forces always being under the command of the Party'\n",
    "    >>> reference3 = 'It is the practical guide for the army always to heed the directions of the party'\n",
    "\n",
    "    >>> round(meteor_score([reference1, reference2, reference3], hypothesis1),4)\n",
    "    0.7398\n",
    "\n",
    "        If there is no words match during the alignment the method returns the\n",
    "        score as 0. We can safely  return a zero instead of raising a\n",
    "        division by zero error as no match usually implies a bad translation.\n",
    "\n",
    "    >>> round(meteor_score(['this is a cat'], 'non matching hypothesis'),4)\n",
    "    0.0\n",
    "\n",
    "    :param references: reference sentences\n",
    "    :type references: list(str)\n",
    "    :param hypothesis: a hypothesis sentence\n",
    "    :type hypothesis: str\n",
    "    :param preprocess: preprocessing function (default str.lower)\n",
    "    :type preprocess: method\n",
    "    :param stemmer: nltk.stem.api.StemmerI object (default PorterStemmer())\n",
    "    :type stemmer: nltk.stem.api.StemmerI or any class that implements a stem method\n",
    "    :param wordnet: a wordnet corpus reader object (default nltk.corpus.wordnet)\n",
    "    :type wordnet: WordNetCorpusReader\n",
    "    :param alpha: parameter for controlling relative weights of precision and recall.\n",
    "    :type alpha: float\n",
    "    :param beta: parameter for controlling shape of penalty as a function\n",
    "                 of as a function of fragmentation.\n",
    "    :type beta: float\n",
    "    :param gamma: relative weight assigned to fragmentation penality.\n",
    "    :type gamma: float\n",
    "    :return: The sentence-level METEOR score.\n",
    "    :rtype: float\n",
    "    \"\"\"\n",
    "    return max(\n",
    "        [\n",
    "            single_meteor_score(\n",
    "                reference,\n",
    "                hypothesis,\n",
    "                preprocess=preprocess,\n",
    "                stemmer=stemmer,\n",
    "                wordnet=wordnet,\n",
    "                alpha=alpha,\n",
    "                beta=beta,\n",
    "                gamma=gamma,\n",
    "            )\n",
    "            for reference in references\n",
    "        ]\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "840d2250",
   "metadata": {},
   "outputs": [],
   "source": [
    "meteorn_wlc=[]\n",
    "for i in range(len(auth_1)):\n",
    "    meteorn_wlc.append(round(meteorn_score_wlc([refs[i]],preds[i]),2))\n",
    "print(meteorn_wlc)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "19c576e5",
   "metadata": {},
   "outputs": [],
   "source": [
    "norm_mn_wlc = []\n",
    "\n",
    "max_mn_wlc=max(meteorn_wlc)\n",
    "min_mn_wlc=min(meteorn_wlc)\n",
    "\n",
    "for a in range(len(meteorn_wlc)):\n",
    "    norm_mn_wlc.append(round(((meteorn_wlc[a])/(max_mn_wlc)),2))\n",
    "print(norm_mn_wlc)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "58120f1f",
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'norm_mn_wlc' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-1-bda04f9b29a3>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[1;32mfrom\u001b[0m \u001b[0mscipy\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mstats\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[0mspearmanr\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      2\u001b[0m \u001b[1;31m# calculate spearman's correlation\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 3\u001b[1;33m \u001b[0mcoef\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mp\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mspearmanr\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mnorm_mn_wlc\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mavg_norm_score\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m      4\u001b[0m \u001b[0mprint\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m'Spearmans correlation coefficient: %.3f'\u001b[0m \u001b[1;33m%\u001b[0m \u001b[0mcoef\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      5\u001b[0m \u001b[1;31m# interpret the significance\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mNameError\u001b[0m: name 'norm_mn_wlc' is not defined"
     ]
    }
   ],
   "source": [
    "from scipy.stats import spearmanr\n",
    "# calculate spearman's correlation\n",
    "coef, p = spearmanr(norm_mn_wlc, avg_norm_score)\n",
    "print('Spearmans correlation coefficient: %.3f' % coef)\n",
    "# interpret the significance\n",
    "alpha = 0.05\n",
    "if p > alpha:\n",
    "\tprint('uncorrelated (fail to reject H0) p=%.3f' % p)\n",
    "else:\n",
    "\tprint('correlated (reject H0) p=%.3f' % p)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "daf2a57e",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
