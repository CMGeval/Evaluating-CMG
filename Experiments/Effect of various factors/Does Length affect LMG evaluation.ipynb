{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "883479b8",
   "metadata": {},
   "outputs": [],
   "source": [
    "import csv\n",
    "import json\n",
    "\n",
    "preds=[]\n",
    "refs=[]\n",
    "auth_1 =[]\n",
    "auth_2 = []\n",
    "auth_3 =[]\n",
    "with open('human_annotations.csv') as csvfile:\n",
    "    ader = csv.reader(csvfile)\n",
    "    for row in ader:\n",
    "        refs.append(row[1])\n",
    "        preds.append(row[0])\n",
    "        auth_1.append(row[2])\n",
    "        auth_2.append(row[3])\n",
    "        auth_3.append(row[4])\n",
    "        \n",
    "refs = refs[:100]\n",
    "preds = preds[:100]\n",
    "\n",
    "auth_1 = auth_1[:100]\n",
    "for i in range(0, len(auth_1)):\n",
    "    auth_1[i] = int(auth_1[i])\n",
    "#print(auth_1)\n",
    "\n",
    "auth_2 = auth_2[:100]\n",
    "for i in range(0, len(auth_2)):\n",
    "    auth_2[i] = int(auth_2[i])\n",
    "#print(auth_2)\n",
    "\n",
    "auth_3 = auth_3[:100]\n",
    "for i in range(0, len(auth_3)):\n",
    "    auth_3[i] = int(auth_3[i])\n",
    "#print(auth_3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "67f13825",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.5, 1.0, 0.75, 0.75, 0.75, 0.75, 1.0, 1.0, 0.5, 0.5, 0.75, 0.75, 0.5, 0.5, 0.5, 0.75, 0.25, 0.75, 0.75, 0.5, 0.5, 0.75, 0.75, 0.75, 0.75, 0.25, 0.75, 1.0, 0.25, 1.0, 0.75, 0.75, 0.75, 0.25, 0.75, 1.0, 0.75, 0.5, 0.75, 0.25, 0.0, 0.75, 0.5, 0.25, 1.0, 0.25, 0.5, 0.5, 0.0, 0.5, 0.5, 0.25, 0.5, 0.25, 0.25, 0.25, 0.25, 0.25, 0.5, 0.5, 0.5, 0.5, 0.5, 0.5, 0.25, 0.25, 0.5, 0.75, 0.5, 0.5, 0.25, 0.25, 0.75, 0.5, 0.5, 0.75, 0.5, 0.25, 0.25, 0.25, 0.25, 0.25, 0.5, 0.5, 0.5, 0.75, 0.25, 0.5, 0.75, 0.5, 0.0, 0.0, 0.25]\n"
     ]
    }
   ],
   "source": [
    "#normalised author 1 scores\n",
    "norm_auth_1 =[]\n",
    "max1 = max(auth_1)\n",
    "min1 = min(auth_1)\n",
    "\n",
    "for a in range(len(auth_1)):\n",
    "    norm_auth_1.append((auth_1[a])/(max1))\n",
    "#print(norm_auth_1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "c3d3528b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.25, 1.0, 0.75, 0.75, 0.75, 0.75, 1.0, 1.0, 0.25, 0.5, 0.5, 0.5, 0.25, 0.5, 0.5, 0.5, 0.0, 0.75, 0.75, 0.25, 0.25, 0.75, 0.5, 0.75, 0.5, 0.25, 0.75, 1.0, 0.0, 0.75, 0.75, 0.75, 0.75, 0.25, 0.5, 1.0, 0.5, 0.5, 0.5, 0.25, 0.0, 0.5, 0.5, 0.25, 1.0, 0.25, 0.25, 0.25, 0.0, 0.5, 0.25, 0.5, 0.25, 0.25, 0.25, 0.25, 0.25, 0.25, 0.25, 0.25, 0.5, 0.5, 0.25, 0.25, 0.25, 0.25, 0.25, 0.75, 0.25, 0.5, 0.25, 0.25, 0.5, 0.5, 0.25, 0.5, 0.5, 0.25, 0.25, 0.25, 0.25, 0.25, 0.25, 0.25, 0.5, 0.5, 0.25, 0.25, 0.5, 0.25, 0.0, 0.0, 0.5]\n"
     ]
    }
   ],
   "source": [
    "#normalised author 2 scores\n",
    "norm_auth_2 =[]\n",
    "max1 = max(auth_2)\n",
    "min1 = min(auth_2)\n",
    "\n",
    "for a in range(len(auth_2)):\n",
    "    norm_auth_2.append((auth_2[a])/(max1))\n",
    "#print(norm_auth_2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "0ebbbdb9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.5, 1.0, 0.75, 0.75, 0.75, 0.75, 1.0, 1.0, 0.5, 0.5, 0.75, 0.75, 0.5, 0.75, 0.5, 0.75, 0.25, 0.75, 0.75, 0.25, 0.5, 0.75, 0.75, 0.75, 0.75, 0.25, 0.75, 1.0, 0.0, 1.0, 0.75, 1.0, 0.75, 0.25, 0.75, 1.0, 0.75, 0.5, 0.75, 0.5, 0.0, 0.75, 0.5, 0.25, 1.0, 0.25, 0.5, 0.5, 0.0, 0.75, 0.5, 0.5, 0.5, 0.25, 0.25, 0.5, 0.5, 0.25, 0.5, 0.25, 0.5, 0.5, 0.5, 0.25, 0.25, 0.25, 0.5, 0.75, 0.5, 0.75, 0.25, 0.25, 0.75, 0.75, 0.5, 0.75, 0.5, 0.25, 0.25, 0.25, 0.25, 0.25, 0.25, 0.5, 0.5, 0.5, 0.25, 0.5, 0.75, 0.5, 0.0, 0.0, 0.25]\n"
     ]
    }
   ],
   "source": [
    "#normalised author 3 scores\n",
    "norm_auth_3 =[]\n",
    "max1 = max(auth_3)\n",
    "min1 = min(auth_3)\n",
    "\n",
    "for a in range(len(auth_3)):\n",
    "    norm_auth_3.append((auth_3[a])/(max1))\n",
    "#print(norm_auth_3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "a54d3627",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.42, 1.0, 0.75, 0.75, 0.75, 0.75, 1.0, 1.0, 0.42, 0.5, 0.67, 0.67, 0.42, 0.58, 0.5, 0.67, 0.17, 0.75, 0.75, 0.33, 0.42, 0.75, 0.67, 0.75, 0.67, 0.25, 0.75, 1.0, 0.08, 0.92, 0.75, 0.83, 0.75, 0.25, 0.67, 1.0, 0.67, 0.5, 0.67, 0.33, 0.0, 0.67, 0.5, 0.25, 1.0, 0.25, 0.42, 0.42, 0.0, 0.58, 0.42, 0.42, 0.42, 0.25, 0.25, 0.33, 0.33, 0.25, 0.42, 0.33, 0.5, 0.5, 0.42, 0.33, 0.25, 0.25, 0.42, 0.75, 0.42, 0.58, 0.25, 0.25, 0.67, 0.58, 0.42, 0.67, 0.5, 0.25, 0.25, 0.25, 0.25, 0.25, 0.33, 0.42, 0.5, 0.58, 0.25, 0.42, 0.67, 0.42, 0.0, 0.0, 0.33]\n"
     ]
    }
   ],
   "source": [
    "#Average normalised author scores\n",
    "avg_norm_score = []\n",
    "for i in range(len(auth_1)):\n",
    "    avg_norm_score.append(round((norm_auth_1[i]+norm_auth_2[i]+norm_auth_3[i])/3,2))\n",
    "#print(avg_norm_score)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "86907851",
   "metadata": {},
   "source": [
    "# BLEU-4 with Length Factor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "9cad2cbd",
   "metadata": {},
   "outputs": [],
   "source": [
    "import math\n",
    "import sys\n",
    "from fractions import Fraction\n",
    "import warnings\n",
    "from collections import Counter\n",
    "\n",
    "from nltk.util import ngrams\n",
    "\n",
    "def sentence_bleu(\n",
    "    references,\n",
    "    hypothesis,\n",
    "    weights=(0.25, 0.25, 0.25, 0.25),\n",
    "    smoothing_function=None,\n",
    "    auto_reweigh=False,\n",
    "):\n",
    "    \"\"\"\n",
    "    Calculate BLEU score (Bilingual Evaluation Understudy) from\n",
    "    Papineni, Kishore, Salim Roukos, Todd Ward, and Wei-Jing Zhu. 2002.\n",
    "    \"BLEU: a method for automatic evaluation of machine translation.\"\n",
    "    In Proceedings of ACL. http://www.aclweb.org/anthology/P02-1040.pdf\n",
    "\n",
    "    >>> hypothesis1 = ['It', 'is', 'a', 'guide', 'to', 'action', 'which',\n",
    "    ...               'ensures', 'that', 'the', 'military', 'always',\n",
    "    ...               'obeys', 'the', 'commands', 'of', 'the', 'party']\n",
    "\n",
    "    >>> hypothesis2 = ['It', 'is', 'to', 'insure', 'the', 'troops',\n",
    "    ...               'forever', 'hearing', 'the', 'activity', 'guidebook',\n",
    "    ...               'that', 'party', 'direct']\n",
    "\n",
    "    >>> reference1 = ['It', 'is', 'a', 'guide', 'to', 'action', 'that',\n",
    "    ...               'ensures', 'that', 'the', 'military', 'will', 'forever',\n",
    "    ...               'heed', 'Party', 'commands']\n",
    "\n",
    "    >>> reference2 = ['It', 'is', 'the', 'guiding', 'principle', 'which',\n",
    "    ...               'guarantees', 'the', 'military', 'forces', 'always',\n",
    "    ...               'being', 'under', 'the', 'command', 'of', 'the',\n",
    "    ...               'Party']\n",
    "\n",
    "    >>> reference3 = ['It', 'is', 'the', 'practical', 'guide', 'for', 'the',\n",
    "    ...               'army', 'always', 'to', 'heed', 'the', 'directions',\n",
    "    ...               'of', 'the', 'party']\n",
    "\n",
    "    >>> sentence_bleu([reference1, reference2, reference3], hypothesis1) # doctest: +ELLIPSIS\n",
    "    0.5045...\n",
    "\n",
    "    If there is no ngrams overlap for any order of n-grams, BLEU returns the\n",
    "    value 0. This is because the precision for the order of n-grams without\n",
    "    overlap is 0, and the geometric mean in the final BLEU score computation\n",
    "    multiplies the 0 with the precision of other n-grams. This results in 0\n",
    "    (independently of the precision of the othe n-gram orders). The following\n",
    "    example has zero 3-gram and 4-gram overlaps:\n",
    "\n",
    "    >>> round(sentence_bleu([reference1, reference2, reference3], hypothesis2),4) # doctest: +ELLIPSIS\n",
    "    0.0\n",
    "\n",
    "    To avoid this harsh behaviour when no ngram overlaps are found a smoothing\n",
    "    function can be used.\n",
    "\n",
    "    >>> chencherry = SmoothingFunction()\n",
    "    >>> sentence_bleu([reference1, reference2, reference3], hypothesis2,\n",
    "    ...     smoothing_function=chencherry.method1) # doctest: +ELLIPSIS\n",
    "    0.0370...\n",
    "\n",
    "    The default BLEU calculates a score for up to 4-grams using uniform\n",
    "    weights (this is called BLEU-4). To evaluate your translations with\n",
    "    higher/lower order ngrams, use customized weights. E.g. when accounting\n",
    "    for up to 5-grams with uniform weights (this is called BLEU-5) use:\n",
    "\n",
    "    >>> weights = (1./5., 1./5., 1./5., 1./5., 1./5.)\n",
    "    >>> sentence_bleu([reference1, reference2, reference3], hypothesis1, weights) # doctest: +ELLIPSIS\n",
    "    0.3920...\n",
    "\n",
    "    :param references: reference sentences\n",
    "    :type references: list(list(str))\n",
    "    :param hypothesis: a hypothesis sentence\n",
    "    :type hypothesis: list(str)\n",
    "    :param weights: weights for unigrams, bigrams, trigrams and so on\n",
    "    :type weights: list(float)\n",
    "    :param smoothing_function:\n",
    "    :type smoothing_function: SmoothingFunction\n",
    "    :param auto_reweigh: Option to re-normalize the weights uniformly.\n",
    "    :type auto_reweigh: bool\n",
    "    :return: The sentence-level BLEU score.\n",
    "    :rtype: float\n",
    "    \"\"\"\n",
    "    return corpus_bleu(\n",
    "        [references], [hypothesis], weights, smoothing_function, auto_reweigh\n",
    "    )\n",
    "\n",
    "\n",
    "\n",
    "def corpus_bleu(\n",
    "    list_of_references,\n",
    "    hypotheses,\n",
    "    weights=(0.25, 0.25, 0.25, 0.25),\n",
    "    smoothing_function=None,\n",
    "    auto_reweigh=False,\n",
    "):\n",
    "    \"\"\"\n",
    "    Calculate a single corpus-level BLEU score (aka. system-level BLEU) for all\n",
    "    the hypotheses and their respective references.\n",
    "\n",
    "    Instead of averaging the sentence level BLEU scores (i.e. macro-average\n",
    "    precision), the original BLEU metric (Papineni et al. 2002) accounts for\n",
    "    the micro-average precision (i.e. summing the numerators and denominators\n",
    "    for each hypothesis-reference(s) pairs before the division).\n",
    "\n",
    "    >>> hyp1 = ['It', 'is', 'a', 'guide', 'to', 'action', 'which',\n",
    "    ...         'ensures', 'that', 'the', 'military', 'always',\n",
    "    ...         'obeys', 'the', 'commands', 'of', 'the', 'party']\n",
    "    >>> ref1a = ['It', 'is', 'a', 'guide', 'to', 'action', 'that',\n",
    "    ...          'ensures', 'that', 'the', 'military', 'will', 'forever',\n",
    "    ...          'heed', 'Party', 'commands']\n",
    "    >>> ref1b = ['It', 'is', 'the', 'guiding', 'principle', 'which',\n",
    "    ...          'guarantees', 'the', 'military', 'forces', 'always',\n",
    "    ...          'being', 'under', 'the', 'command', 'of', 'the', 'Party']\n",
    "    >>> ref1c = ['It', 'is', 'the', 'practical', 'guide', 'for', 'the',\n",
    "    ...          'army', 'always', 'to', 'heed', 'the', 'directions',\n",
    "    ...          'of', 'the', 'party']\n",
    "\n",
    "    >>> hyp2 = ['he', 'read', 'the', 'book', 'because', 'he', 'was',\n",
    "    ...         'interested', 'in', 'world', 'history']\n",
    "    >>> ref2a = ['he', 'was', 'interested', 'in', 'world', 'history',\n",
    "    ...          'because', 'he', 'read', 'the', 'book']\n",
    "\n",
    "    >>> list_of_references = [[ref1a, ref1b, ref1c], [ref2a]]\n",
    "    >>> hypotheses = [hyp1, hyp2]\n",
    "    >>> corpus_bleu(list_of_references, hypotheses) # doctest: +ELLIPSIS\n",
    "    0.5920...\n",
    "\n",
    "    The example below show that corpus_bleu() is different from averaging\n",
    "    sentence_bleu() for hypotheses\n",
    "\n",
    "    >>> score1 = sentence_bleu([ref1a, ref1b, ref1c], hyp1)\n",
    "    >>> score2 = sentence_bleu([ref2a], hyp2)\n",
    "    >>> (score1 + score2) / 2 # doctest: +ELLIPSIS\n",
    "    0.6223...\n",
    "\n",
    "    :param list_of_references: a corpus of lists of reference sentences, w.r.t. hypotheses\n",
    "    :type list_of_references: list(list(list(str)))\n",
    "    :param hypotheses: a list of hypothesis sentences\n",
    "    :type hypotheses: list(list(str))\n",
    "    :param weights: weights for unigrams, bigrams, trigrams and so on\n",
    "    :type weights: list(float)\n",
    "    :param smoothing_function:\n",
    "    :type smoothing_function: SmoothingFunction\n",
    "    :param auto_reweigh: Option to re-normalize the weights uniformly.\n",
    "    :type auto_reweigh: bool\n",
    "    :return: The corpus-level BLEU score.\n",
    "    :rtype: float\n",
    "    \"\"\"\n",
    "    # Before proceeding to compute BLEU, perform sanity checks.\n",
    "\n",
    "    p_numerators = Counter()  # Key = ngram order, and value = no. of ngram matches.\n",
    "    p_denominators = Counter()  # Key = ngram order, and value = no. of ngram in ref.\n",
    "    hyp_lengths, ref_lengths = 0, 0\n",
    "\n",
    "    assert len(list_of_references) == len(hypotheses), (\n",
    "        \"The number of hypotheses and their reference(s) should be the \" \"same \"\n",
    "    )\n",
    "\n",
    "    # Iterate through each hypothesis and their corresponding references.\n",
    "    for references, hypothesis in zip(list_of_references, hypotheses):\n",
    "        # For each order of ngram, calculate the numerator and\n",
    "        # denominator for the corpus-level modified precision.\n",
    "        for i, _ in enumerate(weights, start=1):\n",
    "            p_i = modified_precision(references, hypothesis, i)\n",
    "            p_numerators[i] += p_i.numerator\n",
    "            p_denominators[i] += p_i.denominator\n",
    "\n",
    "        # Calculate the hypothesis length and the closest reference length.\n",
    "        # Adds them to the corpus-level hypothesis and reference counts.\n",
    "        hyp_len = len(hypothesis)\n",
    "        hyp_lengths += hyp_len\n",
    "        ref_lengths += closest_ref_length(references, hyp_len)\n",
    "\n",
    "    # Calculate corpus-level brevity penalty.\n",
    "    bp = brevity_penalty(ref_lengths, hyp_lengths)\n",
    "\n",
    "    # Uniformly re-weighting based on maximum hypothesis lengths if largest\n",
    "    # order of n-grams < 4 and weights is set at default.\n",
    "    if auto_reweigh:\n",
    "        if hyp_lengths < 4 and weights == (0.25, 0.25, 0.25, 0.25):\n",
    "            weights = (1 / hyp_lengths,) * hyp_lengths\n",
    "\n",
    "    # Collects the various precision values for the different ngram orders.\n",
    "    p_n = [\n",
    "        Fraction(p_numerators[i], p_denominators[i], _normalize=False)\n",
    "        for i, _ in enumerate(weights, start=1)\n",
    "    ]\n",
    "\n",
    "    # Returns 0 if there's no matching n-grams\n",
    "    # We only need to check for p_numerators[1] == 0, since if there's\n",
    "    # no unigrams, there won't be any higher order ngrams.\n",
    "    if p_numerators[1] == 0:\n",
    "        return 0\n",
    "\n",
    "    # If there's no smoothing, set use method0 from SmoothinFunction class.\n",
    "    if not smoothing_function:\n",
    "        smoothing_function = SmoothingFunction().method0\n",
    "    # Smoothen the modified precision.\n",
    "    # Note: smoothing_function() may convert values into floats;\n",
    "    #       it tries to retain the Fraction object as much as the\n",
    "    #       smoothing method allows.\n",
    "    p_n = smoothing_function(\n",
    "        p_n, references=references, hypothesis=hypothesis, hyp_len=hyp_lengths\n",
    "    )\n",
    "    s = (w_i * math.log(p_i) for w_i, p_i in zip(weights, p_n))\n",
    "    s = bp * math.exp(math.fsum(s))\n",
    "    return s\n",
    "\n",
    "\n",
    "\n",
    "def modified_precision(references, hypothesis, n):\n",
    "    \"\"\"\n",
    "    Calculate modified ngram precision.\n",
    "\n",
    "    The normal precision method may lead to some wrong translations with\n",
    "    high-precision, e.g., the translation, in which a word of reference\n",
    "    repeats several times, has very high precision.\n",
    "\n",
    "    This function only returns the Fraction object that contains the numerator\n",
    "    and denominator necessary to calculate the corpus-level precision.\n",
    "    To calculate the modified precision for a single pair of hypothesis and\n",
    "    references, cast the Fraction object into a float.\n",
    "\n",
    "    The famous \"the the the ... \" example shows that you can get BLEU precision\n",
    "    by duplicating high frequency words.\n",
    "\n",
    "        >>> reference1 = 'the cat is on the mat'.split()\n",
    "        >>> reference2 = 'there is a cat on the mat'.split()\n",
    "        >>> hypothesis1 = 'the the the the the the the'.split()\n",
    "        >>> references = [reference1, reference2]\n",
    "        >>> float(modified_precision(references, hypothesis1, n=1)) # doctest: +ELLIPSIS\n",
    "        0.2857...\n",
    "\n",
    "    In the modified n-gram precision, a reference word will be considered\n",
    "    exhausted after a matching hypothesis word is identified, e.g.\n",
    "\n",
    "        >>> reference1 = ['It', 'is', 'a', 'guide', 'to', 'action', 'that',\n",
    "        ...               'ensures', 'that', 'the', 'military', 'will',\n",
    "        ...               'forever', 'heed', 'Party', 'commands']\n",
    "        >>> reference2 = ['It', 'is', 'the', 'guiding', 'principle', 'which',\n",
    "        ...               'guarantees', 'the', 'military', 'forces', 'always',\n",
    "        ...               'being', 'under', 'the', 'command', 'of', 'the',\n",
    "        ...               'Party']\n",
    "        >>> reference3 = ['It', 'is', 'the', 'practical', 'guide', 'for', 'the',\n",
    "        ...               'army', 'always', 'to', 'heed', 'the', 'directions',\n",
    "        ...               'of', 'the', 'party']\n",
    "        >>> hypothesis = 'of the'.split()\n",
    "        >>> references = [reference1, reference2, reference3]\n",
    "        >>> float(modified_precision(references, hypothesis, n=1))\n",
    "        1.0\n",
    "        >>> float(modified_precision(references, hypothesis, n=2))\n",
    "        1.0\n",
    "\n",
    "    An example of a normal machine translation hypothesis:\n",
    "\n",
    "        >>> hypothesis1 = ['It', 'is', 'a', 'guide', 'to', 'action', 'which',\n",
    "        ...               'ensures', 'that', 'the', 'military', 'always',\n",
    "        ...               'obeys', 'the', 'commands', 'of', 'the', 'party']\n",
    "\n",
    "        >>> hypothesis2 = ['It', 'is', 'to', 'insure', 'the', 'troops',\n",
    "        ...               'forever', 'hearing', 'the', 'activity', 'guidebook',\n",
    "        ...               'that', 'party', 'direct']\n",
    "\n",
    "        >>> reference1 = ['It', 'is', 'a', 'guide', 'to', 'action', 'that',\n",
    "        ...               'ensures', 'that', 'the', 'military', 'will',\n",
    "        ...               'forever', 'heed', 'Party', 'commands']\n",
    "\n",
    "        >>> reference2 = ['It', 'is', 'the', 'guiding', 'principle', 'which',\n",
    "        ...               'guarantees', 'the', 'military', 'forces', 'always',\n",
    "        ...               'being', 'under', 'the', 'command', 'of', 'the',\n",
    "        ...               'Party']\n",
    "\n",
    "        >>> reference3 = ['It', 'is', 'the', 'practical', 'guide', 'for', 'the',\n",
    "        ...               'army', 'always', 'to', 'heed', 'the', 'directions',\n",
    "        ...               'of', 'the', 'party']\n",
    "        >>> references = [reference1, reference2, reference3]\n",
    "        >>> float(modified_precision(references, hypothesis1, n=1)) # doctest: +ELLIPSIS\n",
    "        0.9444...\n",
    "        >>> float(modified_precision(references, hypothesis2, n=1)) # doctest: +ELLIPSIS\n",
    "        0.5714...\n",
    "        >>> float(modified_precision(references, hypothesis1, n=2)) # doctest: +ELLIPSIS\n",
    "        0.5882352941176471\n",
    "        >>> float(modified_precision(references, hypothesis2, n=2)) # doctest: +ELLIPSIS\n",
    "        0.07692...\n",
    "\n",
    "\n",
    "    :param references: A list of reference translations.\n",
    "    :type references: list(list(str))\n",
    "    :param hypothesis: A hypothesis translation.\n",
    "    :type hypothesis: list(str)\n",
    "    :param n: The ngram order.\n",
    "    :type n: int\n",
    "    :return: BLEU's modified precision for the nth order ngram.\n",
    "    :rtype: Fraction\n",
    "    \"\"\"\n",
    "    # Extracts all ngrams in hypothesis\n",
    "    # Set an empty Counter if hypothesis is empty.\n",
    "    counts = Counter(ngrams(hypothesis, n)) if len(hypothesis) >= n else Counter()\n",
    "    # Extract a union of references' counts.\n",
    "    # max_counts = reduce(or_, [Counter(ngrams(ref, n)) for ref in references])\n",
    "    max_counts = {}\n",
    "    for reference in references:\n",
    "        reference_counts = (\n",
    "            Counter(ngrams(reference, n)) if len(reference) >= n else Counter()\n",
    "        )\n",
    "        for ngram in counts:\n",
    "            max_counts[ngram] = max(max_counts.get(ngram, 0), reference_counts[ngram])\n",
    "\n",
    "    # Assigns the intersection between hypothesis and references' counts.\n",
    "    clipped_counts = {\n",
    "        ngram: min(count, max_counts[ngram]) for ngram, count in counts.items()\n",
    "    }\n",
    "\n",
    "    numerator = sum(clipped_counts.values())\n",
    "    # Ensures that denominator is minimum 1 to avoid ZeroDivisionError.\n",
    "    # Usually this happens when the ngram order is > len(reference).\n",
    "    denominator = max(1, sum(counts.values()))\n",
    "\n",
    "    return Fraction(numerator, denominator, _normalize=False)\n",
    "\n",
    "\n",
    "\n",
    "def closest_ref_length(references, hyp_len):\n",
    "    \"\"\"\n",
    "    This function finds the reference that is the closest length to the\n",
    "    hypothesis. The closest reference length is referred to as *r* variable\n",
    "    from the brevity penalty formula in Papineni et. al. (2002)\n",
    "\n",
    "    :param references: A list of reference translations.\n",
    "    :type references: list(list(str))\n",
    "    :param hyp_len: The length of the hypothesis.\n",
    "    :type hyp_len: int\n",
    "    :return: The length of the reference that's closest to the hypothesis.\n",
    "    :rtype: int\n",
    "    \"\"\"\n",
    "    ref_lens = (len(reference) for reference in references)\n",
    "    closest_ref_len = min(\n",
    "        ref_lens, key=lambda ref_len: (abs(ref_len - hyp_len), ref_len)\n",
    "    )\n",
    "    return closest_ref_len\n",
    "\n",
    "\n",
    "\n",
    "def brevity_penalty(closest_ref_len, hyp_len):\n",
    "    \"\"\"\n",
    "    Calculate brevity penalty.\n",
    "\n",
    "    As the modified n-gram precision still has the problem from the short\n",
    "    length sentence, brevity penalty is used to modify the overall BLEU\n",
    "    score according to length.\n",
    "\n",
    "    An example from the paper. There are three references with length 12, 15\n",
    "    and 17. And a concise hypothesis of the length 12. The brevity penalty is 1.\n",
    "\n",
    "        >>> reference1 = list('aaaaaaaaaaaa')      # i.e. ['a'] * 12\n",
    "        >>> reference2 = list('aaaaaaaaaaaaaaa')   # i.e. ['a'] * 15\n",
    "        >>> reference3 = list('aaaaaaaaaaaaaaaaa') # i.e. ['a'] * 17\n",
    "        >>> hypothesis = list('aaaaaaaaaaaa')      # i.e. ['a'] * 12\n",
    "        >>> references = [reference1, reference2, reference3]\n",
    "        >>> hyp_len = len(hypothesis)\n",
    "        >>> closest_ref_len =  closest_ref_length(references, hyp_len)\n",
    "        >>> brevity_penalty(closest_ref_len, hyp_len)\n",
    "        1.0\n",
    "\n",
    "    In case a hypothesis translation is shorter than the references, penalty is\n",
    "    applied.\n",
    "\n",
    "        >>> references = [['a'] * 28, ['a'] * 28]\n",
    "        >>> hypothesis = ['a'] * 12\n",
    "        >>> hyp_len = len(hypothesis)\n",
    "        >>> closest_ref_len =  closest_ref_length(references, hyp_len)\n",
    "        >>> brevity_penalty(closest_ref_len, hyp_len)\n",
    "        0.2635971381157267\n",
    "\n",
    "    The length of the closest reference is used to compute the penalty. If the\n",
    "    length of a hypothesis is 12, and the reference lengths are 13 and 2, the\n",
    "    penalty is applied because the hypothesis length (12) is less then the\n",
    "    closest reference length (13).\n",
    "\n",
    "        >>> references = [['a'] * 13, ['a'] * 2]\n",
    "        >>> hypothesis = ['a'] * 12\n",
    "        >>> hyp_len = len(hypothesis)\n",
    "        >>> closest_ref_len =  closest_ref_length(references, hyp_len)\n",
    "        >>> brevity_penalty(closest_ref_len, hyp_len) # doctest: +ELLIPSIS\n",
    "        0.9200...\n",
    "\n",
    "    The brevity penalty doesn't depend on reference order. More importantly,\n",
    "    when two reference sentences are at the same distance, the shortest\n",
    "    reference sentence length is used.\n",
    "\n",
    "        >>> references = [['a'] * 13, ['a'] * 11]\n",
    "        >>> hypothesis = ['a'] * 12\n",
    "        >>> hyp_len = len(hypothesis)\n",
    "        >>> closest_ref_len =  closest_ref_length(references, hyp_len)\n",
    "        >>> bp1 = brevity_penalty(closest_ref_len, hyp_len)\n",
    "        >>> hyp_len = len(hypothesis)\n",
    "        >>> closest_ref_len =  closest_ref_length(reversed(references), hyp_len)\n",
    "        >>> bp2 = brevity_penalty(closest_ref_len, hyp_len)\n",
    "        >>> bp1 == bp2 == 1\n",
    "        True\n",
    "\n",
    "    A test example from mteval-v13a.pl (starting from the line 705):\n",
    "\n",
    "        >>> references = [['a'] * 11, ['a'] * 8]\n",
    "        >>> hypothesis = ['a'] * 7\n",
    "        >>> hyp_len = len(hypothesis)\n",
    "        >>> closest_ref_len =  closest_ref_length(references, hyp_len)\n",
    "        >>> brevity_penalty(closest_ref_len, hyp_len) # doctest: +ELLIPSIS\n",
    "        0.8668...\n",
    "\n",
    "        >>> references = [['a'] * 11, ['a'] * 8, ['a'] * 6, ['a'] * 7]\n",
    "        >>> hypothesis = ['a'] * 7\n",
    "        >>> hyp_len = len(hypothesis)\n",
    "        >>> closest_ref_len =  closest_ref_length(references, hyp_len)\n",
    "        >>> brevity_penalty(closest_ref_len, hyp_len)\n",
    "        1.0\n",
    "\n",
    "    :param hyp_len: The length of the hypothesis for a single sentence OR the\n",
    "    sum of all the hypotheses' lengths for a corpus\n",
    "    :type hyp_len: int\n",
    "    :param closest_ref_len: The length of the closest reference for a single\n",
    "    hypothesis OR the sum of all the closest references for every hypotheses.\n",
    "    :type closest_ref_len: int\n",
    "    :return: BLEU's brevity penalty.\n",
    "    :rtype: float\n",
    "    \"\"\"\n",
    "    if hyp_len > closest_ref_len:\n",
    "        return 1\n",
    "    # If hypothesis is empty, brevity penalty = 0 should result in BLEU = 0.0\n",
    "    elif hyp_len == 0:\n",
    "        return 0\n",
    "    else:\n",
    "        return math.exp(1 - closest_ref_len / hyp_len)\n",
    "\n",
    "\n",
    "\n",
    "class SmoothingFunction:\n",
    "    \"\"\"\n",
    "    This is an implementation of the smoothing techniques\n",
    "    for segment-level BLEU scores that was presented in\n",
    "    Boxing Chen and Collin Cherry (2014) A Systematic Comparison of\n",
    "    Smoothing Techniques for Sentence-Level BLEU. In WMT14.\n",
    "    http://acl2014.org/acl2014/W14-33/pdf/W14-3346.pdf\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, epsilon=0.1, alpha=5, k=5):\n",
    "        \"\"\"\n",
    "        This will initialize the parameters required for the various smoothing\n",
    "        techniques, the default values are set to the numbers used in the\n",
    "        experiments from Chen and Cherry (2014).\n",
    "\n",
    "        >>> hypothesis1 = ['It', 'is', 'a', 'guide', 'to', 'action', 'which', 'ensures',\n",
    "        ...                 'that', 'the', 'military', 'always', 'obeys', 'the',\n",
    "        ...                 'commands', 'of', 'the', 'party']\n",
    "        >>> reference1 = ['It', 'is', 'a', 'guide', 'to', 'action', 'that', 'ensures',\n",
    "        ...               'that', 'the', 'military', 'will', 'forever', 'heed',\n",
    "        ...               'Party', 'commands']\n",
    "\n",
    "        >>> chencherry = SmoothingFunction()\n",
    "        >>> print(sentence_bleu([reference1], hypothesis1)) # doctest: +ELLIPSIS\n",
    "        0.4118...\n",
    "        >>> print(sentence_bleu([reference1], hypothesis1, smoothing_function=chencherry.method0)) # doctest: +ELLIPSIS\n",
    "        0.4118...\n",
    "        >>> print(sentence_bleu([reference1], hypothesis1, smoothing_function=chencherry.method1)) # doctest: +ELLIPSIS\n",
    "        0.4118...\n",
    "        >>> print(sentence_bleu([reference1], hypothesis1, smoothing_function=chencherry.method2)) # doctest: +ELLIPSIS\n",
    "        0.4489...\n",
    "        >>> print(sentence_bleu([reference1], hypothesis1, smoothing_function=chencherry.method3)) # doctest: +ELLIPSIS\n",
    "        0.4118...\n",
    "        >>> print(sentence_bleu([reference1], hypothesis1, smoothing_function=chencherry.method4)) # doctest: +ELLIPSIS\n",
    "        0.4118...\n",
    "        >>> print(sentence_bleu([reference1], hypothesis1, smoothing_function=chencherry.method5)) # doctest: +ELLIPSIS\n",
    "        0.4905...\n",
    "        >>> print(sentence_bleu([reference1], hypothesis1, smoothing_function=chencherry.method6)) # doctest: +ELLIPSIS\n",
    "        0.4135...\n",
    "        >>> print(sentence_bleu([reference1], hypothesis1, smoothing_function=chencherry.method7)) # doctest: +ELLIPSIS\n",
    "        0.4905...\n",
    "\n",
    "        :param epsilon: the epsilon value use in method 1\n",
    "        :type epsilon: float\n",
    "        :param alpha: the alpha value use in method 6\n",
    "        :type alpha: int\n",
    "        :param k: the k value use in method 4\n",
    "        :type k: int\n",
    "        \"\"\"\n",
    "        self.epsilon = epsilon\n",
    "        self.alpha = alpha\n",
    "        self.k = k\n",
    "\n",
    "    def method0(self, p_n, *args, **kwargs):\n",
    "        \"\"\"\n",
    "        No smoothing.\n",
    "        \"\"\"\n",
    "        p_n_new = []\n",
    "        for i, p_i in enumerate(p_n):\n",
    "            if p_i.numerator != 0:\n",
    "                p_n_new.append(p_i)\n",
    "            else:\n",
    "                _msg = str(\n",
    "                    \"\\nThe hypothesis contains 0 counts of {}-gram overlaps.\\n\"\n",
    "                    \"Therefore the BLEU score evaluates to 0, independently of\\n\"\n",
    "                    \"how many N-gram overlaps of lower order it contains.\\n\"\n",
    "                    \"Consider using lower n-gram order or use \"\n",
    "                    \"SmoothingFunction()\"\n",
    "                ).format(i + 1)\n",
    "                warnings.warn(_msg)\n",
    "                # When numerator==0 where denonminator==0 or !=0, the result\n",
    "                # for the precision score should be equal to 0 or undefined.\n",
    "                # Due to BLEU geometric mean computation in logarithm space,\n",
    "                # we we need to take the return sys.float_info.min such that\n",
    "                # math.log(sys.float_info.min) returns a 0 precision score.\n",
    "                p_n_new.append(sys.float_info.min)\n",
    "        return p_n_new\n",
    "\n",
    "\n",
    "    def method1(self, p_n, *args, **kwargs):\n",
    "        \"\"\"\n",
    "        Smoothing method 1: Add *epsilon* counts to precision with 0 counts.\n",
    "        \"\"\"\n",
    "        return [\n",
    "            (p_i.numerator + self.epsilon) / p_i.denominator\n",
    "            if p_i.numerator == 0\n",
    "            else p_i\n",
    "            for p_i in p_n\n",
    "        ]\n",
    "\n",
    "\n",
    "    def method2(self, p_n, *args, **kwargs):\n",
    "        \"\"\"\n",
    "        Smoothing method 2: Add 1 to both numerator and denominator from\n",
    "        Chin-Yew Lin and Franz Josef Och (2004) ORANGE: a Method for\n",
    "        Evaluating Automatic Evaluation Metrics for Machine Translation.\n",
    "        In COLING 2004.\n",
    "        \"\"\"\n",
    "        return [\n",
    "            Fraction(p_n[i].numerator + 1, p_n[i].denominator + 1, _normalize=False)\n",
    "            if i != 0 else p_n[0]\n",
    "            for i in range(len(p_n))\n",
    "        ]\n",
    "\n",
    "\n",
    "    def method3(self, p_n, *args, **kwargs):\n",
    "        \"\"\"\n",
    "        Smoothing method 3: NIST geometric sequence smoothing\n",
    "        The smoothing is computed by taking 1 / ( 2^k ), instead of 0, for each\n",
    "        precision score whose matching n-gram count is null.\n",
    "        k is 1 for the first 'n' value for which the n-gram match count is null/\n",
    "        For example, if the text contains:\n",
    "         - one 2-gram match\n",
    "         - and (consequently) two 1-gram matches\n",
    "        the n-gram count for each individual precision score would be:\n",
    "         - n=1  =>  prec_count = 2     (two unigrams)\n",
    "         - n=2  =>  prec_count = 1     (one bigram)\n",
    "         - n=3  =>  prec_count = 1/2   (no trigram,  taking 'smoothed' value of 1 / ( 2^k ), with k=1)\n",
    "         - n=4  =>  prec_count = 1/4   (no fourgram, taking 'smoothed' value of 1 / ( 2^k ), with k=2)\n",
    "        \"\"\"\n",
    "        incvnt = 1  # From the mteval-v13a.pl, it's referred to as k.\n",
    "        for i, p_i in enumerate(p_n):\n",
    "            if p_i.numerator == 0:\n",
    "                p_n[i] = 1 / (2 ** incvnt * p_i.denominator)\n",
    "                incvnt += 1\n",
    "        return p_n\n",
    "\n",
    "\n",
    "    def method4(self, p_n, references, hypothesis, hyp_len=None, *args, **kwargs):\n",
    "        \"\"\"\n",
    "        Smoothing method 4:\n",
    "        Shorter translations may have inflated precision values due to having\n",
    "        smaller denominators; therefore, we give them proportionally\n",
    "        smaller smoothed counts. Instead of scaling to 1/(2^k), Chen and Cherry\n",
    "        suggests dividing by 1/ln(len(T)), where T is the length of the translation.\n",
    "        \"\"\"\n",
    "        incvnt = 1\n",
    "        hyp_len = hyp_len if hyp_len else len(hypothesis.split())\n",
    "        for i, p_i in enumerate(p_n):\n",
    "            if p_i.numerator == 0 and hyp_len >1:\n",
    "#                 incvnt = i + 1 * self.k / math.log(\n",
    "#                     hyp_len\n",
    "#                 )  # Note that this K is different from the K from NIST.\n",
    "#                 p_n[i] = incvnt / p_i.denominator\\\n",
    "                numerator = 1 / (2 ** incvnt * self.k / math.log(hyp_len))\n",
    "                p_n[i] = numerator / p_i.denominator\n",
    "                incvnt += 1\n",
    "        return p_n\n",
    "\n",
    "\n",
    "    def method5(self, p_n, references, hypothesis, hyp_len=None, *args, **kwargs):\n",
    "        \"\"\"\n",
    "        Smoothing method 5:\n",
    "        The matched counts for similar values of n should be similar. To a\n",
    "        calculate the n-gram matched count, it averages the n−1, n and n+1 gram\n",
    "        matched counts.\n",
    "        \"\"\"\n",
    "        hyp_len = hyp_len if hyp_len else len(hypothesis)\n",
    "        m = {}\n",
    "        # Requires an precision value for an addition ngram order.\n",
    "        p_n_plus1 = p_n + [modified_precision(references, hypothesis, 5)]\n",
    "        m[-1] = p_n[0] + 1\n",
    "        for i, p_i in enumerate(p_n):\n",
    "            p_n[i] = (m[i - 1] + p_i + p_n_plus1[i + 1]) / 3\n",
    "            m[i] = p_n[i]\n",
    "        return p_n\n",
    "\n",
    "\n",
    "    def method6(self, p_n, references, hypothesis, hyp_len=None, *args, **kwargs):\n",
    "        \"\"\"\n",
    "        Smoothing method 6:\n",
    "        Interpolates the maximum likelihood estimate of the precision *p_n* with\n",
    "        a prior estimate *pi0*. The prior is estimated by assuming that the ratio\n",
    "        between pn and pn−1 will be the same as that between pn−1 and pn−2; from\n",
    "        Gao and He (2013) Training MRF-Based Phrase Translation Models using\n",
    "        Gradient Ascent. In NAACL.\n",
    "        \"\"\"\n",
    "        hyp_len = hyp_len if hyp_len else len(hypothesis)\n",
    "        # This smoothing only works when p_1 and p_2 is non-zero.\n",
    "        # Raise an error with an appropriate message when the input is too short\n",
    "        # to use this smoothing technique.\n",
    "        assert p_n[2], \"This smoothing method requires non-zero precision for bigrams.\"\n",
    "        for i, p_i in enumerate(p_n):\n",
    "            if i in [0, 1]:  # Skips the first 2 orders of ngrams.\n",
    "                continue\n",
    "            else:\n",
    "                pi0 = 0 if p_n[i - 2] == 0 else p_n[i - 1] ** 2 / p_n[i - 2]\n",
    "                # No. of ngrams in translation that matches the reference.\n",
    "                m = p_i.numerator\n",
    "                # No. of ngrams in translation.\n",
    "                l = sum(1 for _ in ngrams(hypothesis, i + 1))\n",
    "                # Calculates the interpolated precision.\n",
    "                p_n[i] = (m + self.alpha * pi0) / (l + self.alpha)\n",
    "        return p_n\n",
    "\n",
    "\n",
    "    def method7(self, p_n, references, hypothesis, hyp_len=None, *args, **kwargs):\n",
    "        \"\"\"\n",
    "        Smoothing method 7:\n",
    "        Interpolates methods 4 and 5.\n",
    "        \"\"\"\n",
    "        hyp_len = hyp_len if hyp_len else len(hypothesis)\n",
    "        p_n = self.method4(p_n, references, hypothesis, hyp_len)\n",
    "        p_n = self.method5(p_n, references, hypothesis, hyp_len)\n",
    "        return p_n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "ea208f5a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.72, 0.55, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.13, 0.36, 0.29, 0.26, 0.75, 0.49, 0.0, 0.0, 0.45, 0.49, 0.0, 0.26, 0.0, 0.0, 0.71, 0.47, 0.64, 0.53, 0.52, 0.65, 0.0, 0.0, 0.92, 0.0, 0.0, 0.57, 0.63, 0.0, 0.52, 0.36, 0.43, 0.79, 0.0, 0.0, 0.0, 0.17, 0.27, 0.21, 0.49, 0.2, 0.29, 0.26, 0.31, 0.34, 0.12, 0.27, 0.12, 0.35, 0.33, 0.22, 0.0, 0.36, 0.0, 0.0, 0.49, 0.0, 0.0, 0.32, 0.35, 0.0, 0.44, 0.15, 0.49, 0.53, 0.32, 0.27, 0.3, 0.35, 0.22, 0.2, 0.17, 0.42, 0.24, 0.32, 0.17, 0.44, 0.0, 0.0, 0.0, 0.25]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "<ipython-input-6-551caa4dd498>:509: UserWarning: \n",
      "The hypothesis contains 0 counts of 2-gram overlaps.\n",
      "Therefore the BLEU score evaluates to 0, independently of\n",
      "how many N-gram overlaps of lower order it contains.\n",
      "Consider using lower n-gram order or use SmoothingFunction()\n",
      "  warnings.warn(_msg)\n",
      "<ipython-input-6-551caa4dd498>:509: UserWarning: \n",
      "The hypothesis contains 0 counts of 3-gram overlaps.\n",
      "Therefore the BLEU score evaluates to 0, independently of\n",
      "how many N-gram overlaps of lower order it contains.\n",
      "Consider using lower n-gram order or use SmoothingFunction()\n",
      "  warnings.warn(_msg)\n",
      "<ipython-input-6-551caa4dd498>:509: UserWarning: \n",
      "The hypothesis contains 0 counts of 4-gram overlaps.\n",
      "Therefore the BLEU score evaluates to 0, independently of\n",
      "how many N-gram overlaps of lower order it contains.\n",
      "Consider using lower n-gram order or use SmoothingFunction()\n",
      "  warnings.warn(_msg)\n"
     ]
    }
   ],
   "source": [
    "b4_wl=[]\n",
    "\n",
    "for i in range(len(auth_1)):\n",
    "    b4_wl.append(round(sentence_bleu([refs[i]], preds[i]),2))\n",
    "print(b4_wl)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "0d4c408a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.78, 0.6, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.14, 0.39, 0.32, 0.28, 0.82, 0.53, 0.0, 0.0, 0.49, 0.53, 0.0, 0.28, 0.0, 0.0, 0.77, 0.51, 0.7, 0.58, 0.57, 0.71, 0.0, 0.0, 1.0, 0.0, 0.0, 0.62, 0.68, 0.0, 0.57, 0.39, 0.47, 0.86, 0.0, 0.0, 0.0, 0.18, 0.29, 0.23, 0.53, 0.22, 0.32, 0.28, 0.34, 0.37, 0.13, 0.29, 0.13, 0.38, 0.36, 0.24, 0.0, 0.39, 0.0, 0.0, 0.53, 0.0, 0.0, 0.35, 0.38, 0.0, 0.48, 0.16, 0.53, 0.58, 0.35, 0.29, 0.33, 0.38, 0.24, 0.22, 0.18, 0.46, 0.26, 0.35, 0.18, 0.48, 0.0, 0.0, 0.0, 0.27]\n"
     ]
    }
   ],
   "source": [
    "norm_b4_wl = []\n",
    "\n",
    "max_b4=max(b4_wl)\n",
    "min_b4=min(b4_wl)\n",
    "\n",
    "for a in range(len(b4_wl)):\n",
    "    norm_b4_wl.append(round(((b4_wl[a])/(max_b4)),2))\n",
    "print(norm_b4_wl)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "87ea9d83",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Spearmans correlation coefficient: 0.078\n",
      "uncorrelated (fail to reject H0) p=0.439\n"
     ]
    }
   ],
   "source": [
    "from scipy.stats import spearmanr\n",
    "# calculate spearman's correlation\n",
    "coef, p = spearmanr(norm_b4_wl, avg_norm_score)\n",
    "print('Spearmans correlation coefficient: %.3f' % coef)\n",
    "# interpret the significance\n",
    "alpha = 0.05\n",
    "if p > alpha:\n",
    "\tprint('uncorrelated (fail to reject H0) p=%.3f' % p)\n",
    "else:\n",
    "\tprint('correlated (reject H0) p=%.3f' % p)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b8aee4d7",
   "metadata": {},
   "source": [
    "# BLEU-4 without Length Factor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "27fed82f",
   "metadata": {},
   "outputs": [],
   "source": [
    "import math\n",
    "import sys\n",
    "from fractions import Fraction\n",
    "import warnings\n",
    "from collections import Counter\n",
    "\n",
    "from nltk.util import ngrams\n",
    "\n",
    "def sentence_bleu_wol(\n",
    "    references,\n",
    "    hypothesis,\n",
    "    weights=(0.25, 0.25, 0.25, 0.25),\n",
    "    smoothing_function=None,\n",
    "    auto_reweigh=False,\n",
    "):\n",
    "    \n",
    "    return corpus_bleu(\n",
    "        [references], [hypothesis], weights, smoothing_function, auto_reweigh\n",
    "    )\n",
    "\n",
    "\n",
    "\n",
    "def corpus_bleu(\n",
    "    list_of_references,\n",
    "    hypotheses,\n",
    "    weights=(0.25, 0.25, 0.25, 0.25),\n",
    "    smoothing_function=None,\n",
    "    auto_reweigh=False,\n",
    "):\n",
    "    \n",
    "    # Before proceeding to compute BLEU, perform sanity checks.\n",
    "\n",
    "    p_numerators = Counter()  # Key = ngram order, and value = no. of ngram matches.\n",
    "    p_denominators = Counter()  # Key = ngram order, and value = no. of ngram in ref.\n",
    "    hyp_lengths, ref_lengths = 0, 0\n",
    "\n",
    "    assert len(list_of_references) == len(hypotheses), (\n",
    "        \"The number of hypotheses and their reference(s) should be the \" \"same \"\n",
    "    )\n",
    "\n",
    "    # Iterate through each hypothesis and their corresponding references.\n",
    "    for references, hypothesis in zip(list_of_references, hypotheses):\n",
    "        # For each order of ngram, calculate the numerator and\n",
    "        # denominator for the corpus-level modified precision.\n",
    "        for i, _ in enumerate(weights, start=1):\n",
    "            p_i = modified_precision(references, hypothesis, i)\n",
    "            p_numerators[i] += p_i.numerator\n",
    "            p_denominators[i] += p_i.denominator\n",
    "\n",
    "        # Calculate the hypothesis length and the closest reference length.\n",
    "        # Adds them to the corpus-level hypothesis and reference counts.\n",
    "        hyp_len = len(hypothesis.split())\n",
    "        hyp_lengths += hyp_len\n",
    "        ref_lengths += closest_ref_length(references, hyp_len)\n",
    "\n",
    "    # Calculate corpus-level brevity penalty.\n",
    "    #bp = brevity_penalty(ref_lengths, hyp_lengths)\n",
    "\n",
    "    # Uniformly re-weighting based on maximum hypothesis lengths if largest\n",
    "    # order of n-grams < 4 and weights is set at default.\n",
    "    if auto_reweigh:\n",
    "        if hyp_lengths < 4 and weights == (0.25, 0.25, 0.25, 0.25):\n",
    "            weights = (1 / hyp_lengths,) * hyp_lengths\n",
    "\n",
    "    # Collects the various precision values for the different ngram orders.\n",
    "    p_n = [\n",
    "        Fraction(p_numerators[i], p_denominators[i], _normalize=False)\n",
    "        for i, _ in enumerate(weights, start=1)\n",
    "    ]\n",
    "\n",
    "    # Returns 0 if there's no matching n-grams\n",
    "    # We only need to check for p_numerators[1] == 0, since if there's\n",
    "    # no unigrams, there won't be any higher order ngrams.\n",
    "    if p_numerators[1] == 0:\n",
    "        return 0\n",
    "\n",
    "    # If there's no smoothing, set use method0 from SmoothinFunction class.\n",
    "    if not smoothing_function:\n",
    "        smoothing_function = SmoothingFunction().method0\n",
    "    # Smoothen the modified precision.\n",
    "    # Note: smoothing_function() may convert values into floats;\n",
    "    #       it tries to retain the Fraction object as much as the\n",
    "    #       smoothing method allows.\n",
    "    p_n = smoothing_function(\n",
    "        p_n, references=references, hypothesis=hypothesis, hyp_len=hyp_lengths\n",
    "    )\n",
    "    s = (w_i * math.log(p_i) for w_i, p_i in zip(weights, p_n))\n",
    "    s = math.exp(math.fsum(s))\n",
    "    return s\n",
    "\n",
    "\n",
    "\n",
    "def modified_precision(references, hypothesis, n):\n",
    "    \"\"\"\n",
    "    Calculate modified ngram precision.\n",
    "\n",
    "    The normal precision method may lead to some wrong translations with\n",
    "    high-precision, e.g., the translation, in which a word of reference\n",
    "    repeats several times, has very high precision.\n",
    "\n",
    "    This function only returns the Fraction object that contains the numerator\n",
    "    and denominator necessary to calculate the corpus-level precision.\n",
    "    To calculate the modified precision for a single pair of hypothesis and\n",
    "    references, cast the Fraction object into a float.\n",
    "\n",
    "    The famous \"the the the ... \" example shows that you can get BLEU precision\n",
    "    by duplicating high frequency words.\n",
    "\n",
    "        >>> reference1 = 'the cat is on the mat'.split()\n",
    "        >>> reference2 = 'there is a cat on the mat'.split()\n",
    "        >>> hypothesis1 = 'the the the the the the the'.split()\n",
    "        >>> references = [reference1, reference2]\n",
    "        >>> float(modified_precision(references, hypothesis1, n=1)) # doctest: +ELLIPSIS\n",
    "        0.2857...\n",
    "\n",
    "    In the modified n-gram precision, a reference word will be considered\n",
    "    exhausted after a matching hypothesis word is identified, e.g.\n",
    "\n",
    "        >>> reference1 = ['It', 'is', 'a', 'guide', 'to', 'action', 'that',\n",
    "        ...               'ensures', 'that', 'the', 'military', 'will',\n",
    "        ...               'forever', 'heed', 'Party', 'commands']\n",
    "        >>> reference2 = ['It', 'is', 'the', 'guiding', 'principle', 'which',\n",
    "        ...               'guarantees', 'the', 'military', 'forces', 'always',\n",
    "        ...               'being', 'under', 'the', 'command', 'of', 'the',\n",
    "        ...               'Party']\n",
    "        >>> reference3 = ['It', 'is', 'the', 'practical', 'guide', 'for', 'the',\n",
    "        ...               'army', 'always', 'to', 'heed', 'the', 'directions',\n",
    "        ...               'of', 'the', 'party']\n",
    "        >>> hypothesis = 'of the'.split()\n",
    "        >>> references = [reference1, reference2, reference3]\n",
    "        >>> float(modified_precision(references, hypothesis, n=1))\n",
    "        1.0\n",
    "        >>> float(modified_precision(references, hypothesis, n=2))\n",
    "        1.0\n",
    "\n",
    "    An example of a normal machine translation hypothesis:\n",
    "\n",
    "        >>> hypothesis1 = ['It', 'is', 'a', 'guide', 'to', 'action', 'which',\n",
    "        ...               'ensures', 'that', 'the', 'military', 'always',\n",
    "        ...               'obeys', 'the', 'commands', 'of', 'the', 'party']\n",
    "\n",
    "        >>> hypothesis2 = ['It', 'is', 'to', 'insure', 'the', 'troops',\n",
    "        ...               'forever', 'hearing', 'the', 'activity', 'guidebook',\n",
    "        ...               'that', 'party', 'direct']\n",
    "\n",
    "        >>> reference1 = ['It', 'is', 'a', 'guide', 'to', 'action', 'that',\n",
    "        ...               'ensures', 'that', 'the', 'military', 'will',\n",
    "        ...               'forever', 'heed', 'Party', 'commands']\n",
    "\n",
    "        >>> reference2 = ['It', 'is', 'the', 'guiding', 'principle', 'which',\n",
    "        ...               'guarantees', 'the', 'military', 'forces', 'always',\n",
    "        ...               'being', 'under', 'the', 'command', 'of', 'the',\n",
    "        ...               'Party']\n",
    "\n",
    "        >>> reference3 = ['It', 'is', 'the', 'practical', 'guide', 'for', 'the',\n",
    "        ...               'army', 'always', 'to', 'heed', 'the', 'directions',\n",
    "        ...               'of', 'the', 'party']\n",
    "        >>> references = [reference1, reference2, reference3]\n",
    "        >>> float(modified_precision(references, hypothesis1, n=1)) # doctest: +ELLIPSIS\n",
    "        0.9444...\n",
    "        >>> float(modified_precision(references, hypothesis2, n=1)) # doctest: +ELLIPSIS\n",
    "        0.5714...\n",
    "        >>> float(modified_precision(references, hypothesis1, n=2)) # doctest: +ELLIPSIS\n",
    "        0.5882352941176471\n",
    "        >>> float(modified_precision(references, hypothesis2, n=2)) # doctest: +ELLIPSIS\n",
    "        0.07692...\n",
    "\n",
    "\n",
    "    :param references: A list of reference translations.\n",
    "    :type references: list(list(str))\n",
    "    :param hypothesis: A hypothesis translation.\n",
    "    :type hypothesis: list(str)\n",
    "    :param n: The ngram order.\n",
    "    :type n: int\n",
    "    :return: BLEU's modified precision for the nth order ngram.\n",
    "    :rtype: Fraction\n",
    "    \"\"\"\n",
    "    # Extracts all ngrams in hypothesis\n",
    "    # Set an empty Counter if hypothesis is empty.\n",
    "    counts = Counter(ngrams(hypothesis, n)) if len(hypothesis) >= n else Counter()\n",
    "    # Extract a union of references' counts.\n",
    "    # max_counts = reduce(or_, [Counter(ngrams(ref, n)) for ref in references])\n",
    "    max_counts = {}\n",
    "    for reference in references:\n",
    "        reference_counts = (\n",
    "            Counter(ngrams(reference, n)) if len(reference) >= n else Counter()\n",
    "        )\n",
    "        for ngram in counts:\n",
    "            max_counts[ngram] = max(max_counts.get(ngram, 0), reference_counts[ngram])\n",
    "\n",
    "    # Assigns the intersection between hypothesis and references' counts.\n",
    "    clipped_counts = {\n",
    "        ngram: min(count, max_counts[ngram]) for ngram, count in counts.items()\n",
    "    }\n",
    "\n",
    "    numerator = sum(clipped_counts.values())\n",
    "    # Ensures that denominator is minimum 1 to avoid ZeroDivisionError.\n",
    "    # Usually this happens when the ngram order is > len(reference).\n",
    "    denominator = max(1, sum(counts.values()))\n",
    "\n",
    "    return Fraction(numerator, denominator, _normalize=False)\n",
    "\n",
    "\n",
    "\n",
    "def closest_ref_length(references, hyp_len):\n",
    "    \"\"\"\n",
    "    This function finds the reference that is the closest length to the\n",
    "    hypothesis. The closest reference length is referred to as *r* variable\n",
    "    from the brevity penalty formula in Papineni et. al. (2002)\n",
    "\n",
    "    :param references: A list of reference translations.\n",
    "    :type references: list(list(str))\n",
    "    :param hyp_len: The length of the hypothesis.\n",
    "    :type hyp_len: int\n",
    "    :return: The length of the reference that's closest to the hypothesis.\n",
    "    :rtype: int\n",
    "    \"\"\"\n",
    "    ref_lens = (len(reference) for reference in references)\n",
    "    closest_ref_len = min(\n",
    "        ref_lens, key=lambda ref_len: (abs(ref_len - hyp_len), ref_len)\n",
    "    )\n",
    "    return closest_ref_len\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "class SmoothingFunction:\n",
    "    \"\"\"\n",
    "    This is an implementation of the smoothing techniques\n",
    "    for segment-level BLEU scores that was presented in\n",
    "    Boxing Chen and Collin Cherry (2014) A Systematic Comparison of\n",
    "    Smoothing Techniques for Sentence-Level BLEU. In WMT14.\n",
    "    http://acl2014.org/acl2014/W14-33/pdf/W14-3346.pdf\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, epsilon=0.1, alpha=5, k=5):\n",
    "        \"\"\"\n",
    "        This will initialize the parameters required for the various smoothing\n",
    "        techniques, the default values are set to the numbers used in the\n",
    "        experiments from Chen and Cherry (2014).\n",
    "\n",
    "        >>> hypothesis1 = ['It', 'is', 'a', 'guide', 'to', 'action', 'which', 'ensures',\n",
    "        ...                 'that', 'the', 'military', 'always', 'obeys', 'the',\n",
    "        ...                 'commands', 'of', 'the', 'party']\n",
    "        >>> reference1 = ['It', 'is', 'a', 'guide', 'to', 'action', 'that', 'ensures',\n",
    "        ...               'that', 'the', 'military', 'will', 'forever', 'heed',\n",
    "        ...               'Party', 'commands']\n",
    "\n",
    "        >>> chencherry = SmoothingFunction()\n",
    "        >>> print(sentence_bleu([reference1], hypothesis1)) # doctest: +ELLIPSIS\n",
    "        0.4118...\n",
    "        >>> print(sentence_bleu([reference1], hypothesis1, smoothing_function=chencherry.method0)) # doctest: +ELLIPSIS\n",
    "        0.4118...\n",
    "        >>> print(sentence_bleu([reference1], hypothesis1, smoothing_function=chencherry.method1)) # doctest: +ELLIPSIS\n",
    "        0.4118...\n",
    "        >>> print(sentence_bleu([reference1], hypothesis1, smoothing_function=chencherry.method2)) # doctest: +ELLIPSIS\n",
    "        0.4489...\n",
    "        >>> print(sentence_bleu([reference1], hypothesis1, smoothing_function=chencherry.method3)) # doctest: +ELLIPSIS\n",
    "        0.4118...\n",
    "        >>> print(sentence_bleu([reference1], hypothesis1, smoothing_function=chencherry.method4)) # doctest: +ELLIPSIS\n",
    "        0.4118...\n",
    "        >>> print(sentence_bleu([reference1], hypothesis1, smoothing_function=chencherry.method5)) # doctest: +ELLIPSIS\n",
    "        0.4905...\n",
    "        >>> print(sentence_bleu([reference1], hypothesis1, smoothing_function=chencherry.method6)) # doctest: +ELLIPSIS\n",
    "        0.4135...\n",
    "        >>> print(sentence_bleu([reference1], hypothesis1, smoothing_function=chencherry.method7)) # doctest: +ELLIPSIS\n",
    "        0.4905...\n",
    "\n",
    "        :param epsilon: the epsilon value use in method 1\n",
    "        :type epsilon: float\n",
    "        :param alpha: the alpha value use in method 6\n",
    "        :type alpha: int\n",
    "        :param k: the k value use in method 4\n",
    "        :type k: int\n",
    "        \"\"\"\n",
    "        self.epsilon = epsilon\n",
    "        self.alpha = alpha\n",
    "        self.k = k\n",
    "\n",
    "    def method0(self, p_n, *args, **kwargs):\n",
    "        \"\"\"\n",
    "        No smoothing.\n",
    "        \"\"\"\n",
    "        p_n_new = []\n",
    "        for i, p_i in enumerate(p_n):\n",
    "            if p_i.numerator != 0:\n",
    "                p_n_new.append(p_i)\n",
    "            else:\n",
    "                _msg = str(\n",
    "                    \"\\nThe hypothesis contains 0 counts of {}-gram overlaps.\\n\"\n",
    "                    \"Therefore the BLEU score evaluates to 0, independently of\\n\"\n",
    "                    \"how many N-gram overlaps of lower order it contains.\\n\"\n",
    "                    \"Consider using lower n-gram order or use \"\n",
    "                    \"SmoothingFunction()\"\n",
    "                ).format(i + 1)\n",
    "                warnings.warn(_msg)\n",
    "                # When numerator==0 where denonminator==0 or !=0, the result\n",
    "                # for the precision score should be equal to 0 or undefined.\n",
    "                # Due to BLEU geometric mean computation in logarithm space,\n",
    "                # we we need to take the return sys.float_info.min such that\n",
    "                # math.log(sys.float_info.min) returns a 0 precision score.\n",
    "                p_n_new.append(sys.float_info.min)\n",
    "        return p_n_new\n",
    "\n",
    "\n",
    "    def method1(self, p_n, *args, **kwargs):\n",
    "        \"\"\"\n",
    "        Smoothing method 1: Add *epsilon* counts to precision with 0 counts.\n",
    "        \"\"\"\n",
    "        return [\n",
    "            (p_i.numerator + self.epsilon) / p_i.denominator\n",
    "            if p_i.numerator == 0\n",
    "            else p_i\n",
    "            for p_i in p_n\n",
    "        ]\n",
    "\n",
    "\n",
    "    def method2(self, p_n, *args, **kwargs):\n",
    "        \"\"\"\n",
    "        Smoothing method 2: Add 1 to both numerator and denominator from\n",
    "        Chin-Yew Lin and Franz Josef Och (2004) ORANGE: a Method for\n",
    "        Evaluating Automatic Evaluation Metrics for Machine Translation.\n",
    "        In COLING 2004.\n",
    "        \"\"\"\n",
    "        return [\n",
    "            Fraction(p_n[i].numerator + 1, p_n[i].denominator + 1, _normalize=False)\n",
    "            if i != 0 else p_n[0]\n",
    "            for i in range(len(p_n))\n",
    "        ]\n",
    "\n",
    "\n",
    "    def method3(self, p_n, *args, **kwargs):\n",
    "        \"\"\"\n",
    "        Smoothing method 3: NIST geometric sequence smoothing\n",
    "        The smoothing is computed by taking 1 / ( 2^k ), instead of 0, for each\n",
    "        precision score whose matching n-gram count is null.\n",
    "        k is 1 for the first 'n' value for which the n-gram match count is null/\n",
    "        For example, if the text contains:\n",
    "         - one 2-gram match\n",
    "         - and (consequently) two 1-gram matches\n",
    "        the n-gram count for each individual precision score would be:\n",
    "         - n=1  =>  prec_count = 2     (two unigrams)\n",
    "         - n=2  =>  prec_count = 1     (one bigram)\n",
    "         - n=3  =>  prec_count = 1/2   (no trigram,  taking 'smoothed' value of 1 / ( 2^k ), with k=1)\n",
    "         - n=4  =>  prec_count = 1/4   (no fourgram, taking 'smoothed' value of 1 / ( 2^k ), with k=2)\n",
    "        \"\"\"\n",
    "        incvnt = 1  # From the mteval-v13a.pl, it's referred to as k.\n",
    "        for i, p_i in enumerate(p_n):\n",
    "            if p_i.numerator == 0:\n",
    "                p_n[i] = 1 / (2 ** incvnt * p_i.denominator)\n",
    "                incvnt += 1\n",
    "        return p_n\n",
    "\n",
    "\n",
    "    def method4(self, p_n, references, hypothesis, hyp_len=None, *args, **kwargs):\n",
    "        \"\"\"\n",
    "        Smoothing method 4:\n",
    "        Shorter translations may have inflated precision values due to having\n",
    "        smaller denominators; therefore, we give them proportionally\n",
    "        smaller smoothed counts. Instead of scaling to 1/(2^k), Chen and Cherry\n",
    "        suggests dividing by 1/ln(len(T)), where T is the length of the translation.\n",
    "        \"\"\"\n",
    "        incvnt = 1\n",
    "        hyp_len = hyp_len if hyp_len else len(hypothesis)\n",
    "        for i, p_i in enumerate(p_n):\n",
    "            if p_i.numerator == 0 and hyp_len >1:\n",
    "#                 incvnt = i + 1 * self.k / math.log(\n",
    "#                     hyp_len\n",
    "#                 )  # Note that this K is different from the K from NIST.\n",
    "#                 p_n[i] = incvnt / p_i.denominator\\\n",
    "                numerator = 1 / (2 ** incvnt * self.k / math.log(hyp_len))\n",
    "                p_n[i] = numerator / p_i.denominator\n",
    "                incvnt += 1\n",
    "        return p_n\n",
    "\n",
    "\n",
    "    def method5(self, p_n, references, hypothesis, hyp_len=None, *args, **kwargs):\n",
    "        \"\"\"\n",
    "        Smoothing method 5:\n",
    "        The matched counts for similar values of n should be similar. To a\n",
    "        calculate the n-gram matched count, it averages the n−1, n and n+1 gram\n",
    "        matched counts.\n",
    "        \"\"\"\n",
    "        hyp_len = hyp_len if hyp_len else len(hypothesis)\n",
    "        m = {}\n",
    "        # Requires an precision value for an addition ngram order.\n",
    "        p_n_plus1 = p_n + [modified_precision(references, hypothesis, 5)]\n",
    "        m[-1] = p_n[0] + 1\n",
    "        for i, p_i in enumerate(p_n):\n",
    "            p_n[i] = (m[i - 1] + p_i + p_n_plus1[i + 1]) / 3\n",
    "            m[i] = p_n[i]\n",
    "        return p_n\n",
    "\n",
    "\n",
    "    def method6(self, p_n, references, hypothesis, hyp_len=None, *args, **kwargs):\n",
    "        \"\"\"\n",
    "        Smoothing method 6:\n",
    "        Interpolates the maximum likelihood estimate of the precision *p_n* with\n",
    "        a prior estimate *pi0*. The prior is estimated by assuming that the ratio\n",
    "        between pn and pn−1 will be the same as that between pn−1 and pn−2; from\n",
    "        Gao and He (2013) Training MRF-Based Phrase Translation Models using\n",
    "        Gradient Ascent. In NAACL.\n",
    "        \"\"\"\n",
    "        hyp_len = hyp_len if hyp_len else len(hypothesis.split())\n",
    "        # This smoothing only works when p_1 and p_2 is non-zero.\n",
    "        # Raise an error with an appropriate message when the input is too short\n",
    "        # to use this smoothing technique.\n",
    "        assert p_n[2], \"This smoothing method requires non-zero precision for bigrams.\"\n",
    "        for i, p_i in enumerate(p_n):\n",
    "            if i in [0, 1]:  # Skips the first 2 orders of ngrams.\n",
    "                continue\n",
    "            else:\n",
    "                pi0 = 0 if p_n[i - 2] == 0 else p_n[i - 1] ** 2 / p_n[i - 2]\n",
    "                # No. of ngrams in translation that matches the reference.\n",
    "                m = p_i.numerator\n",
    "                # No. of ngrams in translation.\n",
    "                l = sum(1 for _ in ngrams(hypothesis, i + 1))\n",
    "                # Calculates the interpolated precision.\n",
    "                p_n[i] = (m + self.alpha * pi0) / (l + self.alpha)\n",
    "        return p_n\n",
    "\n",
    "\n",
    "    def method7(self, p_n, references, hypothesis, hyp_len=None, *args, **kwargs):\n",
    "        \"\"\"\n",
    "        Smoothing method 7:\n",
    "        Interpolates methods 4 and 5.\n",
    "        \"\"\"\n",
    "        hyp_len = hyp_len if hyp_len else len(hypothesis)\n",
    "        p_n = self.method4(p_n, references, hypothesis, hyp_len)\n",
    "        p_n = self.method5(p_n, references, hypothesis, hyp_len)\n",
    "        return p_n\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "7822249a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.72, 0.91, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.13, 0.36, 0.29, 0.32, 0.75, 0.49, 0.0, 0.0, 0.95, 0.56, 0.0, 0.91, 0.0, 0.0, 0.71, 0.47, 0.8, 0.53, 0.52, 0.8, 0.0, 0.0, 0.92, 0.0, 0.0, 0.71, 0.63, 0.0, 0.52, 0.36, 0.43, 0.79, 0.0, 0.0, 0.0, 0.17, 0.27, 0.21, 0.81, 0.24, 0.29, 0.26, 0.38, 0.83, 0.18, 0.41, 0.15, 0.35, 0.57, 0.22, 0.0, 0.36, 0.0, 0.0, 0.49, 0.0, 0.0, 0.49, 0.35, 0.0, 0.44, 0.21, 0.49, 0.53, 0.32, 0.27, 0.47, 0.35, 0.28, 0.28, 0.2, 0.42, 0.65, 0.58, 0.17, 0.44, 0.0, 0.0, 0.0, 0.25]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "<ipython-input-10-621e0b7f56d7>:296: UserWarning: \n",
      "The hypothesis contains 0 counts of 2-gram overlaps.\n",
      "Therefore the BLEU score evaluates to 0, independently of\n",
      "how many N-gram overlaps of lower order it contains.\n",
      "Consider using lower n-gram order or use SmoothingFunction()\n",
      "  warnings.warn(_msg)\n",
      "<ipython-input-10-621e0b7f56d7>:296: UserWarning: \n",
      "The hypothesis contains 0 counts of 3-gram overlaps.\n",
      "Therefore the BLEU score evaluates to 0, independently of\n",
      "how many N-gram overlaps of lower order it contains.\n",
      "Consider using lower n-gram order or use SmoothingFunction()\n",
      "  warnings.warn(_msg)\n",
      "<ipython-input-10-621e0b7f56d7>:296: UserWarning: \n",
      "The hypothesis contains 0 counts of 4-gram overlaps.\n",
      "Therefore the BLEU score evaluates to 0, independently of\n",
      "how many N-gram overlaps of lower order it contains.\n",
      "Consider using lower n-gram order or use SmoothingFunction()\n",
      "  warnings.warn(_msg)\n"
     ]
    }
   ],
   "source": [
    "b4_wol=[]\n",
    "\n",
    "for i in range(len(auth_1)):\n",
    "    b4_wol.append(round(sentence_bleu_wol([refs[i]], preds[i]),2))\n",
    "print(b4_wol)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "5cc74102",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.76, 0.96, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.14, 0.38, 0.31, 0.34, 0.79, 0.52, 0.0, 0.0, 1.0, 0.59, 0.0, 0.96, 0.0, 0.0, 0.75, 0.49, 0.84, 0.56, 0.55, 0.84, 0.0, 0.0, 0.97, 0.0, 0.0, 0.75, 0.66, 0.0, 0.55, 0.38, 0.45, 0.83, 0.0, 0.0, 0.0, 0.18, 0.28, 0.22, 0.85, 0.25, 0.31, 0.27, 0.4, 0.87, 0.19, 0.43, 0.16, 0.37, 0.6, 0.23, 0.0, 0.38, 0.0, 0.0, 0.52, 0.0, 0.0, 0.52, 0.37, 0.0, 0.46, 0.22, 0.52, 0.56, 0.34, 0.28, 0.49, 0.37, 0.29, 0.29, 0.21, 0.44, 0.68, 0.61, 0.18, 0.46, 0.0, 0.0, 0.0, 0.26]\n"
     ]
    }
   ],
   "source": [
    "norm_b4_wol = []\n",
    "\n",
    "max_b4=max(b4_wol)\n",
    "min_b4=min(b4_wol)\n",
    "\n",
    "for a in range(len(b4_wol)):\n",
    "    norm_b4_wol.append(round(((b4_wol[a])/(max_b4)),2))\n",
    "print(norm_b4_wol)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "0fe93b98",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Spearmans correlation coefficient: 0.062\n",
      "uncorrelated (fail to reject H0) p=0.541\n"
     ]
    }
   ],
   "source": [
    "from scipy.stats import spearmanr\n",
    "# calculate spearman's correlation\n",
    "coef, p = spearmanr(norm_b4_wol, avg_norm_score)\n",
    "print('Spearmans correlation coefficient: %.3f' % coef)\n",
    "# interpret the significance\n",
    "alpha = 0.05\n",
    "if p > alpha:\n",
    "\tprint('uncorrelated (fail to reject H0) p=%.3f' % p)\n",
    "else:\n",
    "\tprint('correlated (reject H0) p=%.3f' % p)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "301c3af8",
   "metadata": {},
   "source": [
    "# BLEU-NN with Length Factor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "2714173f",
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0.59, 0.59, 0.84, 0.84, 0.71, 0.71, 0.71, 0.15, 0.65, 0.66, 0.67, 0.73, 0.55, 0.22, 0.43, 0.49, 0.26, 0.21, 0.21, 0.56, 0.16, 0.4, 0.32, 0.29, 0.76, 0.51, 0.32, 0.22, 0.45, 0.5, 0.29, 0.26, 0.28, 0.3, 0.73, 0.48, 0.64, 0.54, 0.53, 0.66, 0.24, 0.24, 0.92, 0.08, 0.26, 0.58, 0.64, 0.1, 0.53, 0.38, 0.44, 0.8, 0.21, 0.11, 0.12, 0.21, 0.27, 0.25, 0.5, 0.23, 0.31, 0.28, 0.32, 0.34, 0.16, 0.29, 0.16, 0.36, 0.33, 0.24, 0.12, 0.39, 0.15, 0.14, 0.51, 0.05, 0.22, 0.32, 0.37, 0.1, 0.45, 0.18, 0.5, 0.54, 0.33, 0.29, 0.31, 0.36, 0.24, 0.22, 0.2, 0.44, 0.25, 0.33, 0.18, 0.46, 0.09, 0.11, 0.05, 0.28]\n"
     ]
    }
   ],
   "source": [
    "bleun_wl=[]\n",
    "for i in range(len(auth_1)):\n",
    "    bleun_wl.append(round(sentence_bleu([refs[i]], preds[i],smoothing_function=SmoothingFunction().method2),2))\n",
    "print(bleun_wl)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "676b563e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0.64, 0.64, 0.91, 0.91, 0.77, 0.77, 0.77, 0.16, 0.71, 0.72, 0.73, 0.79, 0.6, 0.24, 0.47, 0.53, 0.28, 0.23, 0.23, 0.61, 0.17, 0.43, 0.35, 0.32, 0.83, 0.55, 0.35, 0.24, 0.49, 0.54, 0.32, 0.28, 0.3, 0.33, 0.79, 0.52, 0.7, 0.59, 0.58, 0.72, 0.26, 0.26, 1.0, 0.09, 0.28, 0.63, 0.7, 0.11, 0.58, 0.41, 0.48, 0.87, 0.23, 0.12, 0.13, 0.23, 0.29, 0.27, 0.54, 0.25, 0.34, 0.3, 0.35, 0.37, 0.17, 0.32, 0.17, 0.39, 0.36, 0.26, 0.13, 0.42, 0.16, 0.15, 0.55, 0.05, 0.24, 0.35, 0.4, 0.11, 0.49, 0.2, 0.54, 0.59, 0.36, 0.32, 0.34, 0.39, 0.26, 0.24, 0.22, 0.48, 0.27, 0.36, 0.2, 0.5, 0.1, 0.12, 0.05, 0.3]\n"
     ]
    }
   ],
   "source": [
    "norm_bn_wl = []\n",
    "\n",
    "max_bn=max(bleun_wl)\n",
    "min_bn=min(bleun_wl)\n",
    "\n",
    "for a in range(len(bleun_wl)):\n",
    "    norm_bn_wl.append(round(((bleun_wl[a])/(max_bn)),2))\n",
    "print(norm_bn_wl)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "c27d42a2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Spearmans correlation coefficient: 0.567\n",
      "correlated (reject H0) p=0.000\n"
     ]
    }
   ],
   "source": [
    "from scipy.stats import spearmanr\n",
    "# calculate spearman's correlation\n",
    "coef, p = spearmanr(norm_bn_wl, avg_norm_score)\n",
    "print('Spearmans correlation coefficient: %.3f' % coef)\n",
    "# interpret the significance\n",
    "alpha = 0.05\n",
    "if p > alpha:\n",
    "\tprint('uncorrelated (fail to reject H0) p=%.3f' % p)\n",
    "else:\n",
    "\tprint('correlated (reject H0) p=%.3f' % p)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "id": "3ae03c48",
   "metadata": {},
   "outputs": [],
   "source": [
    "def csvdict(num):\n",
    "    dictforcsv = {}\n",
    "    dictforcsv['Ref'] = refs[num-1]\n",
    "    dictforcsv['Pred'] = preds[num-1]\n",
    "    dictforcsv['BLEU-CC without length'] = norm_bcc_wol[num-1]\n",
    "    dictforcsv['BLEU-CC with length'] =  norm_bcc_wl[num-1]\n",
    "    dictforcsv['Avg Human score'] = avg_norm_score[num-1]\n",
    "    return dictforcsv"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "id": "f273caab",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import csv\n",
    "\n",
    "csvlist = []\n",
    "for i in range(len(auth_1)):\n",
    "    csvlist.append(csvdict(i+1))\n",
    "#print(csvlist)\n",
    "\n",
    "fields = ['Ref','Pred','BLEU-CC without length', 'BLEU-CC with length','Avg Human score']\n",
    "\n",
    "testcsvfile = \"temp2.csv\"\n",
    "\n",
    "# writing to csv file \n",
    "with open(testcsvfile, 'w') as csvfile: \n",
    "    # creating a csv dict writer object \n",
    "    writer = csv.DictWriter(csvfile, fieldnames = fields) \n",
    "        \n",
    "    # writing headers (field names) \n",
    "    writer.writeheader() \n",
    "        \n",
    "    # writing data rows \n",
    "    writer.writerows(csvlist) \n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3cc38bc5",
   "metadata": {},
   "source": [
    "# BLEU-NN without length factor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "8d5fa02e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0.59, 0.59, 0.84, 0.84, 0.71, 0.71, 0.71, 0.69, 0.65, 0.66, 0.67, 0.73, 0.91, 0.59, 0.71, 0.49, 0.71, 0.21, 0.21, 0.56, 0.16, 0.4, 0.32, 0.36, 0.76, 0.51, 0.44, 0.59, 0.96, 0.57, 0.78, 0.91, 0.28, 0.83, 0.73, 0.48, 0.8, 0.54, 0.53, 0.8, 0.46, 0.24, 0.92, 0.56, 0.26, 0.72, 0.64, 0.13, 0.53, 0.38, 0.44, 0.8, 0.58, 0.11, 0.12, 0.21, 0.27, 0.25, 0.82, 0.27, 0.31, 0.28, 0.39, 0.83, 0.24, 0.43, 0.2, 0.36, 0.58, 0.24, 0.12, 0.39, 0.15, 0.14, 0.51, 0.06, 0.22, 0.51, 0.37, 0.1, 0.45, 0.25, 0.5, 0.54, 0.33, 0.29, 0.49, 0.36, 0.3, 0.31, 0.23, 0.44, 0.67, 0.61, 0.18, 0.46, 0.09, 0.11, 0.05, 0.28]\n"
     ]
    }
   ],
   "source": [
    "bleun_wol=[]\n",
    "for i in range(len(auth_1)):\n",
    "    bleun_wol.append(round(sentence_bleu_wol([refs[i]], preds[i],smoothing_function=SmoothingFunction().method2),2))\n",
    "print(bleun_wol)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "fa78256a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0.61, 0.61, 0.88, 0.88, 0.74, 0.74, 0.74, 0.72, 0.68, 0.69, 0.7, 0.76, 0.95, 0.61, 0.74, 0.51, 0.74, 0.22, 0.22, 0.58, 0.17, 0.42, 0.33, 0.38, 0.79, 0.53, 0.46, 0.61, 1.0, 0.59, 0.81, 0.95, 0.29, 0.86, 0.76, 0.5, 0.83, 0.56, 0.55, 0.83, 0.48, 0.25, 0.96, 0.58, 0.27, 0.75, 0.67, 0.14, 0.55, 0.4, 0.46, 0.83, 0.6, 0.11, 0.12, 0.22, 0.28, 0.26, 0.85, 0.28, 0.32, 0.29, 0.41, 0.86, 0.25, 0.45, 0.21, 0.38, 0.6, 0.25, 0.12, 0.41, 0.16, 0.15, 0.53, 0.06, 0.23, 0.53, 0.39, 0.1, 0.47, 0.26, 0.52, 0.56, 0.34, 0.3, 0.51, 0.38, 0.31, 0.32, 0.24, 0.46, 0.7, 0.64, 0.19, 0.48, 0.09, 0.11, 0.05, 0.29]\n"
     ]
    }
   ],
   "source": [
    "norm_bn_wol = []\n",
    "\n",
    "max_bn=max(bleun_wol)\n",
    "min_bn=min(bleun_wol)\n",
    "\n",
    "for a in range(len(bleun_wol)):\n",
    "    norm_bn_wol.append(round(((bleun_wol[a])/(max_bn)),2))\n",
    "print(norm_bn_wol)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "d246c226",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Spearmans correlation coefficient: 0.586\n",
      "correlated (reject H0) p=0.000\n"
     ]
    }
   ],
   "source": [
    "from scipy.stats import spearmanr\n",
    "# calculate spearman's correlation\n",
    "coef, p = spearmanr(norm_bn_wol, avg_norm_score)\n",
    "print('Spearmans correlation coefficient: %.3f' % coef)\n",
    "# interpret the significance\n",
    "alpha = 0.05\n",
    "if p > alpha:\n",
    "\tprint('uncorrelated (fail to reject H0) p=%.3f' % p)\n",
    "else:\n",
    "\tprint('correlated (reject H0) p=%.3f' % p)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1b4334c1",
   "metadata": {},
   "source": [
    "# BLEU-CC with Length Factor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "9f48bba2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0.19, 0.19, 0.7, 0.7, 0.39, 0.39, 0.39, 0.08, 0.33, 0.51, 0.53, 0.75, 0.56, 0.07, 0.24, 0.35, 0.14, 0.39, 0.39, 0.41, 0.21, 0.42, 0.36, 0.35, 0.78, 0.54, 0.22, 0.1, 0.46, 0.57, 0.23, 0.26, 0.38, 0.25, 0.75, 0.52, 0.71, 0.62, 0.61, 0.73, 0.17, 0.31, 1.03, 0.04, 0.36, 0.64, 0.74, 0.12, 0.57, 0.45, 0.53, 0.89, 0.16, 0.17, 0.13, 0.27, 0.37, 0.28, 0.56, 0.27, 0.38, 0.35, 0.38, 0.38, 0.18, 0.33, 0.2, 0.44, 0.39, 0.29, 0.16, 0.42, 0.18, 0.21, 0.58, 0.08, 0.37, 0.38, 0.44, 0.16, 0.54, 0.21, 0.59, 0.62, 0.41, 0.38, 0.36, 0.45, 0.3, 0.27, 0.25, 0.51, 0.27, 0.37, 0.24, 0.49, 0.14, 0.16, 0.09, 0.32]\n"
     ]
    }
   ],
   "source": [
    "bleucc_wl=[]\n",
    "for i in range(len(auth_1)):\n",
    "    bleucc_wl.append(round(sentence_bleu([refs[i]], preds[i],smoothing_function=SmoothingFunction().method5),2))\n",
    "print(bleucc_wl)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "f32f860c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0.18, 0.18, 0.68, 0.68, 0.38, 0.38, 0.38, 0.08, 0.32, 0.5, 0.51, 0.73, 0.54, 0.07, 0.23, 0.34, 0.14, 0.38, 0.38, 0.4, 0.2, 0.41, 0.35, 0.34, 0.76, 0.52, 0.21, 0.1, 0.45, 0.55, 0.22, 0.25, 0.37, 0.24, 0.73, 0.5, 0.69, 0.6, 0.59, 0.71, 0.17, 0.3, 1.0, 0.04, 0.35, 0.62, 0.72, 0.12, 0.55, 0.44, 0.51, 0.86, 0.16, 0.17, 0.13, 0.26, 0.36, 0.27, 0.54, 0.26, 0.37, 0.34, 0.37, 0.37, 0.17, 0.32, 0.19, 0.43, 0.38, 0.28, 0.16, 0.41, 0.17, 0.2, 0.56, 0.08, 0.36, 0.37, 0.43, 0.16, 0.52, 0.2, 0.57, 0.6, 0.4, 0.37, 0.35, 0.44, 0.29, 0.26, 0.24, 0.5, 0.26, 0.36, 0.23, 0.48, 0.14, 0.16, 0.09, 0.31]\n"
     ]
    }
   ],
   "source": [
    "norm_bcc_wl = []\n",
    "\n",
    "max_bcc=max(bleucc_wl)\n",
    "min_bcc=min(bleucc_wl)\n",
    "\n",
    "for a in range(len(bleucc_wl)):\n",
    "    norm_bcc_wl.append(round(((bleucc_wl[a])/(max_bcc)),2))\n",
    "print(norm_bcc_wl)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "e2a53b17",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Spearmans correlation coefficient: 0.374\n",
      "correlated (reject H0) p=0.000\n"
     ]
    }
   ],
   "source": [
    "from scipy.stats import spearmanr\n",
    "# calculate spearman's correlation\n",
    "coef, p = spearmanr(norm_bcc_wl, avg_norm_score)\n",
    "print('Spearmans correlation coefficient: %.3f' % coef)\n",
    "# interpret the significance\n",
    "alpha = 0.05\n",
    "if p > alpha:\n",
    "\tprint('uncorrelated (fail to reject H0) p=%.3f' % p)\n",
    "else:\n",
    "\tprint('correlated (reject H0) p=%.3f' % p)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f11cf656",
   "metadata": {},
   "source": [
    "# BLEU-CC without Length Factor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "id": "e72ba624",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0.19, 0.19, 0.7, 0.7, 0.39, 0.39, 0.39, 0.37, 0.33, 0.51, 0.53, 0.75, 0.92, 0.19, 0.39, 0.35, 0.39, 0.39, 0.39, 0.41, 0.21, 0.42, 0.36, 0.42, 0.78, 0.54, 0.3, 0.28, 0.96, 0.66, 0.64, 0.92, 0.38, 0.69, 0.75, 0.52, 0.89, 0.62, 0.61, 0.9, 0.33, 0.31, 1.03, 0.27, 0.36, 0.8, 0.74, 0.15, 0.57, 0.45, 0.53, 0.89, 0.43, 0.17, 0.13, 0.27, 0.37, 0.28, 0.92, 0.31, 0.38, 0.35, 0.47, 0.93, 0.27, 0.5, 0.24, 0.44, 0.67, 0.29, 0.16, 0.42, 0.18, 0.21, 0.58, 0.09, 0.37, 0.59, 0.44, 0.16, 0.54, 0.3, 0.59, 0.62, 0.41, 0.38, 0.57, 0.45, 0.38, 0.37, 0.29, 0.51, 0.74, 0.67, 0.24, 0.49, 0.14, 0.16, 0.09, 0.32]\n"
     ]
    }
   ],
   "source": [
    "bleucc_wol=[]\n",
    "for i in range(len(auth_1)):\n",
    "    bleucc_wol.append(round(sentence_bleu_wol([refs[i]], preds[i],smoothing_function=SmoothingFunction().method5),2))\n",
    "print(bleucc_wol)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "id": "6aee035c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0.18, 0.18, 0.68, 0.68, 0.38, 0.38, 0.38, 0.36, 0.32, 0.5, 0.51, 0.73, 0.89, 0.18, 0.38, 0.34, 0.38, 0.38, 0.38, 0.4, 0.2, 0.41, 0.35, 0.41, 0.76, 0.52, 0.29, 0.27, 0.93, 0.64, 0.62, 0.89, 0.37, 0.67, 0.73, 0.5, 0.86, 0.6, 0.59, 0.87, 0.32, 0.3, 1.0, 0.26, 0.35, 0.78, 0.72, 0.15, 0.55, 0.44, 0.51, 0.86, 0.42, 0.17, 0.13, 0.26, 0.36, 0.27, 0.89, 0.3, 0.37, 0.34, 0.46, 0.9, 0.26, 0.49, 0.23, 0.43, 0.65, 0.28, 0.16, 0.41, 0.17, 0.2, 0.56, 0.09, 0.36, 0.57, 0.43, 0.16, 0.52, 0.29, 0.57, 0.6, 0.4, 0.37, 0.55, 0.44, 0.37, 0.36, 0.28, 0.5, 0.72, 0.65, 0.23, 0.48, 0.14, 0.16, 0.09, 0.31]\n"
     ]
    }
   ],
   "source": [
    "norm_bcc_wol = []\n",
    "\n",
    "max_bcc=max(bleucc_wol)\n",
    "min_bcc=min(bleucc_wol)\n",
    "\n",
    "for a in range(len(bleucc_wol)):\n",
    "    norm_bcc_wol.append(round(((bleucc_wol[a])/(max_bcc)),2))\n",
    "print(norm_bcc_wol)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "id": "e9159d10",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Spearmans correlation coefficient: 0.360\n",
      "correlated (reject H0) p=0.000\n"
     ]
    }
   ],
   "source": [
    "from scipy.stats import spearmanr\n",
    "# calculate spearman's correlation\n",
    "coef, p = spearmanr(norm_bcc_wol, avg_norm_score)\n",
    "print('Spearmans correlation coefficient: %.3f' % coef)\n",
    "# interpret the significance\n",
    "alpha = 0.05\n",
    "if p > alpha:\n",
    "\tprint('uncorrelated (fail to reject H0) p=%.3f' % p)\n",
    "else:\n",
    "\tprint('correlated (reject H0) p=%.3f' % p)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
