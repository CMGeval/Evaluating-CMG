{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "da3d344d",
   "metadata": {},
   "outputs": [],
   "source": [
    "import csv\n",
    "import json\n",
    "\n",
    "preds=[]\n",
    "refs=[]\n",
    "auth_1 =[]\n",
    "auth_2 = []\n",
    "auth_3 =[]\n",
    "with open('human_annotations.csv') as csvfile:\n",
    "    ader = csv.reader(csvfile)\n",
    "    for row in ader:\n",
    "        refs.append(row[1])\n",
    "        preds.append(row[0])\n",
    "        auth_1.append(row[2])\n",
    "        auth_2.append(row[3])\n",
    "        auth_3.append(row[4])\n",
    "        \n",
    "refs = refs[:100]\n",
    "preds = preds[:100]\n",
    "\n",
    "auth_1 = auth_1[:100]\n",
    "for i in range(0, len(auth_1)):\n",
    "    auth_1[i] = int(auth_1[i])\n",
    "#print(auth_1)\n",
    "\n",
    "auth_2 = auth_2[:100]\n",
    "for i in range(0, len(auth_2)):\n",
    "    auth_2[i] = int(auth_2[i])\n",
    "#print(auth_2)\n",
    "\n",
    "auth_3 = auth_3[:100]\n",
    "for i in range(0, len(auth_3)):\n",
    "    auth_3[i] = int(auth_3[i])\n",
    "#print(auth_3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "7bc37996",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.5, 1.0, 0.75, 0.75, 0.75, 0.75, 1.0, 1.0, 0.5, 0.5, 0.75, 0.75, 0.5, 0.5, 0.5, 0.75, 0.25, 0.75, 0.75, 0.5, 0.5, 0.75, 0.75, 0.75, 0.75, 0.25, 0.75, 1.0, 0.25, 1.0, 0.75, 0.75, 0.75, 0.25, 0.75, 1.0, 0.75, 0.5, 0.75, 0.25, 0.0, 0.75, 0.5, 0.25, 1.0, 0.25, 0.5, 0.5, 0.0, 0.5, 0.5, 0.25, 0.5, 0.25, 0.25, 0.25, 0.25, 0.25, 0.5, 0.5, 0.5, 0.5, 0.5, 0.5, 0.25, 0.25, 0.5, 0.75, 0.5, 0.5, 0.25, 0.25, 0.75, 0.5, 0.5, 0.75, 0.5, 0.25, 0.25, 0.25, 0.25, 0.25, 0.5, 0.5, 0.5, 0.75, 0.25, 0.5, 0.75, 0.5, 0.0, 0.0, 0.25]\n"
     ]
    }
   ],
   "source": [
    "#normalised author 1 scores\n",
    "norm_auth_1 =[]\n",
    "max1 = max(auth_1)\n",
    "min1 = min(auth_1)\n",
    "\n",
    "for a in range(len(auth_1)):\n",
    "    norm_auth_1.append((auth_1[a])/(max1))\n",
    "print(norm_auth_1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "3a01d024",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.25, 1.0, 0.75, 0.75, 0.75, 0.75, 1.0, 1.0, 0.25, 0.5, 0.5, 0.5, 0.25, 0.5, 0.5, 0.5, 0.0, 0.75, 0.75, 0.25, 0.25, 0.75, 0.5, 0.75, 0.5, 0.25, 0.75, 1.0, 0.0, 0.75, 0.75, 0.75, 0.75, 0.25, 0.5, 1.0, 0.5, 0.5, 0.5, 0.25, 0.0, 0.5, 0.5, 0.25, 1.0, 0.25, 0.25, 0.25, 0.0, 0.5, 0.25, 0.5, 0.25, 0.25, 0.25, 0.25, 0.25, 0.25, 0.25, 0.25, 0.5, 0.5, 0.25, 0.25, 0.25, 0.25, 0.25, 0.75, 0.25, 0.5, 0.25, 0.25, 0.5, 0.5, 0.25, 0.5, 0.5, 0.25, 0.25, 0.25, 0.25, 0.25, 0.25, 0.25, 0.5, 0.5, 0.25, 0.25, 0.5, 0.25, 0.0, 0.0, 0.5]\n"
     ]
    }
   ],
   "source": [
    "#normalised author 2 scores\n",
    "norm_auth_2 =[]\n",
    "max1 = max(auth_2)\n",
    "min1 = min(auth_2)\n",
    "\n",
    "for a in range(len(auth_2)):\n",
    "    norm_auth_2.append((auth_2[a])/(max1))\n",
    "print(norm_auth_2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "d271f92b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.5, 1.0, 0.75, 0.75, 0.75, 0.75, 1.0, 1.0, 0.5, 0.5, 0.75, 0.75, 0.5, 0.75, 0.5, 0.75, 0.25, 0.75, 0.75, 0.25, 0.5, 0.75, 0.75, 0.75, 0.75, 0.25, 0.75, 1.0, 0.0, 1.0, 0.75, 1.0, 0.75, 0.25, 0.75, 1.0, 0.75, 0.5, 0.75, 0.5, 0.0, 0.75, 0.5, 0.25, 1.0, 0.25, 0.5, 0.5, 0.0, 0.75, 0.5, 0.5, 0.5, 0.25, 0.25, 0.5, 0.5, 0.25, 0.5, 0.25, 0.5, 0.5, 0.5, 0.25, 0.25, 0.25, 0.5, 0.75, 0.5, 0.75, 0.25, 0.25, 0.75, 0.75, 0.5, 0.75, 0.5, 0.25, 0.25, 0.25, 0.25, 0.25, 0.25, 0.5, 0.5, 0.5, 0.25, 0.5, 0.75, 0.5, 0.0, 0.0, 0.25]\n"
     ]
    }
   ],
   "source": [
    "#normalised author 3 scores\n",
    "norm_auth_3 =[]\n",
    "max1 = max(auth_3)\n",
    "min1 = min(auth_3)\n",
    "\n",
    "for a in range(len(auth_3)):\n",
    "    norm_auth_3.append((auth_3[a])/(max1))\n",
    "print(norm_auth_3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "fa67ac97",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.42, 1.0, 0.75, 0.75, 0.75, 0.75, 1.0, 1.0, 0.42, 0.5, 0.67, 0.67, 0.42, 0.58, 0.5, 0.67, 0.17, 0.75, 0.75, 0.33, 0.42, 0.75, 0.67, 0.75, 0.67, 0.25, 0.75, 1.0, 0.08, 0.92, 0.75, 0.83, 0.75, 0.25, 0.67, 1.0, 0.67, 0.5, 0.67, 0.33, 0.0, 0.67, 0.5, 0.25, 1.0, 0.25, 0.42, 0.42, 0.0, 0.58, 0.42, 0.42, 0.42, 0.25, 0.25, 0.33, 0.33, 0.25, 0.42, 0.33, 0.5, 0.5, 0.42, 0.33, 0.25, 0.25, 0.42, 0.75, 0.42, 0.58, 0.25, 0.25, 0.67, 0.58, 0.42, 0.67, 0.5, 0.25, 0.25, 0.25, 0.25, 0.25, 0.33, 0.42, 0.5, 0.58, 0.25, 0.42, 0.67, 0.42, 0.0, 0.0, 0.33]\n"
     ]
    }
   ],
   "source": [
    "#Average normalised author scores\n",
    "avg_norm_score = []\n",
    "for i in range(len(auth_1)):\n",
    "    avg_norm_score.append(round((norm_auth_1[i]+norm_auth_2[i]+norm_auth_3[i])/3,2))\n",
    "print(avg_norm_score)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "72508f6c",
   "metadata": {},
   "source": [
    "# METEOR without Semantic Scoring"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "ca6604a0",
   "metadata": {},
   "outputs": [],
   "source": [
    "from nltk.stem.porter import PorterStemmer\n",
    "from nltk.corpus import wordnet\n",
    "from itertools import chain, product\n",
    "\n",
    "\n",
    "def _generate_enums(hypothesis, reference, preprocess=str.lower):\n",
    "    \"\"\"\n",
    "    Takes in string inputs for hypothesis and reference and returns\n",
    "    enumerated word lists for each of them\n",
    "\n",
    "    :param hypothesis: hypothesis string\n",
    "    :type hypothesis: str\n",
    "    :param reference: reference string\n",
    "    :type reference: str\n",
    "    :preprocess: preprocessing method (default str.lower)\n",
    "    :type preprocess: method\n",
    "    :return: enumerated words list\n",
    "    :rtype: list of 2D tuples, list of 2D tuples\n",
    "    \"\"\"\n",
    "    hypothesis_list = list(enumerate(preprocess(hypothesis).split()))\n",
    "    reference_list = list(enumerate(preprocess(reference).split()))\n",
    "    return hypothesis_list, reference_list\n",
    "\n",
    "\n",
    "def exact_match(hypothesis, reference):\n",
    "    \"\"\"\n",
    "    matches exact words in hypothesis and reference\n",
    "    and returns a word mapping based on the enumerated\n",
    "    word id between hypothesis and reference\n",
    "\n",
    "    :param hypothesis: hypothesis string\n",
    "    :type hypothesis: str\n",
    "    :param reference: reference string\n",
    "    :type reference: str\n",
    "    :return: enumerated matched tuples, enumerated unmatched hypothesis tuples,\n",
    "             enumerated unmatched reference tuples\n",
    "    :rtype: list of 2D tuples, list of 2D tuples,  list of 2D tuples\n",
    "    \"\"\"\n",
    "    hypothesis_list, reference_list = _generate_enums(hypothesis, reference)\n",
    "    return _match_enums(hypothesis_list, reference_list)\n",
    "\n",
    "\n",
    "\n",
    "def _match_enums(enum_hypothesis_list, enum_reference_list):\n",
    "    \"\"\"\n",
    "    matches exact words in hypothesis and reference and returns\n",
    "    a word mapping between enum_hypothesis_list and enum_reference_list\n",
    "    based on the enumerated word id.\n",
    "\n",
    "    :param enum_hypothesis_list: enumerated hypothesis list\n",
    "    :type enum_hypothesis_list: list of tuples\n",
    "    :param enum_reference_list: enumerated reference list\n",
    "    :type enum_reference_list: list of 2D tuples\n",
    "    :return: enumerated matched tuples, enumerated unmatched hypothesis tuples,\n",
    "             enumerated unmatched reference tuples\n",
    "    :rtype: list of 2D tuples, list of 2D tuples,  list of 2D tuples\n",
    "    \"\"\"\n",
    "    word_match = []\n",
    "    for i in range(len(enum_hypothesis_list))[::-1]:\n",
    "        for j in range(len(enum_reference_list))[::-1]:\n",
    "            if enum_hypothesis_list[i][1] == enum_reference_list[j][1]:\n",
    "                word_match.append(\n",
    "                    (enum_hypothesis_list[i][0], enum_reference_list[j][0])\n",
    "                )\n",
    "                (enum_hypothesis_list.pop(i)[1], enum_reference_list.pop(j)[1])\n",
    "                break\n",
    "    return word_match, enum_hypothesis_list, enum_reference_list\n",
    "\n",
    "\n",
    "def _enum_stem_match(\n",
    "    enum_hypothesis_list, enum_reference_list, stemmer=PorterStemmer()\n",
    "):\n",
    "    \"\"\"\n",
    "    Stems each word and matches them in hypothesis and reference\n",
    "    and returns a word mapping between enum_hypothesis_list and\n",
    "    enum_reference_list based on the enumerated word id. The function also\n",
    "    returns a enumerated list of unmatched words for hypothesis and reference.\n",
    "\n",
    "    :param enum_hypothesis_list:\n",
    "    :type enum_hypothesis_list:\n",
    "    :param enum_reference_list:\n",
    "    :type enum_reference_list:\n",
    "    :param stemmer: nltk.stem.api.StemmerI object (default PorterStemmer())\n",
    "    :type stemmer: nltk.stem.api.StemmerI or any class that implements a stem method\n",
    "    :return: enumerated matched tuples, enumerated unmatched hypothesis tuples,\n",
    "             enumerated unmatched reference tuples\n",
    "    :rtype: list of 2D tuples, list of 2D tuples,  list of 2D tuples\n",
    "    \"\"\"\n",
    "    stemmed_enum_list1 = [\n",
    "        (word_pair[0], stemmer.stem(word_pair[1])) for word_pair in enum_hypothesis_list\n",
    "    ]\n",
    "\n",
    "    stemmed_enum_list2 = [\n",
    "        (word_pair[0], stemmer.stem(word_pair[1])) for word_pair in enum_reference_list\n",
    "    ]\n",
    "\n",
    "    word_match, enum_unmat_hypo_list, enum_unmat_ref_list = _match_enums(\n",
    "        stemmed_enum_list1, stemmed_enum_list2\n",
    "    )\n",
    "\n",
    "    enum_unmat_hypo_list = (\n",
    "        list(zip(*enum_unmat_hypo_list)) if len(enum_unmat_hypo_list) > 0 else []\n",
    "    )\n",
    "\n",
    "    enum_unmat_ref_list = (\n",
    "        list(zip(*enum_unmat_ref_list)) if len(enum_unmat_ref_list) > 0 else []\n",
    "    )\n",
    "\n",
    "    enum_hypothesis_list = list(\n",
    "        filter(lambda x: x[0] not in enum_unmat_hypo_list, enum_hypothesis_list)\n",
    "    )\n",
    "\n",
    "    enum_reference_list = list(\n",
    "        filter(lambda x: x[0] not in enum_unmat_ref_list, enum_reference_list)\n",
    "    )\n",
    "\n",
    "    return word_match, enum_hypothesis_list, enum_reference_list\n",
    "\n",
    "\n",
    "def stem_match(hypothesis, reference, stemmer=PorterStemmer()):\n",
    "    \"\"\"\n",
    "    Stems each word and matches them in hypothesis and reference\n",
    "    and returns a word mapping between hypothesis and reference\n",
    "\n",
    "    :param hypothesis:\n",
    "    :type hypothesis:\n",
    "    :param reference:\n",
    "    :type reference:\n",
    "    :param stemmer: nltk.stem.api.StemmerI object (default PorterStemmer())\n",
    "    :type stemmer: nltk.stem.api.StemmerI or any class that\n",
    "                   implements a stem method\n",
    "    :return: enumerated matched tuples, enumerated unmatched hypothesis tuples,\n",
    "             enumerated unmatched reference tuples\n",
    "    :rtype: list of 2D tuples, list of 2D tuples,  list of 2D tuples\n",
    "    \"\"\"\n",
    "    enum_hypothesis_list, enum_reference_list = _generate_enums(hypothesis, reference)\n",
    "    return _enum_stem_match(enum_hypothesis_list, enum_reference_list, stemmer=stemmer)\n",
    "\n",
    "\n",
    "\n",
    "def _enum_wordnetsyn_match(enum_hypothesis_list, enum_reference_list, wordnet=wordnet):\n",
    "    \"\"\"\n",
    "    Matches each word in reference to a word in hypothesis\n",
    "    if any synonym of a hypothesis word is the exact match\n",
    "    to the reference word.\n",
    "\n",
    "    :param enum_hypothesis_list: enumerated hypothesis list\n",
    "    :param enum_reference_list: enumerated reference list\n",
    "    :param wordnet: a wordnet corpus reader object (default nltk.corpus.wordnet)\n",
    "    :type wordnet: WordNetCorpusReader\n",
    "    :return: list of matched tuples, unmatched hypothesis list, unmatched reference list\n",
    "    :rtype:  list of tuples, list of tuples, list of tuples\n",
    "\n",
    "    \"\"\"\n",
    "    word_match = []\n",
    "    for i in range(len(enum_hypothesis_list))[::-1]:\n",
    "        hypothesis_syns = set(\n",
    "            chain.from_iterable(\n",
    "                (\n",
    "                    lemma.name()\n",
    "                    for lemma in synset.lemmas()\n",
    "                    if lemma.name().find(\"_\") < 0\n",
    "                )\n",
    "                for synset in wordnet.synsets(enum_hypothesis_list[i][1])\n",
    "            )\n",
    "        ).union({enum_hypothesis_list[i][1]})\n",
    "        for j in range(len(enum_reference_list))[::-1]:\n",
    "            if enum_reference_list[j][1] in hypothesis_syns:\n",
    "                word_match.append(\n",
    "                    (enum_hypothesis_list[i][0], enum_reference_list[j][0])\n",
    "                )\n",
    "                enum_hypothesis_list.pop(i), enum_reference_list.pop(j)\n",
    "                break\n",
    "    return word_match, enum_hypothesis_list, enum_reference_list\n",
    "\n",
    "\n",
    "def wordnetsyn_match(hypothesis, reference, wordnet=wordnet):\n",
    "    \"\"\"\n",
    "    Matches each word in reference to a word in hypothesis if any synonym\n",
    "    of a hypothesis word is the exact match to the reference word.\n",
    "\n",
    "    :param hypothesis: hypothesis string\n",
    "    :param reference: reference string\n",
    "    :param wordnet: a wordnet corpus reader object (default nltk.corpus.wordnet)\n",
    "    :type wordnet: WordNetCorpusReader\n",
    "    :return: list of mapped tuples\n",
    "    :rtype: list of tuples\n",
    "    \"\"\"\n",
    "    enum_hypothesis_list, enum_reference_list = _generate_enums(hypothesis, reference)\n",
    "    return _enum_wordnetsyn_match(\n",
    "        enum_hypothesis_list, enum_reference_list, wordnet=wordnet\n",
    "    )\n",
    "\n",
    "\n",
    "\n",
    "def _enum_allign_words(\n",
    "    enum_hypothesis_list, enum_reference_list, stemmer=PorterStemmer(), wordnet=wordnet\n",
    "):\n",
    "    \"\"\"\n",
    "    Aligns/matches words in the hypothesis to reference by sequentially\n",
    "    applying exact match, stemmed match and wordnet based synonym match.\n",
    "    in case there are multiple matches the match which has the least number\n",
    "    of crossing is chosen. Takes enumerated list as input instead of\n",
    "    string input\n",
    "\n",
    "    :param enum_hypothesis_list: enumerated hypothesis list\n",
    "    :param enum_reference_list: enumerated reference list\n",
    "    :param stemmer: nltk.stem.api.StemmerI object (default PorterStemmer())\n",
    "    :type stemmer: nltk.stem.api.StemmerI or any class that implements a stem method\n",
    "    :param wordnet: a wordnet corpus reader object (default nltk.corpus.wordnet)\n",
    "    :type wordnet: WordNetCorpusReader\n",
    "    :return: sorted list of matched tuples, unmatched hypothesis list,\n",
    "             unmatched reference list\n",
    "    :rtype: list of tuples, list of tuples, list of tuples\n",
    "    \"\"\"\n",
    "    exact_matches, enum_hypothesis_list, enum_reference_list = _match_enums(\n",
    "        enum_hypothesis_list, enum_reference_list\n",
    "    )\n",
    "\n",
    "    stem_matches, enum_hypothesis_list, enum_reference_list = _enum_stem_match(\n",
    "        enum_hypothesis_list, enum_reference_list, stemmer=stemmer\n",
    "    )\n",
    "\n",
    "    wns_matches, enum_hypothesis_list, enum_reference_list = _enum_wordnetsyn_match(\n",
    "        enum_hypothesis_list, enum_reference_list, wordnet=wordnet\n",
    "    )\n",
    "\n",
    "    return (\n",
    "        sorted(\n",
    "            exact_matches, key=lambda wordpair: wordpair[0]\n",
    "        ),\n",
    "        enum_hypothesis_list,\n",
    "        enum_reference_list,\n",
    "    )\n",
    "\n",
    "\n",
    "def allign_words(hypothesis, reference, stemmer=PorterStemmer(), wordnet=wordnet):\n",
    "    \"\"\"\n",
    "    Aligns/matches words in the hypothesis to reference by sequentially\n",
    "    applying exact match, stemmed match and wordnet based synonym match.\n",
    "    In case there are multiple matches the match which has the least number\n",
    "    of crossing is chosen.\n",
    "\n",
    "    :param hypothesis: hypothesis string\n",
    "    :param reference: reference string\n",
    "    :param stemmer: nltk.stem.api.StemmerI object (default PorterStemmer())\n",
    "    :type stemmer: nltk.stem.api.StemmerI or any class that implements a stem method\n",
    "    :param wordnet: a wordnet corpus reader object (default nltk.corpus.wordnet)\n",
    "    :type wordnet: WordNetCorpusReader\n",
    "    :return: sorted list of matched tuples, unmatched hypothesis list, unmatched reference list\n",
    "    :rtype: list of tuples, list of tuples, list of tuples\n",
    "    \"\"\"\n",
    "    enum_hypothesis_list, enum_reference_list = _generate_enums(hypothesis, reference)\n",
    "    return _enum_allign_words(\n",
    "        enum_hypothesis_list, enum_reference_list, stemmer=stemmer, wordnet=wordnet\n",
    "    )\n",
    "\n",
    "\n",
    "\n",
    "def _count_chunks(matches):\n",
    "    \"\"\"\n",
    "    Counts the fewest possible number of chunks such that matched unigrams\n",
    "    of each chunk are adjacent to each other. This is used to caluclate the\n",
    "    fragmentation part of the metric.\n",
    "\n",
    "    :param matches: list containing a mapping of matched words (output of allign_words)\n",
    "    :return: Number of chunks a sentence is divided into post allignment\n",
    "    :rtype: int\n",
    "    \"\"\"\n",
    "    i = 0\n",
    "    chunks = 1\n",
    "    while i < len(matches) - 1:\n",
    "        if (matches[i + 1][0] == matches[i][0] + 1) and (\n",
    "            matches[i + 1][1] == matches[i][1] + 1\n",
    "        ):\n",
    "            i += 1\n",
    "            continue\n",
    "        i += 1\n",
    "        chunks += 1\n",
    "    return chunks\n",
    "\n",
    "\n",
    "def single_meteor_score(\n",
    "    reference,\n",
    "    hypothesis,\n",
    "    preprocess=str.lower,\n",
    "    stemmer=PorterStemmer(),\n",
    "    wordnet=wordnet,\n",
    "    alpha=0.9,\n",
    "    beta=3,\n",
    "    gamma=0.5,\n",
    "):\n",
    "    \"\"\"\n",
    "    Calculates METEOR score for single hypothesis and reference as per\n",
    "    \"Meteor: An Automatic Metric for MT Evaluation with HighLevels of\n",
    "    Correlation with Human Judgments\" by Alon Lavie and Abhaya Agarwal,\n",
    "    in Proceedings of ACL.\n",
    "    http://www.cs.cmu.edu/~alavie/METEOR/pdf/Lavie-Agarwal-2007-METEOR.pdf\n",
    "\n",
    "\n",
    "    >>> hypothesis1 = 'It is a guide to action which ensures that the military always obeys the commands of the party'\n",
    "\n",
    "    >>> reference1 = 'It is a guide to action that ensures that the military will forever heed Party commands'\n",
    "\n",
    "\n",
    "    >>> round(single_meteor_score(reference1, hypothesis1),4)\n",
    "    0.7398\n",
    "\n",
    "        If there is no words match during the alignment the method returns the\n",
    "        score as 0. We can safely  return a zero instead of raising a\n",
    "        division by zero error as no match usually implies a bad translation.\n",
    "\n",
    "    >>> round(meteor_score('this is a cat', 'non matching hypothesis'),4)\n",
    "    0.0\n",
    "\n",
    "    :param references: reference sentences\n",
    "    :type references: list(str)\n",
    "    :param hypothesis: a hypothesis sentence\n",
    "    :type hypothesis: str\n",
    "    :param preprocess: preprocessing function (default str.lower)\n",
    "    :type preprocess: method\n",
    "    :param stemmer: nltk.stem.api.StemmerI object (default PorterStemmer())\n",
    "    :type stemmer: nltk.stem.api.StemmerI or any class that implements a stem method\n",
    "    :param wordnet: a wordnet corpus reader object (default nltk.corpus.wordnet)\n",
    "    :type wordnet: WordNetCorpusReader\n",
    "    :param alpha: parameter for controlling relative weights of precision and recall.\n",
    "    :type alpha: float\n",
    "    :param beta: parameter for controlling shape of penalty as a\n",
    "                 function of as a function of fragmentation.\n",
    "    :type beta: float\n",
    "    :param gamma: relative weight assigned to fragmentation penality.\n",
    "    :type gamma: float\n",
    "    :return: The sentence-level METEOR score.\n",
    "    :rtype: float\n",
    "    \"\"\"\n",
    "    enum_hypothesis, enum_reference = _generate_enums(\n",
    "        hypothesis, reference, preprocess=preprocess\n",
    "    )\n",
    "    translation_length = len(enum_hypothesis)\n",
    "    reference_length = len(enum_reference)\n",
    "    matches, _, _ = _enum_allign_words(enum_hypothesis, enum_reference, stemmer=stemmer)\n",
    "    matches_count = len(matches)\n",
    "    try:\n",
    "        precision = float(matches_count) / translation_length\n",
    "        recall = float(matches_count) / reference_length\n",
    "        fmean = (precision * recall) / (alpha * precision + (1 - alpha) * recall)\n",
    "        chunk_count = float(_count_chunks(matches))\n",
    "        frag_frac = chunk_count / matches_count\n",
    "    except ZeroDivisionError:\n",
    "        return 0.0\n",
    "    penalty = gamma * frag_frac ** beta\n",
    "    return (1 - penalty) * fmean\n",
    "\n",
    "\n",
    "\n",
    "def meteor_score_woss(\n",
    "    references,\n",
    "    hypothesis,\n",
    "    preprocess=str.lower,\n",
    "    stemmer=PorterStemmer(),\n",
    "    wordnet=wordnet,\n",
    "    alpha=0.9,\n",
    "    beta=3,\n",
    "    gamma=0.5,\n",
    "):\n",
    "    \"\"\"\n",
    "    Calculates METEOR score for hypothesis with multiple references as\n",
    "    described in \"Meteor: An Automatic Metric for MT Evaluation with\n",
    "    HighLevels of Correlation with Human Judgments\" by Alon Lavie and\n",
    "    Abhaya Agarwal, in Proceedings of ACL.\n",
    "    http://www.cs.cmu.edu/~alavie/METEOR/pdf/Lavie-Agarwal-2007-METEOR.pdf\n",
    "\n",
    "\n",
    "    In case of multiple references the best score is chosen. This method\n",
    "    iterates over single_meteor_score and picks the best pair among all\n",
    "    the references for a given hypothesis\n",
    "\n",
    "    >>> hypothesis1 = 'It is a guide to action which ensures that the military always obeys the commands of the party'\n",
    "    >>> hypothesis2 = 'It is to insure the troops forever hearing the activity guidebook that party direct'\n",
    "\n",
    "    >>> reference1 = 'It is a guide to action that ensures that the military will forever heed Party commands'\n",
    "    >>> reference2 = 'It is the guiding principle which guarantees the military forces always being under the command of the Party'\n",
    "    >>> reference3 = 'It is the practical guide for the army always to heed the directions of the party'\n",
    "\n",
    "    >>> round(meteor_score([reference1, reference2, reference3], hypothesis1),4)\n",
    "    0.7398\n",
    "\n",
    "        If there is no words match during the alignment the method returns the\n",
    "        score as 0. We can safely  return a zero instead of raising a\n",
    "        division by zero error as no match usually implies a bad translation.\n",
    "\n",
    "    >>> round(meteor_score(['this is a cat'], 'non matching hypothesis'),4)\n",
    "    0.0\n",
    "\n",
    "    :param references: reference sentences\n",
    "    :type references: list(str)\n",
    "    :param hypothesis: a hypothesis sentence\n",
    "    :type hypothesis: str\n",
    "    :param preprocess: preprocessing function (default str.lower)\n",
    "    :type preprocess: method\n",
    "    :param stemmer: nltk.stem.api.StemmerI object (default PorterStemmer())\n",
    "    :type stemmer: nltk.stem.api.StemmerI or any class that implements a stem method\n",
    "    :param wordnet: a wordnet corpus reader object (default nltk.corpus.wordnet)\n",
    "    :type wordnet: WordNetCorpusReader\n",
    "    :param alpha: parameter for controlling relative weights of precision and recall.\n",
    "    :type alpha: float\n",
    "    :param beta: parameter for controlling shape of penalty as a function\n",
    "                 of as a function of fragmentation.\n",
    "    :type beta: float\n",
    "    :param gamma: relative weight assigned to fragmentation penality.\n",
    "    :type gamma: float\n",
    "    :return: The sentence-level METEOR score.\n",
    "    :rtype: float\n",
    "    \"\"\"\n",
    "    return max(\n",
    "        [\n",
    "            single_meteor_score(\n",
    "                reference,\n",
    "                hypothesis,\n",
    "                preprocess=preprocess,\n",
    "                stemmer=stemmer,\n",
    "                wordnet=wordnet,\n",
    "                alpha=alpha,\n",
    "                beta=beta,\n",
    "                gamma=gamma,\n",
    "            )\n",
    "            for reference in references\n",
    "        ]\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "14f17169",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0.5, 0.5, 0.5, 0.5, 0.5, 0.5, 0.5, 0.11, 0.25, 0.33, 0.33, 0.38, 0.34, 0.26, 0.34, 0.17, 0.26, 0.47, 0.47, 0.17, 0.38, 0.25, 0.48, 0.25, 0.38, 0.36, 0.13, 0.13, 0.3, 0.32, 0.26, 0.24, 0.16, 0.26, 0.48, 0.11, 0.32, 0.2, 0.32, 0.25, 0.1, 0.45, 0.5, 0.18, 0.31, 0.32, 0.2, 0.1, 0.33, 0.22, 0.25, 0.49, 0.09, 0.16, 0.22, 0.1, 0.42, 0.24, 0.17, 0.22, 0.16, 0.29, 0.17, 0.25, 0.15, 0.15, 0.08, 0.28, 0.18, 0.12, 0.15, 0.12, 0.15, 0.15, 0.08, 0.07, 0.29, 0.2, 0.14, 0.4, 0.15, 0.19, 0.36, 0.21, 0.13, 0.13, 0.12, 0.14, 0.16, 0.13, 0.11, 0.14, 0.16, 0.13, 0.33, 0.33, 0.2, 0.1, 0.2, 0.12]\n"
     ]
    }
   ],
   "source": [
    "meteor_woss=[]\n",
    "for i in range(len(auth_1)):\n",
    "    meteor_woss.append(round(meteor_score_woss([refs[i]], preds[i]),2))\n",
    "print(meteor_woss)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "cbdf1b95",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.22, 0.5, 0.66, 0.66, 0.76, 0.68, 0.52, 0.68, 0.34, 0.52, 0.94, 0.94, 0.34, 0.76, 0.5, 0.96, 0.5, 0.76, 0.72, 0.26, 0.26, 0.6, 0.64, 0.52, 0.48, 0.32, 0.52, 0.96, 0.22, 0.64, 0.4, 0.64, 0.5, 0.2, 0.9, 1.0, 0.36, 0.62, 0.64, 0.4, 0.2, 0.66, 0.44, 0.5, 0.98, 0.18, 0.32, 0.44, 0.2, 0.84, 0.48, 0.34, 0.44, 0.32, 0.58, 0.34, 0.5, 0.3, 0.3, 0.16, 0.56, 0.36, 0.24, 0.3, 0.24, 0.3, 0.3, 0.16, 0.14, 0.58, 0.4, 0.28, 0.8, 0.3, 0.38, 0.72, 0.42, 0.26, 0.26, 0.24, 0.28, 0.32, 0.26, 0.22, 0.28, 0.32, 0.26, 0.66, 0.66, 0.4, 0.2, 0.4, 0.24]\n"
     ]
    }
   ],
   "source": [
    "norm_m_woss = []\n",
    "\n",
    "max_m=max(meteor_woss)\n",
    "min_m=min(meteor_woss)\n",
    "\n",
    "for a in range(len(meteor_woss)):\n",
    "    norm_m_woss.append(round(((meteor_woss[a])/(max_m)),2))\n",
    "print(norm_m_woss)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "52d88192",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Spearmans correlation coefficient: 0.760\n",
      "correlated (reject H0) p=0.000\n"
     ]
    }
   ],
   "source": [
    "from scipy.stats import spearmanr\n",
    "# calculate spearman's correlation\n",
    "coef, p = spearmanr(norm_m_woss, avg_norm_score)\n",
    "print('Spearmans correlation coefficient: %.3f' % coef)\n",
    "# interpret the significance\n",
    "alpha = 0.05\n",
    "if p > alpha:\n",
    "\tprint('uncorrelated (fail to reject H0) p=%.3f' % p)\n",
    "else:\n",
    "\tprint('correlated (reject H0) p=%.3f' % p)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ae8b0b77",
   "metadata": {},
   "source": [
    "# METEOR with Semantic Scoring"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "21a40b80",
   "metadata": {},
   "outputs": [],
   "source": [
    "from nltk.stem.porter import PorterStemmer\n",
    "from nltk.corpus import wordnet\n",
    "from itertools import chain, product\n",
    "\n",
    "\n",
    "def _generate_enums(hypothesis, reference, preprocess=str.lower):\n",
    "    \"\"\"\n",
    "    Takes in string inputs for hypothesis and reference and returns\n",
    "    enumerated word lists for each of them\n",
    "\n",
    "    :param hypothesis: hypothesis string\n",
    "    :type hypothesis: str\n",
    "    :param reference: reference string\n",
    "    :type reference: str\n",
    "    :preprocess: preprocessing method (default str.lower)\n",
    "    :type preprocess: method\n",
    "    :return: enumerated words list\n",
    "    :rtype: list of 2D tuples, list of 2D tuples\n",
    "    \"\"\"\n",
    "    hypothesis_list = list(enumerate(preprocess(hypothesis).split()))\n",
    "    reference_list = list(enumerate(preprocess(reference).split()))\n",
    "    return hypothesis_list, reference_list\n",
    "\n",
    "\n",
    "def exact_match(hypothesis, reference):\n",
    "    \"\"\"\n",
    "    matches exact words in hypothesis and reference\n",
    "    and returns a word mapping based on the enumerated\n",
    "    word id between hypothesis and reference\n",
    "\n",
    "    :param hypothesis: hypothesis string\n",
    "    :type hypothesis: str\n",
    "    :param reference: reference string\n",
    "    :type reference: str\n",
    "    :return: enumerated matched tuples, enumerated unmatched hypothesis tuples,\n",
    "             enumerated unmatched reference tuples\n",
    "    :rtype: list of 2D tuples, list of 2D tuples,  list of 2D tuples\n",
    "    \"\"\"\n",
    "    hypothesis_list, reference_list = _generate_enums(hypothesis, reference)\n",
    "    return _match_enums(hypothesis_list, reference_list)\n",
    "\n",
    "\n",
    "\n",
    "def _match_enums(enum_hypothesis_list, enum_reference_list):\n",
    "    \"\"\"\n",
    "    matches exact words in hypothesis and reference and returns\n",
    "    a word mapping between enum_hypothesis_list and enum_reference_list\n",
    "    based on the enumerated word id.\n",
    "\n",
    "    :param enum_hypothesis_list: enumerated hypothesis list\n",
    "    :type enum_hypothesis_list: list of tuples\n",
    "    :param enum_reference_list: enumerated reference list\n",
    "    :type enum_reference_list: list of 2D tuples\n",
    "    :return: enumerated matched tuples, enumerated unmatched hypothesis tuples,\n",
    "             enumerated unmatched reference tuples\n",
    "    :rtype: list of 2D tuples, list of 2D tuples,  list of 2D tuples\n",
    "    \"\"\"\n",
    "    word_match = []\n",
    "    for i in range(len(enum_hypothesis_list))[::-1]:\n",
    "        for j in range(len(enum_reference_list))[::-1]:\n",
    "            if enum_hypothesis_list[i][1] == enum_reference_list[j][1]:\n",
    "                word_match.append(\n",
    "                    (enum_hypothesis_list[i][0], enum_reference_list[j][0])\n",
    "                )\n",
    "                (enum_hypothesis_list.pop(i)[1], enum_reference_list.pop(j)[1])\n",
    "                break\n",
    "    return word_match, enum_hypothesis_list, enum_reference_list\n",
    "\n",
    "\n",
    "def _enum_stem_match(\n",
    "    enum_hypothesis_list, enum_reference_list, stemmer=PorterStemmer()\n",
    "):\n",
    "    \"\"\"\n",
    "    Stems each word and matches them in hypothesis and reference\n",
    "    and returns a word mapping between enum_hypothesis_list and\n",
    "    enum_reference_list based on the enumerated word id. The function also\n",
    "    returns a enumerated list of unmatched words for hypothesis and reference.\n",
    "\n",
    "    :param enum_hypothesis_list:\n",
    "    :type enum_hypothesis_list:\n",
    "    :param enum_reference_list:\n",
    "    :type enum_reference_list:\n",
    "    :param stemmer: nltk.stem.api.StemmerI object (default PorterStemmer())\n",
    "    :type stemmer: nltk.stem.api.StemmerI or any class that implements a stem method\n",
    "    :return: enumerated matched tuples, enumerated unmatched hypothesis tuples,\n",
    "             enumerated unmatched reference tuples\n",
    "    :rtype: list of 2D tuples, list of 2D tuples,  list of 2D tuples\n",
    "    \"\"\"\n",
    "    stemmed_enum_list1 = [\n",
    "        (word_pair[0], stemmer.stem(word_pair[1])) for word_pair in enum_hypothesis_list\n",
    "    ]\n",
    "\n",
    "    stemmed_enum_list2 = [\n",
    "        (word_pair[0], stemmer.stem(word_pair[1])) for word_pair in enum_reference_list\n",
    "    ]\n",
    "\n",
    "    word_match, enum_unmat_hypo_list, enum_unmat_ref_list = _match_enums(\n",
    "        stemmed_enum_list1, stemmed_enum_list2\n",
    "    )\n",
    "\n",
    "    enum_unmat_hypo_list = (\n",
    "        list(zip(*enum_unmat_hypo_list)) if len(enum_unmat_hypo_list) > 0 else []\n",
    "    )\n",
    "\n",
    "    enum_unmat_ref_list = (\n",
    "        list(zip(*enum_unmat_ref_list)) if len(enum_unmat_ref_list) > 0 else []\n",
    "    )\n",
    "\n",
    "    enum_hypothesis_list = list(\n",
    "        filter(lambda x: x[0] not in enum_unmat_hypo_list, enum_hypothesis_list)\n",
    "    )\n",
    "\n",
    "    enum_reference_list = list(\n",
    "        filter(lambda x: x[0] not in enum_unmat_ref_list, enum_reference_list)\n",
    "    )\n",
    "\n",
    "    return word_match, enum_hypothesis_list, enum_reference_list\n",
    "\n",
    "\n",
    "def stem_match(hypothesis, reference, stemmer=PorterStemmer()):\n",
    "    \"\"\"\n",
    "    Stems each word and matches them in hypothesis and reference\n",
    "    and returns a word mapping between hypothesis and reference\n",
    "\n",
    "    :param hypothesis:\n",
    "    :type hypothesis:\n",
    "    :param reference:\n",
    "    :type reference:\n",
    "    :param stemmer: nltk.stem.api.StemmerI object (default PorterStemmer())\n",
    "    :type stemmer: nltk.stem.api.StemmerI or any class that\n",
    "                   implements a stem method\n",
    "    :return: enumerated matched tuples, enumerated unmatched hypothesis tuples,\n",
    "             enumerated unmatched reference tuples\n",
    "    :rtype: list of 2D tuples, list of 2D tuples,  list of 2D tuples\n",
    "    \"\"\"\n",
    "    enum_hypothesis_list, enum_reference_list = _generate_enums(hypothesis, reference)\n",
    "    return _enum_stem_match(enum_hypothesis_list, enum_reference_list, stemmer=stemmer)\n",
    "\n",
    "\n",
    "\n",
    "def _enum_wordnetsyn_match(enum_hypothesis_list, enum_reference_list, wordnet=wordnet):\n",
    "    \"\"\"\n",
    "    Matches each word in reference to a word in hypothesis\n",
    "    if any synonym of a hypothesis word is the exact match\n",
    "    to the reference word.\n",
    "\n",
    "    :param enum_hypothesis_list: enumerated hypothesis list\n",
    "    :param enum_reference_list: enumerated reference list\n",
    "    :param wordnet: a wordnet corpus reader object (default nltk.corpus.wordnet)\n",
    "    :type wordnet: WordNetCorpusReader\n",
    "    :return: list of matched tuples, unmatched hypothesis list, unmatched reference list\n",
    "    :rtype:  list of tuples, list of tuples, list of tuples\n",
    "\n",
    "    \"\"\"\n",
    "    word_match = []\n",
    "    for i in range(len(enum_hypothesis_list))[::-1]:\n",
    "        hypothesis_syns = set(\n",
    "            chain.from_iterable(\n",
    "                (\n",
    "                    lemma.name()\n",
    "                    for lemma in synset.lemmas()\n",
    "                    if lemma.name().find(\"_\") < 0\n",
    "                )\n",
    "                for synset in wordnet.synsets(enum_hypothesis_list[i][1])\n",
    "            )\n",
    "        ).union({enum_hypothesis_list[i][1]})\n",
    "        for j in range(len(enum_reference_list))[::-1]:\n",
    "            if enum_reference_list[j][1] in hypothesis_syns:\n",
    "                word_match.append(\n",
    "                    (enum_hypothesis_list[i][0], enum_reference_list[j][0])\n",
    "                )\n",
    "                enum_hypothesis_list.pop(i), enum_reference_list.pop(j)\n",
    "                break\n",
    "    return word_match, enum_hypothesis_list, enum_reference_list\n",
    "\n",
    "\n",
    "def wordnetsyn_match(hypothesis, reference, wordnet=wordnet):\n",
    "    \"\"\"\n",
    "    Matches each word in reference to a word in hypothesis if any synonym\n",
    "    of a hypothesis word is the exact match to the reference word.\n",
    "\n",
    "    :param hypothesis: hypothesis string\n",
    "    :param reference: reference string\n",
    "    :param wordnet: a wordnet corpus reader object (default nltk.corpus.wordnet)\n",
    "    :type wordnet: WordNetCorpusReader\n",
    "    :return: list of mapped tuples\n",
    "    :rtype: list of tuples\n",
    "    \"\"\"\n",
    "    enum_hypothesis_list, enum_reference_list = _generate_enums(hypothesis, reference)\n",
    "    return _enum_wordnetsyn_match(\n",
    "        enum_hypothesis_list, enum_reference_list, wordnet=wordnet\n",
    "    )\n",
    "\n",
    "\n",
    "\n",
    "def _enum_allign_words(\n",
    "    enum_hypothesis_list, enum_reference_list, stemmer=PorterStemmer(), wordnet=wordnet\n",
    "):\n",
    "    \"\"\"\n",
    "    Aligns/matches words in the hypothesis to reference by sequentially\n",
    "    applying exact match, stemmed match and wordnet based synonym match.\n",
    "    in case there are multiple matches the match which has the least number\n",
    "    of crossing is chosen. Takes enumerated list as input instead of\n",
    "    string input\n",
    "\n",
    "    :param enum_hypothesis_list: enumerated hypothesis list\n",
    "    :param enum_reference_list: enumerated reference list\n",
    "    :param stemmer: nltk.stem.api.StemmerI object (default PorterStemmer())\n",
    "    :type stemmer: nltk.stem.api.StemmerI or any class that implements a stem method\n",
    "    :param wordnet: a wordnet corpus reader object (default nltk.corpus.wordnet)\n",
    "    :type wordnet: WordNetCorpusReader\n",
    "    :return: sorted list of matched tuples, unmatched hypothesis list,\n",
    "             unmatched reference list\n",
    "    :rtype: list of tuples, list of tuples, list of tuples\n",
    "    \"\"\"\n",
    "    exact_matches, enum_hypothesis_list, enum_reference_list = _match_enums(\n",
    "        enum_hypothesis_list, enum_reference_list\n",
    "    )\n",
    "\n",
    "    stem_matches, enum_hypothesis_list, enum_reference_list = _enum_stem_match(\n",
    "        enum_hypothesis_list, enum_reference_list, stemmer=stemmer\n",
    "    )\n",
    "\n",
    "    wns_matches, enum_hypothesis_list, enum_reference_list = _enum_wordnetsyn_match(\n",
    "        enum_hypothesis_list, enum_reference_list, wordnet=wordnet\n",
    "    )\n",
    "\n",
    "    return (\n",
    "        sorted(\n",
    "            exact_matches + stem_matches + wns_matches, key=lambda wordpair: wordpair[0]\n",
    "        ),\n",
    "        enum_hypothesis_list,\n",
    "        enum_reference_list,\n",
    "    )\n",
    "\n",
    "\n",
    "def allign_words(hypothesis, reference, stemmer=PorterStemmer(), wordnet=wordnet):\n",
    "    \"\"\"\n",
    "    Aligns/matches words in the hypothesis to reference by sequentially\n",
    "    applying exact match, stemmed match and wordnet based synonym match.\n",
    "    In case there are multiple matches the match which has the least number\n",
    "    of crossing is chosen.\n",
    "\n",
    "    :param hypothesis: hypothesis string\n",
    "    :param reference: reference string\n",
    "    :param stemmer: nltk.stem.api.StemmerI object (default PorterStemmer())\n",
    "    :type stemmer: nltk.stem.api.StemmerI or any class that implements a stem method\n",
    "    :param wordnet: a wordnet corpus reader object (default nltk.corpus.wordnet)\n",
    "    :type wordnet: WordNetCorpusReader\n",
    "    :return: sorted list of matched tuples, unmatched hypothesis list, unmatched reference list\n",
    "    :rtype: list of tuples, list of tuples, list of tuples\n",
    "    \"\"\"\n",
    "    enum_hypothesis_list, enum_reference_list = _generate_enums(hypothesis, reference)\n",
    "    return _enum_allign_words(\n",
    "        enum_hypothesis_list, enum_reference_list, stemmer=stemmer, wordnet=wordnet\n",
    "    )\n",
    "\n",
    "\n",
    "\n",
    "def _count_chunks(matches):\n",
    "    \"\"\"\n",
    "    Counts the fewest possible number of chunks such that matched unigrams\n",
    "    of each chunk are adjacent to each other. This is used to caluclate the\n",
    "    fragmentation part of the metric.\n",
    "\n",
    "    :param matches: list containing a mapping of matched words (output of allign_words)\n",
    "    :return: Number of chunks a sentence is divided into post allignment\n",
    "    :rtype: int\n",
    "    \"\"\"\n",
    "    i = 0\n",
    "    chunks = 1\n",
    "    while i < len(matches) - 1:\n",
    "        if (matches[i + 1][0] == matches[i][0] + 1) and (\n",
    "            matches[i + 1][1] == matches[i][1] + 1\n",
    "        ):\n",
    "            i += 1\n",
    "            continue\n",
    "        i += 1\n",
    "        chunks += 1\n",
    "    return chunks\n",
    "\n",
    "\n",
    "def single_meteor_score(\n",
    "    reference,\n",
    "    hypothesis,\n",
    "    preprocess=str.lower,\n",
    "    stemmer=PorterStemmer(),\n",
    "    wordnet=wordnet,\n",
    "    alpha=0.9,\n",
    "    beta=3,\n",
    "    gamma=0.5,\n",
    "):\n",
    "    \"\"\"\n",
    "    Calculates METEOR score for single hypothesis and reference as per\n",
    "    \"Meteor: An Automatic Metric for MT Evaluation with HighLevels of\n",
    "    Correlation with Human Judgments\" by Alon Lavie and Abhaya Agarwal,\n",
    "    in Proceedings of ACL.\n",
    "    http://www.cs.cmu.edu/~alavie/METEOR/pdf/Lavie-Agarwal-2007-METEOR.pdf\n",
    "\n",
    "\n",
    "    >>> hypothesis1 = 'It is a guide to action which ensures that the military always obeys the commands of the party'\n",
    "\n",
    "    >>> reference1 = 'It is a guide to action that ensures that the military will forever heed Party commands'\n",
    "\n",
    "\n",
    "    >>> round(single_meteor_score(reference1, hypothesis1),4)\n",
    "    0.7398\n",
    "\n",
    "        If there is no words match during the alignment the method returns the\n",
    "        score as 0. We can safely  return a zero instead of raising a\n",
    "        division by zero error as no match usually implies a bad translation.\n",
    "\n",
    "    >>> round(meteor_score('this is a cat', 'non matching hypothesis'),4)\n",
    "    0.0\n",
    "\n",
    "    :param references: reference sentences\n",
    "    :type references: list(str)\n",
    "    :param hypothesis: a hypothesis sentence\n",
    "    :type hypothesis: str\n",
    "    :param preprocess: preprocessing function (default str.lower)\n",
    "    :type preprocess: method\n",
    "    :param stemmer: nltk.stem.api.StemmerI object (default PorterStemmer())\n",
    "    :type stemmer: nltk.stem.api.StemmerI or any class that implements a stem method\n",
    "    :param wordnet: a wordnet corpus reader object (default nltk.corpus.wordnet)\n",
    "    :type wordnet: WordNetCorpusReader\n",
    "    :param alpha: parameter for controlling relative weights of precision and recall.\n",
    "    :type alpha: float\n",
    "    :param beta: parameter for controlling shape of penalty as a\n",
    "                 function of as a function of fragmentation.\n",
    "    :type beta: float\n",
    "    :param gamma: relative weight assigned to fragmentation penality.\n",
    "    :type gamma: float\n",
    "    :return: The sentence-level METEOR score.\n",
    "    :rtype: float\n",
    "    \"\"\"\n",
    "    enum_hypothesis, enum_reference = _generate_enums(\n",
    "        hypothesis, reference, preprocess=preprocess\n",
    "    )\n",
    "    translation_length = len(enum_hypothesis)\n",
    "    reference_length = len(enum_reference)\n",
    "    matches, _, _ = _enum_allign_words(enum_hypothesis, enum_reference, stemmer=stemmer)\n",
    "    matches_count = len(matches)\n",
    "    try:\n",
    "        precision = float(matches_count) / translation_length\n",
    "        recall = float(matches_count) / reference_length\n",
    "        fmean = (precision * recall) / (alpha * precision + (1 - alpha) * recall)\n",
    "        chunk_count = float(_count_chunks(matches))\n",
    "        frag_frac = chunk_count / matches_count\n",
    "    except ZeroDivisionError:\n",
    "        return 0.0\n",
    "    penalty = gamma * frag_frac ** beta\n",
    "    return (1 - penalty) * fmean\n",
    "\n",
    "\n",
    "\n",
    "def meteor_score_wss(\n",
    "    references,\n",
    "    hypothesis,\n",
    "    preprocess=str.lower,\n",
    "    stemmer=PorterStemmer(),\n",
    "    wordnet=wordnet,\n",
    "    alpha=0.9,\n",
    "    beta=3,\n",
    "    gamma=0.5,\n",
    "):\n",
    "    \"\"\"\n",
    "    Calculates METEOR score for hypothesis with multiple references as\n",
    "    described in \"Meteor: An Automatic Metric for MT Evaluation with\n",
    "    HighLevels of Correlation with Human Judgments\" by Alon Lavie and\n",
    "    Abhaya Agarwal, in Proceedings of ACL.\n",
    "    http://www.cs.cmu.edu/~alavie/METEOR/pdf/Lavie-Agarwal-2007-METEOR.pdf\n",
    "\n",
    "\n",
    "    In case of multiple references the best score is chosen. This method\n",
    "    iterates over single_meteor_score and picks the best pair among all\n",
    "    the references for a given hypothesis\n",
    "\n",
    "    >>> hypothesis1 = 'It is a guide to action which ensures that the military always obeys the commands of the party'\n",
    "    >>> hypothesis2 = 'It is to insure the troops forever hearing the activity guidebook that party direct'\n",
    "\n",
    "    >>> reference1 = 'It is a guide to action that ensures that the military will forever heed Party commands'\n",
    "    >>> reference2 = 'It is the guiding principle which guarantees the military forces always being under the command of the Party'\n",
    "    >>> reference3 = 'It is the practical guide for the army always to heed the directions of the party'\n",
    "\n",
    "    >>> round(meteor_score([reference1, reference2, reference3], hypothesis1),4)\n",
    "    0.7398\n",
    "\n",
    "        If there is no words match during the alignment the method returns the\n",
    "        score as 0. We can safely  return a zero instead of raising a\n",
    "        division by zero error as no match usually implies a bad translation.\n",
    "\n",
    "    >>> round(meteor_score(['this is a cat'], 'non matching hypothesis'),4)\n",
    "    0.0\n",
    "\n",
    "    :param references: reference sentences\n",
    "    :type references: list(str)\n",
    "    :param hypothesis: a hypothesis sentence\n",
    "    :type hypothesis: str\n",
    "    :param preprocess: preprocessing function (default str.lower)\n",
    "    :type preprocess: method\n",
    "    :param stemmer: nltk.stem.api.StemmerI object (default PorterStemmer())\n",
    "    :type stemmer: nltk.stem.api.StemmerI or any class that implements a stem method\n",
    "    :param wordnet: a wordnet corpus reader object (default nltk.corpus.wordnet)\n",
    "    :type wordnet: WordNetCorpusReader\n",
    "    :param alpha: parameter for controlling relative weights of precision and recall.\n",
    "    :type alpha: float\n",
    "    :param beta: parameter for controlling shape of penalty as a function\n",
    "                 of as a function of fragmentation.\n",
    "    :type beta: float\n",
    "    :param gamma: relative weight assigned to fragmentation penality.\n",
    "    :type gamma: float\n",
    "    :return: The sentence-level METEOR score.\n",
    "    :rtype: float\n",
    "    \"\"\"\n",
    "    return max(\n",
    "        [\n",
    "            single_meteor_score(\n",
    "                reference,\n",
    "                hypothesis,\n",
    "                preprocess=preprocess,\n",
    "                stemmer=stemmer,\n",
    "                wordnet=wordnet,\n",
    "                alpha=alpha,\n",
    "                beta=beta,\n",
    "                gamma=gamma,\n",
    "            )\n",
    "            for reference in references\n",
    "        ]\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "ae4e05d1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0.5, 0.5, 0.98, 0.98, 0.94, 0.94, 0.94, 0.11, 1.28, 0.62, 0.62, 0.74, 0.68, 0.26, 0.34, 0.17, 0.49, 0.8, 0.8, 0.17, 0.64, 0.47, 0.89, 0.5, 0.38, 0.7, 0.13, 0.13, 0.56, 0.56, 0.45, 0.47, 0.16, 0.45, 0.89, 0.11, 0.69, 0.63, 0.72, 0.43, 0.1, 0.77, 1.0, 0.33, 0.31, 0.53, 0.37, 0.1, 0.65, 0.41, 0.42, 0.98, 0.09, 0.16, 0.22, 0.1, 0.69, 0.24, 0.34, 0.22, 0.27, 0.52, 0.38, 0.43, 0.15, 0.15, 0.08, 0.37, 0.34, 0.12, 0.15, 0.12, 0.15, 0.15, 0.17, 0.07, 0.29, 0.2, 0.21, 0.4, 0.28, 0.19, 0.61, 0.21, 0.18, 0.13, 0.23, 0.26, 0.28, 0.13, 0.11, 0.14, 0.27, 0.13, 0.33, 0.33, 0.2, 0.1, 0.2, 0.12]\n"
     ]
    }
   ],
   "source": [
    "meteor_wss=[]\n",
    "for i in range(len(auth_1)):\n",
    "    meteor_wss.append(round(meteor_score_wss([refs[i]], preds[i]),2))\n",
    "print(meteor_wss)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "4b1938f5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0.39, 0.39, 0.77, 0.77, 0.73, 0.73, 0.73, 0.09, 1.0, 0.48, 0.48, 0.58, 0.53, 0.2, 0.27, 0.13, 0.38, 0.62, 0.62, 0.13, 0.5, 0.37, 0.7, 0.39, 0.3, 0.55, 0.1, 0.1, 0.44, 0.44, 0.35, 0.37, 0.12, 0.35, 0.7, 0.09, 0.54, 0.49, 0.56, 0.34, 0.08, 0.6, 0.78, 0.26, 0.24, 0.41, 0.29, 0.08, 0.51, 0.32, 0.33, 0.77, 0.07, 0.12, 0.17, 0.08, 0.54, 0.19, 0.27, 0.17, 0.21, 0.41, 0.3, 0.34, 0.12, 0.12, 0.06, 0.29, 0.27, 0.09, 0.12, 0.09, 0.12, 0.12, 0.13, 0.05, 0.23, 0.16, 0.16, 0.31, 0.22, 0.15, 0.48, 0.16, 0.14, 0.1, 0.18, 0.2, 0.22, 0.1, 0.09, 0.11, 0.21, 0.1, 0.26, 0.26, 0.16, 0.08, 0.16, 0.09]\n"
     ]
    }
   ],
   "source": [
    "norm_m_wss = []\n",
    "\n",
    "max_m=max(meteor_wss)\n",
    "min_m=min(meteor_wss)\n",
    "\n",
    "for a in range(len(meteor_wss)):\n",
    "    norm_m_wss.append(round(((meteor_wss[a])/(max_m)),2))\n",
    "print(norm_m_wss)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "5f85bbea",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Spearmans correlation coefficient: 0.748\n",
      "correlated (reject H0) p=0.000\n"
     ]
    }
   ],
   "source": [
    "from scipy.stats import spearmanr\n",
    "# calculate spearman's correlation\n",
    "coef, p = spearmanr(norm_m_wss, avg_norm_score)\n",
    "print('Spearmans correlation coefficient: %.3f' % coef)\n",
    "# interpret the significance\n",
    "alpha = 0.05\n",
    "if p > alpha:\n",
    "\tprint('uncorrelated (fail to reject H0) p=%.3f' % p)\n",
    "else:\n",
    "\tprint('correlated (reject H0) p=%.3f' % p)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1115c5ac",
   "metadata": {},
   "source": [
    "# METEOR-NEXT without Semantic Scoring"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "eafe15fd",
   "metadata": {},
   "outputs": [],
   "source": [
    "from nltk.stem.porter import PorterStemmer\n",
    "from nltk.corpus import wordnet\n",
    "from itertools import chain, product\n",
    "\n",
    "\n",
    "def _generate_enums(hypothesis, reference, preprocess=str.lower):\n",
    "    \"\"\"\n",
    "    Takes in string inputs for hypothesis and reference and returns\n",
    "    enumerated word lists for each of them\n",
    "\n",
    "    :param hypothesis: hypothesis string\n",
    "    :type hypothesis: str\n",
    "    :param reference: reference string\n",
    "    :type reference: str\n",
    "    :preprocess: preprocessing method (default str.lower)\n",
    "    :type preprocess: method\n",
    "    :return: enumerated words list\n",
    "    :rtype: list of 2D tuples, list of 2D tuples\n",
    "    \"\"\"\n",
    "    hypothesis_list = list(enumerate(preprocess(hypothesis).split()))\n",
    "    reference_list = list(enumerate(preprocess(reference).split()))\n",
    "    return hypothesis_list, reference_list\n",
    "\n",
    "\n",
    "def exact_match(hypothesis, reference):\n",
    "    \"\"\"\n",
    "    matches exact words in hypothesis and reference\n",
    "    and returns a word mapping based on the enumerated\n",
    "    word id between hypothesis and reference\n",
    "\n",
    "    :param hypothesis: hypothesis string\n",
    "    :type hypothesis: str\n",
    "    :param reference: reference string\n",
    "    :type reference: str\n",
    "    :return: enumerated matched tuples, enumerated unmatched hypothesis tuples,\n",
    "             enumerated unmatched reference tuples\n",
    "    :rtype: list of 2D tuples, list of 2D tuples,  list of 2D tuples\n",
    "    \"\"\"\n",
    "    hypothesis_list, reference_list = _generate_enums(hypothesis, reference)\n",
    "    return _match_enums(hypothesis_list, reference_list)\n",
    "\n",
    "\n",
    "\n",
    "def _match_enums(enum_hypothesis_list, enum_reference_list):\n",
    "    \"\"\"\n",
    "    matches exact words in hypothesis and reference and returns\n",
    "    a word mapping between enum_hypothesis_list and enum_reference_list\n",
    "    based on the enumerated word id.\n",
    "\n",
    "    :param enum_hypothesis_list: enumerated hypothesis list\n",
    "    :type enum_hypothesis_list: list of tuples\n",
    "    :param enum_reference_list: enumerated reference list\n",
    "    :type enum_reference_list: list of 2D tuples\n",
    "    :return: enumerated matched tuples, enumerated unmatched hypothesis tuples,\n",
    "             enumerated unmatched reference tuples\n",
    "    :rtype: list of 2D tuples, list of 2D tuples,  list of 2D tuples\n",
    "    \"\"\"\n",
    "    word_match = []\n",
    "    for i in range(len(enum_hypothesis_list))[::-1]:\n",
    "        for j in range(len(enum_reference_list))[::-1]:\n",
    "            if enum_hypothesis_list[i][1] == enum_reference_list[j][1]:\n",
    "                word_match.append(\n",
    "                    (enum_hypothesis_list[i][0], enum_reference_list[j][0])\n",
    "                )\n",
    "                (enum_hypothesis_list.pop(i)[1], enum_reference_list.pop(j)[1])\n",
    "                break\n",
    "    return word_match, enum_hypothesis_list, enum_reference_list\n",
    "\n",
    "\n",
    "def _enum_stem_match(\n",
    "    enum_hypothesis_list, enum_reference_list, stemmer=PorterStemmer()\n",
    "):\n",
    "    \"\"\"\n",
    "    Stems each word and matches them in hypothesis and reference\n",
    "    and returns a word mapping between enum_hypothesis_list and\n",
    "    enum_reference_list based on the enumerated word id. The function also\n",
    "    returns a enumerated list of unmatched words for hypothesis and reference.\n",
    "\n",
    "    :param enum_hypothesis_list:\n",
    "    :type enum_hypothesis_list:\n",
    "    :param enum_reference_list:\n",
    "    :type enum_reference_list:\n",
    "    :param stemmer: nltk.stem.api.StemmerI object (default PorterStemmer())\n",
    "    :type stemmer: nltk.stem.api.StemmerI or any class that implements a stem method\n",
    "    :return: enumerated matched tuples, enumerated unmatched hypothesis tuples,\n",
    "             enumerated unmatched reference tuples\n",
    "    :rtype: list of 2D tuples, list of 2D tuples,  list of 2D tuples\n",
    "    \"\"\"\n",
    "    stemmed_enum_list1 = [\n",
    "        (word_pair[0], stemmer.stem(word_pair[1])) for word_pair in enum_hypothesis_list\n",
    "    ]\n",
    "\n",
    "    stemmed_enum_list2 = [\n",
    "        (word_pair[0], stemmer.stem(word_pair[1])) for word_pair in enum_reference_list\n",
    "    ]\n",
    "\n",
    "    word_match, enum_unmat_hypo_list, enum_unmat_ref_list = _match_enums(\n",
    "        stemmed_enum_list1, stemmed_enum_list2\n",
    "    )\n",
    "\n",
    "    enum_unmat_hypo_list = (\n",
    "        list(zip(*enum_unmat_hypo_list)) if len(enum_unmat_hypo_list) > 0 else []\n",
    "    )\n",
    "\n",
    "    enum_unmat_ref_list = (\n",
    "        list(zip(*enum_unmat_ref_list)) if len(enum_unmat_ref_list) > 0 else []\n",
    "    )\n",
    "\n",
    "    enum_hypothesis_list = list(\n",
    "        filter(lambda x: x[0] not in enum_unmat_hypo_list, enum_hypothesis_list)\n",
    "    )\n",
    "\n",
    "    enum_reference_list = list(\n",
    "        filter(lambda x: x[0] not in enum_unmat_ref_list, enum_reference_list)\n",
    "    )\n",
    "\n",
    "    return word_match, enum_hypothesis_list, enum_reference_list\n",
    "\n",
    "\n",
    "def stem_match(hypothesis, reference, stemmer=PorterStemmer()):\n",
    "    \"\"\"\n",
    "    Stems each word and matches them in hypothesis and reference\n",
    "    and returns a word mapping between hypothesis and reference\n",
    "\n",
    "    :param hypothesis:\n",
    "    :type hypothesis:\n",
    "    :param reference:\n",
    "    :type reference:\n",
    "    :param stemmer: nltk.stem.api.StemmerI object (default PorterStemmer())\n",
    "    :type stemmer: nltk.stem.api.StemmerI or any class that\n",
    "                   implements a stem method\n",
    "    :return: enumerated matched tuples, enumerated unmatched hypothesis tuples,\n",
    "             enumerated unmatched reference tuples\n",
    "    :rtype: list of 2D tuples, list of 2D tuples,  list of 2D tuples\n",
    "    \"\"\"\n",
    "    enum_hypothesis_list, enum_reference_list = _generate_enums(hypothesis, reference)\n",
    "    return _enum_stem_match(enum_hypothesis_list, enum_reference_list, stemmer=stemmer)\n",
    "\n",
    "\n",
    "\n",
    "def _enum_wordnetsyn_match(enum_hypothesis_list, enum_reference_list, wordnet=wordnet):\n",
    "    \"\"\"\n",
    "    Matches each word in reference to a word in hypothesis\n",
    "    if any synonym of a hypothesis word is the exact match\n",
    "    to the reference word.\n",
    "\n",
    "    :param enum_hypothesis_list: enumerated hypothesis list\n",
    "    :param enum_reference_list: enumerated reference list\n",
    "    :param wordnet: a wordnet corpus reader object (default nltk.corpus.wordnet)\n",
    "    :type wordnet: WordNetCorpusReader\n",
    "    :return: list of matched tuples, unmatched hypothesis list, unmatched reference list\n",
    "    :rtype:  list of tuples, list of tuples, list of tuples\n",
    "\n",
    "    \"\"\"\n",
    "    word_match = []\n",
    "    for i in range(len(enum_hypothesis_list))[::-1]:\n",
    "        hypothesis_syns = set(\n",
    "            chain.from_iterable(\n",
    "                (\n",
    "                    lemma.name()\n",
    "                    for lemma in synset.lemmas()\n",
    "                    if lemma.name().find(\"_\") < 0\n",
    "                )\n",
    "                for synset in wordnet.synsets(enum_hypothesis_list[i][1])\n",
    "            )\n",
    "        ).union({enum_hypothesis_list[i][1]})\n",
    "        for j in range(len(enum_reference_list))[::-1]:\n",
    "            if enum_reference_list[j][1] in hypothesis_syns:\n",
    "                word_match.append(\n",
    "                    (enum_hypothesis_list[i][0], enum_reference_list[j][0])\n",
    "                )\n",
    "                enum_hypothesis_list.pop(i), enum_reference_list.pop(j)\n",
    "                break\n",
    "    return word_match, enum_hypothesis_list, enum_reference_list\n",
    "\n",
    "\n",
    "def wordnetsyn_match(hypothesis, reference, wordnet=wordnet):\n",
    "    \"\"\"\n",
    "    Matches each word in reference to a word in hypothesis if any synonym\n",
    "    of a hypothesis word is the exact match to the reference word.\n",
    "\n",
    "    :param hypothesis: hypothesis string\n",
    "    :param reference: reference string\n",
    "    :param wordnet: a wordnet corpus reader object (default nltk.corpus.wordnet)\n",
    "    :type wordnet: WordNetCorpusReader\n",
    "    :return: list of mapped tuples\n",
    "    :rtype: list of tuples\n",
    "    \"\"\"\n",
    "    enum_hypothesis_list, enum_reference_list = _generate_enums(hypothesis, reference)\n",
    "    return _enum_wordnetsyn_match(\n",
    "        enum_hypothesis_list, enum_reference_list, wordnet=wordnet\n",
    "    )\n",
    "\n",
    "\n",
    "\n",
    "def _enum_allign_words(\n",
    "    enum_hypothesis_list, enum_reference_list, stemmer=PorterStemmer(), wordnet=wordnet\n",
    "):\n",
    "    \"\"\"\n",
    "    Aligns/matches words in the hypothesis to reference by sequentially\n",
    "    applying exact match, stemmed match and wordnet based synonym match.\n",
    "    in case there are multiple matches the match which has the least number\n",
    "    of crossing is chosen. Takes enumerated list as input instead of\n",
    "    string input\n",
    "\n",
    "    :param enum_hypothesis_list: enumerated hypothesis list\n",
    "    :param enum_reference_list: enumerated reference list\n",
    "    :param stemmer: nltk.stem.api.StemmerI object (default PorterStemmer())\n",
    "    :type stemmer: nltk.stem.api.StemmerI or any class that implements a stem method\n",
    "    :param wordnet: a wordnet corpus reader object (default nltk.corpus.wordnet)\n",
    "    :type wordnet: WordNetCorpusReader\n",
    "    :return: sorted list of matched tuples, unmatched hypothesis list,\n",
    "             unmatched reference list\n",
    "    :rtype: list of tuples, list of tuples, list of tuples\n",
    "    \"\"\"\n",
    "    exact_matches, enum_hypothesis_list, enum_reference_list = _match_enums(\n",
    "        enum_hypothesis_list, enum_reference_list\n",
    "    )\n",
    "\n",
    "    stem_matches, enum_hypothesis_list, enum_reference_list = _enum_stem_match(\n",
    "        enum_hypothesis_list, enum_reference_list, stemmer=stemmer\n",
    "    )\n",
    "\n",
    "    wns_matches, enum_hypothesis_list, enum_reference_list = _enum_wordnetsyn_match(\n",
    "        enum_hypothesis_list, enum_reference_list, wordnet=wordnet\n",
    "    )\n",
    "\n",
    "    return (\n",
    "        sorted(\n",
    "            exact_matches, key=lambda wordpair: wordpair[0]\n",
    "        ),\n",
    "        enum_hypothesis_list,\n",
    "        enum_reference_list,\n",
    "    )\n",
    "\n",
    "\n",
    "def allign_words(hypothesis, reference, stemmer=PorterStemmer(), wordnet=wordnet):\n",
    "    \"\"\"\n",
    "    Aligns/matches words in the hypothesis to reference by sequentially\n",
    "    applying exact match, stemmed match and wordnet based synonym match.\n",
    "    In case there are multiple matches the match which has the least number\n",
    "    of crossing is chosen.\n",
    "\n",
    "    :param hypothesis: hypothesis string\n",
    "    :param reference: reference string\n",
    "    :param stemmer: nltk.stem.api.StemmerI object (default PorterStemmer())\n",
    "    :type stemmer: nltk.stem.api.StemmerI or any class that implements a stem method\n",
    "    :param wordnet: a wordnet corpus reader object (default nltk.corpus.wordnet)\n",
    "    :type wordnet: WordNetCorpusReader\n",
    "    :return: sorted list of matched tuples, unmatched hypothesis list, unmatched reference list\n",
    "    :rtype: list of tuples, list of tuples, list of tuples\n",
    "    \"\"\"\n",
    "    enum_hypothesis_list, enum_reference_list = _generate_enums(hypothesis, reference)\n",
    "    return _enum_allign_words(\n",
    "        enum_hypothesis_list, enum_reference_list, stemmer=stemmer, wordnet=wordnet\n",
    "    )\n",
    "\n",
    "\n",
    "\n",
    "def _count_chunks(matches):\n",
    "    \"\"\"\n",
    "    Counts the fewest possible number of chunks such that matched unigrams\n",
    "    of each chunk are adjacent to each other. This is used to caluclate the\n",
    "    fragmentation part of the metric.\n",
    "\n",
    "    :param matches: list containing a mapping of matched words (output of allign_words)\n",
    "    :return: Number of chunks a sentence is divided into post allignment\n",
    "    :rtype: int\n",
    "    \"\"\"\n",
    "    i = 0\n",
    "    chunks = 1\n",
    "    while i < len(matches) - 1:\n",
    "        if (matches[i + 1][0] == matches[i][0] + 1) and (\n",
    "            matches[i + 1][1] == matches[i][1] + 1\n",
    "        ):\n",
    "            i += 1\n",
    "            continue\n",
    "        i += 1\n",
    "        chunks += 1\n",
    "    return chunks\n",
    "\n",
    "\n",
    "def single_meteor_score(\n",
    "    reference,\n",
    "    hypothesis,\n",
    "    preprocess=str.lower,\n",
    "    stemmer=PorterStemmer(),\n",
    "    wordnet=wordnet,\n",
    "    alpha=0.85,\n",
    "    beta=2.35,\n",
    "    gamma=0.45,\n",
    "    w_1=1,\n",
    "    w_2=0.8,\n",
    "    w_3=0.6\n",
    "):\n",
    "    \"\"\"\n",
    "    Calculates METEOR score for single hypothesis and reference as per\n",
    "    \"Meteor: An Automatic Metric for MT Evaluation with HighLevels of\n",
    "    Correlation with Human Judgments\" by Alon Lavie and Abhaya Agarwal,\n",
    "    in Proceedings of ACL.\n",
    "    http://www.cs.cmu.edu/~alavie/METEOR/pdf/Lavie-Agarwal-2007-METEOR.pdf\n",
    "\n",
    "\n",
    "    >>> hypothesis1 = 'It is a guide to action which ensures that the military always obeys the commands of the party'\n",
    "\n",
    "    >>> reference1 = 'It is a guide to action that ensures that the military will forever heed Party commands'\n",
    "\n",
    "\n",
    "    >>> round(single_meteor_score(reference1, hypothesis1),4)\n",
    "    0.7398\n",
    "\n",
    "        If there is no words match during the alignment the method returns the\n",
    "        score as 0. We can safely  return a zero instead of raising a\n",
    "        division by zero error as no match usually implies a bad translation.\n",
    "\n",
    "    >>> round(meteor_score('this is a cat', 'non matching hypothesis'),4)\n",
    "    0.0\n",
    "\n",
    "    :param references: reference sentences\n",
    "    :type references: list(str)\n",
    "    :param hypothesis: a hypothesis sentence\n",
    "    :type hypothesis: str\n",
    "    :param preprocess: preprocessing function (default str.lower)\n",
    "    :type preprocess: method\n",
    "    :param stemmer: nltk.stem.api.StemmerI object (default PorterStemmer())\n",
    "    :type stemmer: nltk.stem.api.StemmerI or any class that implements a stem method\n",
    "    :param wordnet: a wordnet corpus reader object (default nltk.corpus.wordnet)\n",
    "    :type wordnet: WordNetCorpusReader\n",
    "    :param alpha: parameter for controlling relative weights of precision and recall.\n",
    "    :type alpha: float\n",
    "    :param beta: parameter for controlling shape of penalty as a\n",
    "                 function of as a function of fragmentation.\n",
    "    :type beta: float\n",
    "    :param gamma: relative weight assigned to fragmentation penality.\n",
    "    :type gamma: float\n",
    "    :return: The sentence-level METEOR score.\n",
    "    :rtype: float\n",
    "    \"\"\"\n",
    "    enum_hypothesis, enum_reference = _generate_enums(\n",
    "        hypothesis, reference, preprocess=preprocess\n",
    "    )\n",
    "    translation_length = len(enum_hypothesis)\n",
    "    reference_length = len(enum_reference)\n",
    "    matches, _, _ = _enum_allign_words(enum_hypothesis, enum_reference, stemmer=stemmer)\n",
    "    print(matches)\n",
    "    exact_m,_,_ = exact_match(hypothesis, reference)\n",
    "    print(exact_m)\n",
    "    stem_m,_,_ = _enum_stem_match(enum_hypothesis, enum_reference, stemmer=PorterStemmer())\n",
    "    print(stem_m)\n",
    "    syn_m,_,_ = _enum_wordnetsyn_match(enum_hypothesis, enum_reference, wordnet=wordnet)\n",
    "    print(syn_m)\n",
    "    \n",
    "    \n",
    "    \n",
    "    exact_count = len(exact_m)\n",
    "    stem_count = len((set(stem_m).difference(exact_m)))\n",
    "    syn_count = len((set(syn_m).difference(stem_m)))\n",
    "    print(exact_count)\n",
    "    matches_count = len(matches)\n",
    "    print(matches_count)\n",
    "    try:\n",
    "        precision = float(w_1*exact_count) / translation_length\n",
    "        recall = float(w_1*exact_count) / reference_length\n",
    "        #precision = float(matches_count) / translation_length\n",
    "        #recall = float(matches_count) / reference_length\n",
    "        fmean = (precision * recall) / (alpha * precision + (1 - alpha) * recall)\n",
    "        chunk_count = float(_count_chunks(matches))\n",
    "        frag_frac = chunk_count / matches_count\n",
    "    except ZeroDivisionError:\n",
    "        return 0.0\n",
    "    penalty = gamma * frag_frac ** beta\n",
    "    return (1 - penalty) * fmean\n",
    "\n",
    "\n",
    "\n",
    "def meteorn_score_woss(\n",
    "    references,\n",
    "    hypothesis,\n",
    "    preprocess=str.lower,\n",
    "    stemmer=PorterStemmer(),\n",
    "    wordnet=wordnet,\n",
    "    alpha=0.85,\n",
    "    beta=2.35,\n",
    "    gamma=0.45,\n",
    "    w_1=1,\n",
    "    w_2=0.8,\n",
    "    w_3=0.6\n",
    "):\n",
    "    \"\"\"\n",
    "    Calculates METEOR score for hypothesis with multiple references as\n",
    "    described in \"Meteor: An Automatic Metric for MT Evaluation with\n",
    "    HighLevels of Correlation with Human Judgments\" by Alon Lavie and\n",
    "    Abhaya Agarwal, in Proceedings of ACL.\n",
    "    http://www.cs.cmu.edu/~alavie/METEOR/pdf/Lavie-Agarwal-2007-METEOR.pdf\n",
    "\n",
    "\n",
    "    In case of multiple references the best score is chosen. This method\n",
    "    iterates over single_meteor_score and picks the best pair among all\n",
    "    the references for a given hypothesis\n",
    "\n",
    "    >>> hypothesis1 = 'It is a guide to action which ensures that the military always obeys the commands of the party'\n",
    "    >>> hypothesis2 = 'It is to insure the troops forever hearing the activity guidebook that party direct'\n",
    "\n",
    "    >>> reference1 = 'It is a guide to action that ensures that the military will forever heed Party commands'\n",
    "    >>> reference2 = 'It is the guiding principle which guarantees the military forces always being under the command of the Party'\n",
    "    >>> reference3 = 'It is the practical guide for the army always to heed the directions of the party'\n",
    "\n",
    "    >>> round(meteor_score([reference1, reference2, reference3], hypothesis1),4)\n",
    "    0.7398\n",
    "\n",
    "        If there is no words match during the alignment the method returns the\n",
    "        score as 0. We can safely  return a zero instead of raising a\n",
    "        division by zero error as no match usually implies a bad translation.\n",
    "\n",
    "    >>> round(meteor_score(['this is a cat'], 'non matching hypothesis'),4)\n",
    "    0.0\n",
    "\n",
    "    :param references: reference sentences\n",
    "    :type references: list(str)\n",
    "    :param hypothesis: a hypothesis sentence\n",
    "    :type hypothesis: str\n",
    "    :param preprocess: preprocessing function (default str.lower)\n",
    "    :type preprocess: method\n",
    "    :param stemmer: nltk.stem.api.StemmerI object (default PorterStemmer())\n",
    "    :type stemmer: nltk.stem.api.StemmerI or any class that implements a stem method\n",
    "    :param wordnet: a wordnet corpus reader object (default nltk.corpus.wordnet)\n",
    "    :type wordnet: WordNetCorpusReader\n",
    "    :param alpha: parameter for controlling relative weights of precision and recall.\n",
    "    :type alpha: float\n",
    "    :param beta: parameter for controlling shape of penalty as a function\n",
    "                 of as a function of fragmentation.\n",
    "    :type beta: float\n",
    "    :param gamma: relative weight assigned to fragmentation penality.\n",
    "    :type gamma: float\n",
    "    :return: The sentence-level METEOR score.\n",
    "    :rtype: float\n",
    "    \"\"\"\n",
    "    return max(\n",
    "        [\n",
    "            single_meteor_score(\n",
    "                reference,\n",
    "                hypothesis,\n",
    "                preprocess=preprocess,\n",
    "                stemmer=stemmer,\n",
    "                wordnet=wordnet,\n",
    "                alpha=alpha,\n",
    "                beta=beta,\n",
    "                gamma=gamma,\n",
    "            )\n",
    "            for reference in references\n",
    "        ]\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "d3bf85d3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[(0, 0)]\n",
      "[(0, 0)]\n",
      "[]\n",
      "[]\n",
      "1\n",
      "1\n",
      "[(0, 0)]\n",
      "[(0, 0)]\n",
      "[]\n",
      "[]\n",
      "1\n",
      "1\n",
      "[(0, 0), (1, 1), (2, 2)]\n",
      "[(2, 2), (1, 1), (0, 0)]\n",
      "[]\n",
      "[]\n",
      "3\n",
      "3\n",
      "[(0, 0), (1, 1), (2, 2)]\n",
      "[(2, 2), (1, 1), (0, 0)]\n",
      "[]\n",
      "[]\n",
      "3\n",
      "3\n",
      "[(0, 0), (1, 1)]\n",
      "[(1, 1), (0, 0)]\n",
      "[]\n",
      "[]\n",
      "2\n",
      "2\n",
      "[(0, 0), (1, 1)]\n",
      "[(1, 1), (0, 0)]\n",
      "[]\n",
      "[]\n",
      "2\n",
      "2\n",
      "[(0, 0), (1, 1)]\n",
      "[(1, 1), (0, 0)]\n",
      "[]\n",
      "[]\n",
      "2\n",
      "2\n",
      "[(1, 2)]\n",
      "[(1, 2)]\n",
      "[]\n",
      "[]\n",
      "1\n",
      "1\n",
      "[(1, 1)]\n",
      "[(1, 1)]\n",
      "[(0, 0)]\n",
      "[(0, 0)]\n",
      "1\n",
      "1\n",
      "[(0, 0), (1, 1)]\n",
      "[(1, 1), (0, 0)]\n",
      "[]\n",
      "[]\n",
      "2\n",
      "2\n",
      "[(0, 0), (1, 1)]\n",
      "[(1, 1), (0, 0)]\n",
      "[]\n",
      "[]\n",
      "2\n",
      "2\n",
      "[(0, 0), (1, 1), (2, 2)]\n",
      "[(2, 2), (1, 1), (0, 0)]\n",
      "[]\n",
      "[]\n",
      "3\n",
      "3\n",
      "[(0, 0), (1, 1), (2, 2), (3, 3)]\n",
      "[(3, 3), (2, 2), (1, 1), (0, 0)]\n",
      "[]\n",
      "[]\n",
      "4\n",
      "4\n",
      "[(0, 0)]\n",
      "[(0, 0)]\n",
      "[]\n",
      "[]\n",
      "1\n",
      "1\n",
      "[(0, 0), (1, 2)]\n",
      "[(1, 2), (0, 0)]\n",
      "[]\n",
      "[]\n",
      "2\n",
      "2\n",
      "[(0, 0)]\n",
      "[(0, 0)]\n",
      "[]\n",
      "[]\n",
      "1\n",
      "1\n",
      "[(0, 0), (1, 1)]\n",
      "[(1, 1), (0, 0)]\n",
      "[]\n",
      "[]\n",
      "2\n",
      "2\n",
      "[(0, 0), (1, 1), (3, 2)]\n",
      "[(3, 2), (1, 1), (0, 0)]\n",
      "[]\n",
      "[]\n",
      "3\n",
      "3\n",
      "[(0, 0), (1, 1), (3, 2)]\n",
      "[(3, 2), (1, 1), (0, 0)]\n",
      "[]\n",
      "[]\n",
      "3\n",
      "3\n",
      "[(1, 2)]\n",
      "[(1, 2)]\n",
      "[]\n",
      "[]\n",
      "1\n",
      "1\n",
      "[(0, 0), (2, 2), (3, 3)]\n",
      "[(3, 3), (2, 2), (0, 0)]\n",
      "[]\n",
      "[]\n",
      "3\n",
      "3\n",
      "[(0, 0), (1, 1)]\n",
      "[(1, 1), (0, 0)]\n",
      "[]\n",
      "[]\n",
      "2\n",
      "2\n",
      "[(1, 0), (3, 1), (4, 2), (5, 3)]\n",
      "[(5, 3), (4, 2), (3, 1), (1, 0)]\n",
      "[]\n",
      "[]\n",
      "4\n",
      "4\n",
      "[(0, 0), (1, 1), (2, 2)]\n",
      "[(2, 2), (1, 1), (0, 0)]\n",
      "[]\n",
      "[]\n",
      "3\n",
      "3\n",
      "[(0, 0), (1, 3), (3, 2)]\n",
      "[(3, 2), (1, 3), (0, 0)]\n",
      "[]\n",
      "[]\n",
      "3\n",
      "3\n",
      "[(0, 0), (1, 1), (2, 2)]\n",
      "[(2, 2), (1, 1), (0, 0)]\n",
      "[]\n",
      "[]\n",
      "3\n",
      "3\n",
      "[(1, 3)]\n",
      "[(1, 3)]\n",
      "[]\n",
      "[]\n",
      "1\n",
      "1\n",
      "[(0, 0)]\n",
      "[(0, 0)]\n",
      "[]\n",
      "[]\n",
      "1\n",
      "1\n",
      "[(0, 0), (1, 1), (2, 2), (3, 5)]\n",
      "[(3, 5), (2, 2), (1, 1), (0, 0)]\n",
      "[]\n",
      "[]\n",
      "4\n",
      "4\n",
      "[(0, 0), (1, 1), (2, 2), (5, 4), (6, 7)]\n",
      "[(6, 7), (5, 4), (2, 2), (1, 1), (0, 0)]\n",
      "[]\n",
      "[]\n",
      "5\n",
      "5\n",
      "[(0, 2), (1, 4), (2, 5)]\n",
      "[(2, 5), (1, 4), (0, 2)]\n",
      "[]\n",
      "[]\n",
      "3\n",
      "3\n",
      "[(0, 2), (1, 3), (2, 4), (3, 5)]\n",
      "[(3, 5), (2, 4), (1, 3), (0, 2)]\n",
      "[]\n",
      "[]\n",
      "4\n",
      "4\n",
      "[(0, 1)]\n",
      "[(0, 1)]\n",
      "[]\n",
      "[]\n",
      "1\n",
      "1\n",
      "[(0, 0), (1, 1), (2, 5)]\n",
      "[(2, 5), (1, 1), (0, 0)]\n",
      "[]\n",
      "[]\n",
      "3\n",
      "3\n",
      "[(0, 0), (1, 1), (4, 2), (5, 3)]\n",
      "[(5, 3), (4, 2), (1, 1), (0, 0)]\n",
      "[]\n",
      "[]\n",
      "4\n",
      "4\n",
      "[(8, 1)]\n",
      "[(8, 1)]\n",
      "[]\n",
      "[]\n",
      "1\n",
      "1\n",
      "[(0, 0), (2, 2), (4, 4), (5, 5), (6, 6), (7, 8), (8, 1)]\n",
      "[(8, 1), (7, 8), (6, 6), (5, 5), (4, 4), (2, 2), (0, 0)]\n",
      "[(3, 3)]\n",
      "[]\n",
      "7\n",
      "7\n",
      "[(0, 0), (1, 1)]\n",
      "[(1, 1), (0, 0)]\n",
      "[(2, 4)]\n",
      "[(2, 4)]\n",
      "2\n",
      "2\n",
      "[(0, 0), (2, 2), (4, 4), (7, 1)]\n",
      "[(7, 1), (4, 4), (2, 2), (0, 0)]\n",
      "[(3, 3)]\n",
      "[]\n",
      "4\n",
      "4\n",
      "[(0, 0), (3, 4), (4, 5)]\n",
      "[(4, 5), (3, 4), (0, 0)]\n",
      "[]\n",
      "[]\n",
      "3\n",
      "3\n",
      "[(1, 4)]\n",
      "[(1, 4)]\n",
      "[]\n",
      "[]\n",
      "1\n",
      "1\n",
      "[(0, 0), (1, 1), (4, 2)]\n",
      "[(4, 2), (1, 1), (0, 0)]\n",
      "[]\n",
      "[]\n",
      "3\n",
      "3\n",
      "[(0, 0), (1, 1), (2, 2), (3, 3), (4, 4), (5, 5), (6, 6), (7, 7), (8, 8), (9, 9)]\n",
      "[(9, 9), (8, 8), (7, 7), (6, 6), (5, 5), (4, 4), (3, 3), (2, 2), (1, 1), (0, 0)]\n",
      "[]\n",
      "[]\n",
      "10\n",
      "10\n",
      "[(0, 0), (1, 1)]\n",
      "[(1, 1), (0, 0)]\n",
      "[]\n",
      "[]\n",
      "2\n",
      "2\n",
      "[(1, 0), (4, 2)]\n",
      "[(4, 2), (1, 0)]\n",
      "[]\n",
      "[]\n",
      "2\n",
      "2\n",
      "[(0, 0), (1, 1), (2, 2), (3, 5), (4, 9), (5, 3), (8, 10)]\n",
      "[(8, 10), (5, 3), (4, 9), (3, 5), (2, 2), (1, 1), (0, 0)]\n",
      "[]\n",
      "[]\n",
      "7\n",
      "7\n",
      "[(0, 0), (1, 1)]\n",
      "[(1, 1), (0, 0)]\n",
      "[]\n",
      "[]\n",
      "2\n",
      "2\n",
      "[(3, 4)]\n",
      "[(3, 4)]\n",
      "[]\n",
      "[]\n",
      "1\n",
      "1\n",
      "[(6, 1), (7, 2), (8, 3)]\n",
      "[(8, 3), (7, 2), (6, 1)]\n",
      "[]\n",
      "[]\n",
      "3\n",
      "3\n",
      "[(3, 2), (7, 6), (8, 7), (9, 8)]\n",
      "[(9, 8), (8, 7), (7, 6), (3, 2)]\n",
      "[]\n",
      "[]\n",
      "4\n",
      "4\n",
      "[(0, 0), (1, 1), (6, 5)]\n",
      "[(6, 5), (1, 1), (0, 0)]\n",
      "[]\n",
      "[]\n",
      "3\n",
      "3\n",
      "[(0, 0), (1, 1), (2, 2), (4, 3), (5, 4), (6, 5), (7, 6), (8, 7), (9, 8)]\n",
      "[(9, 8), (8, 7), (7, 6), (6, 5), (5, 4), (4, 3), (2, 2), (1, 1), (0, 0)]\n",
      "[]\n",
      "[]\n",
      "9\n",
      "9\n",
      "[(1, 1)]\n",
      "[(1, 1)]\n",
      "[]\n",
      "[]\n",
      "1\n",
      "1\n",
      "[(0, 0)]\n",
      "[(0, 0)]\n",
      "[]\n",
      "[]\n",
      "1\n",
      "1\n",
      "[(4, 1)]\n",
      "[(4, 1)]\n",
      "[]\n",
      "[]\n",
      "1\n",
      "1\n",
      "[(4, 4)]\n",
      "[(4, 4)]\n",
      "[]\n",
      "[]\n",
      "1\n",
      "1\n",
      "[(11, 0), (13, 2), (14, 1), (15, 4), (16, 5), (17, 6), (19, 3)]\n",
      "[(19, 3), (17, 6), (16, 5), (15, 4), (14, 1), (13, 2), (11, 0)]\n",
      "[]\n",
      "[]\n",
      "7\n",
      "7\n",
      "[(0, 0), (4, 3)]\n",
      "[(4, 3), (0, 0)]\n",
      "[]\n",
      "[]\n",
      "2\n",
      "2\n",
      "[(2, 3), (3, 4), (4, 5)]\n",
      "[(4, 5), (3, 4), (2, 3)]\n",
      "[]\n",
      "[]\n",
      "3\n",
      "3\n",
      "[(1, 5), (4, 3), (5, 6)]\n",
      "[(5, 6), (4, 3), (1, 5)]\n",
      "[]\n",
      "[]\n",
      "3\n",
      "3\n",
      "[(0, 0), (2, 2), (8, 1)]\n",
      "[(8, 1), (2, 2), (0, 0)]\n",
      "[(1, 6)]\n",
      "[(1, 6)]\n",
      "3\n",
      "3\n",
      "[(5, 2), (8, 1), (10, 5), (11, 6), (12, 7)]\n",
      "[(12, 7), (11, 6), (10, 5), (8, 1), (5, 2)]\n",
      "[]\n",
      "[]\n",
      "5\n",
      "5\n",
      "[(0, 0), (1, 1), (2, 2), (9, 3)]\n",
      "[(9, 3), (2, 2), (1, 1), (0, 0)]\n",
      "[(3, 11)]\n",
      "[]\n",
      "4\n",
      "4\n",
      "[(0, 0), (1, 1), (2, 2), (3, 5), (4, 10), (6, 8), (7, 9), (8, 16)]\n",
      "[(8, 16), (7, 9), (6, 8), (4, 10), (3, 5), (2, 2), (1, 1), (0, 0)]\n",
      "[]\n",
      "[]\n",
      "8\n",
      "8\n",
      "[(2, 1), (4, 3)]\n",
      "[(4, 3), (2, 1)]\n",
      "[]\n",
      "[]\n",
      "2\n",
      "2\n",
      "[(0, 0), (3, 3)]\n",
      "[(3, 3), (0, 0)]\n",
      "[]\n",
      "[]\n",
      "2\n",
      "2\n",
      "[(0, 0)]\n",
      "[(0, 0)]\n",
      "[]\n",
      "[]\n",
      "1\n",
      "1\n",
      "[(0, 0), (4, 4), (5, 5), (7, 7), (9, 10), (10, 13), (12, 1), (15, 2)]\n",
      "[(15, 2), (12, 1), (10, 13), (9, 10), (7, 7), (5, 5), (4, 4), (0, 0)]\n",
      "[]\n",
      "[]\n",
      "8\n",
      "8\n",
      "[(0, 0), (1, 1), (2, 2), (4, 3), (9, 4), (10, 5)]\n",
      "[(10, 5), (9, 4), (4, 3), (2, 2), (1, 1), (0, 0)]\n",
      "[]\n",
      "[]\n",
      "6\n",
      "6\n",
      "[(2, 0)]\n",
      "[(2, 0)]\n",
      "[]\n",
      "[]\n",
      "1\n",
      "1\n",
      "[(0, 0)]\n",
      "[(0, 0)]\n",
      "[]\n",
      "[]\n",
      "1\n",
      "1\n",
      "[(4, 1)]\n",
      "[(4, 1)]\n",
      "[]\n",
      "[]\n",
      "1\n",
      "1\n",
      "[(4, 2)]\n",
      "[(4, 2)]\n",
      "[]\n",
      "[]\n",
      "1\n",
      "1\n",
      "[(0, 0)]\n",
      "[(0, 0)]\n",
      "[]\n",
      "[]\n",
      "1\n",
      "1\n",
      "[(0, 0)]\n",
      "[(0, 0)]\n",
      "[(3, 2)]\n",
      "[]\n",
      "1\n",
      "1\n",
      "[(1, 0)]\n",
      "[(1, 0)]\n",
      "[]\n",
      "[]\n",
      "1\n",
      "1\n",
      "[(1, 0), (6, 2)]\n",
      "[(6, 2), (1, 0)]\n",
      "[]\n",
      "[]\n",
      "2\n",
      "2\n",
      "[(0, 0), (2, 2), (6, 7), (7, 9), (8, 1)]\n",
      "[(8, 1), (7, 9), (6, 7), (2, 2), (0, 0)]\n",
      "[]\n",
      "[]\n",
      "5\n",
      "5\n",
      "[(4, 2), (6, 3)]\n",
      "[(6, 3), (4, 2)]\n",
      "[]\n",
      "[(0, 0)]\n",
      "2\n",
      "2\n",
      "[(0, 0), (2, 1)]\n",
      "[(2, 1), (0, 0)]\n",
      "[]\n",
      "[]\n",
      "2\n",
      "2\n",
      "[(8, 3), (9, 4), (11, 1), (13, 2)]\n",
      "[(13, 2), (11, 1), (9, 4), (8, 3)]\n",
      "[(2, 0)]\n",
      "[]\n",
      "4\n",
      "4\n",
      "[(0, 0), (2, 3), (5, 7)]\n",
      "[(5, 7), (2, 3), (0, 0)]\n",
      "[]\n",
      "[]\n",
      "3\n",
      "3\n",
      "[(0, 0), (4, 5), (6, 6), (7, 7), (9, 2), (10, 3)]\n",
      "[(10, 3), (9, 2), (7, 7), (6, 6), (4, 5), (0, 0)]\n",
      "[]\n",
      "[]\n",
      "6\n",
      "6\n",
      "[(0, 0), (3, 2), (5, 5)]\n",
      "[(5, 5), (3, 2), (0, 0)]\n",
      "[]\n",
      "[]\n",
      "3\n",
      "3\n",
      "[(3, 0), (4, 10), (5, 2)]\n",
      "[(5, 2), (4, 10), (3, 0)]\n",
      "[(11, 1)]\n",
      "[]\n",
      "3\n",
      "3\n",
      "[(3, 0), (4, 8), (5, 2)]\n",
      "[(5, 2), (4, 8), (3, 0)]\n",
      "[]\n",
      "[]\n",
      "3\n",
      "3\n",
      "[(0, 0), (1, 1), (2, 2)]\n",
      "[(2, 2), (1, 1), (0, 0)]\n",
      "[]\n",
      "[]\n",
      "3\n",
      "3\n",
      "[(0, 0), (1, 1)]\n",
      "[(1, 1), (0, 0)]\n",
      "[]\n",
      "[]\n",
      "2\n",
      "2\n",
      "[(5, 11), (8, 1), (10, 13), (11, 14), (12, 15)]\n",
      "[(12, 15), (11, 14), (10, 13), (8, 1), (5, 11)]\n",
      "[]\n",
      "[]\n",
      "5\n",
      "5\n",
      "[(1, 0), (4, 7)]\n",
      "[(4, 7), (1, 0)]\n",
      "[]\n",
      "[]\n",
      "2\n",
      "2\n",
      "[(3, 5), (7, 8)]\n",
      "[(7, 8), (3, 5)]\n",
      "[]\n",
      "[]\n",
      "2\n",
      "2\n",
      "[(6, 4), (7, 6)]\n",
      "[(7, 6), (6, 4)]\n",
      "[]\n",
      "[]\n",
      "2\n",
      "2\n",
      "[(0, 0), (2, 2), (3, 3)]\n",
      "[(3, 3), (2, 2), (0, 0)]\n",
      "[]\n",
      "[]\n",
      "3\n",
      "3\n",
      "[(1, 1), (3, 0)]\n",
      "[(3, 0), (1, 1)]\n",
      "[]\n",
      "[]\n",
      "2\n",
      "2\n",
      "[(0, 0), (2, 2), (8, 3)]\n",
      "[(8, 3), (2, 2), (0, 0)]\n",
      "[]\n",
      "[]\n",
      "3\n",
      "3\n",
      "[(0, 0), (1, 2), (8, 3)]\n",
      "[(8, 3), (1, 2), (0, 0)]\n",
      "[]\n",
      "[]\n",
      "3\n",
      "3\n",
      "[(0, 0)]\n",
      "[(0, 0)]\n",
      "[]\n",
      "[]\n",
      "1\n",
      "1\n",
      "[(6, 4)]\n",
      "[(6, 4)]\n",
      "[]\n",
      "[]\n",
      "1\n",
      "1\n",
      "[(6, 1)]\n",
      "[(6, 1)]\n",
      "[]\n",
      "[]\n",
      "1\n",
      "1\n",
      "[(1, 1)]\n",
      "[(1, 1)]\n",
      "[]\n",
      "[]\n",
      "1\n",
      "1\n",
      "[0.55, 0.55, 0.97, 0.97, 0.91, 0.91, 0.91, 0.12, 0.28, 0.61, 0.61, 0.72, 0.69, 0.3, 0.39, 0.18, 0.49, 0.75, 0.75, 0.18, 0.62, 0.46, 0.85, 0.5, 0.41, 0.67, 0.14, 0.15, 0.56, 0.55, 0.45, 0.48, 0.17, 0.45, 0.85, 0.12, 0.52, 0.36, 0.35, 0.42, 0.12, 0.72, 1.0, 0.34, 0.33, 0.52, 0.35, 0.11, 0.61, 0.4, 0.4, 0.97, 0.1, 0.17, 0.22, 0.11, 0.62, 0.26, 0.34, 0.24, 0.18, 0.49, 0.31, 0.43, 0.16, 0.16, 0.09, 0.37, 0.34, 0.13, 0.16, 0.13, 0.16, 0.16, 0.09, 0.08, 0.31, 0.22, 0.16, 0.4, 0.23, 0.21, 0.58, 0.23, 0.14, 0.14, 0.23, 0.25, 0.28, 0.14, 0.12, 0.15, 0.27, 0.15, 0.35, 0.35, 0.2, 0.1, 0.2, 0.12]\n"
     ]
    }
   ],
   "source": [
    "meteorn_woss=[]\n",
    "for i in range(len(auth_1)):\n",
    "    meteorn_woss.append(round(meteorn_score_woss([refs[i]],preds[i]),2))\n",
    "print(meteorn_woss)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "c794808d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0.55, 0.55, 0.97, 0.97, 0.91, 0.91, 0.91, 0.12, 0.28, 0.61, 0.61, 0.72, 0.69, 0.3, 0.39, 0.18, 0.49, 0.75, 0.75, 0.18, 0.62, 0.46, 0.85, 0.5, 0.41, 0.67, 0.14, 0.15, 0.56, 0.55, 0.45, 0.48, 0.17, 0.45, 0.85, 0.12, 0.52, 0.36, 0.35, 0.42, 0.12, 0.72, 1.0, 0.34, 0.33, 0.52, 0.35, 0.11, 0.61, 0.4, 0.4, 0.97, 0.1, 0.17, 0.22, 0.11, 0.62, 0.26, 0.34, 0.24, 0.18, 0.49, 0.31, 0.43, 0.16, 0.16, 0.09, 0.37, 0.34, 0.13, 0.16, 0.13, 0.16, 0.16, 0.09, 0.08, 0.31, 0.22, 0.16, 0.4, 0.23, 0.21, 0.58, 0.23, 0.14, 0.14, 0.23, 0.25, 0.28, 0.14, 0.12, 0.15, 0.27, 0.15, 0.35, 0.35, 0.2, 0.1, 0.2, 0.12]\n"
     ]
    }
   ],
   "source": [
    "norm_mn_woss = []\n",
    "\n",
    "max_mn=max(meteorn_woss)\n",
    "min_mn=min(meteorn_woss)\n",
    "\n",
    "for a in range(len(meteorn_woss)):\n",
    "    norm_mn_woss.append(round(((meteorn_woss[a])/(max_mn)),2))\n",
    "print(norm_mn_woss)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "21769b24",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Spearmans correlation coefficient: 0.722\n",
      "correlated (reject H0) p=0.000\n"
     ]
    }
   ],
   "source": [
    "from scipy.stats import spearmanr\n",
    "# calculate spearman's correlation\n",
    "coef, p = spearmanr(norm_mn_woss, avg_norm_score)\n",
    "print('Spearmans correlation coefficient: %.3f' % coef)\n",
    "# interpret the significance\n",
    "alpha = 0.05\n",
    "if p > alpha:\n",
    "\tprint('uncorrelated (fail to reject H0) p=%.3f' % p)\n",
    "else:\n",
    "\tprint('correlated (reject H0) p=%.3f' % p)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bb16bb37",
   "metadata": {},
   "source": [
    "# METEOR-NEXT with semantic scoring"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "d654ac21",
   "metadata": {},
   "outputs": [],
   "source": [
    "from nltk.stem.porter import PorterStemmer\n",
    "from nltk.corpus import wordnet\n",
    "from itertools import chain, product\n",
    "\n",
    "\n",
    "def _generate_enums(hypothesis, reference, preprocess=str.lower):\n",
    "    \"\"\"\n",
    "    Takes in string inputs for hypothesis and reference and returns\n",
    "    enumerated word lists for each of them\n",
    "\n",
    "    :param hypothesis: hypothesis string\n",
    "    :type hypothesis: str\n",
    "    :param reference: reference string\n",
    "    :type reference: str\n",
    "    :preprocess: preprocessing method (default str.lower)\n",
    "    :type preprocess: method\n",
    "    :return: enumerated words list\n",
    "    :rtype: list of 2D tuples, list of 2D tuples\n",
    "    \"\"\"\n",
    "    hypothesis_list = list(enumerate(preprocess(hypothesis).split()))\n",
    "    reference_list = list(enumerate(preprocess(reference).split()))\n",
    "    return hypothesis_list, reference_list\n",
    "\n",
    "\n",
    "def exact_match(hypothesis, reference):\n",
    "    \"\"\"\n",
    "    matches exact words in hypothesis and reference\n",
    "    and returns a word mapping based on the enumerated\n",
    "    word id between hypothesis and reference\n",
    "\n",
    "    :param hypothesis: hypothesis string\n",
    "    :type hypothesis: str\n",
    "    :param reference: reference string\n",
    "    :type reference: str\n",
    "    :return: enumerated matched tuples, enumerated unmatched hypothesis tuples,\n",
    "             enumerated unmatched reference tuples\n",
    "    :rtype: list of 2D tuples, list of 2D tuples,  list of 2D tuples\n",
    "    \"\"\"\n",
    "    hypothesis_list, reference_list = _generate_enums(hypothesis, reference)\n",
    "    return _match_enums(hypothesis_list, reference_list)\n",
    "\n",
    "\n",
    "\n",
    "def _match_enums(enum_hypothesis_list, enum_reference_list):\n",
    "    \"\"\"\n",
    "    matches exact words in hypothesis and reference and returns\n",
    "    a word mapping between enum_hypothesis_list and enum_reference_list\n",
    "    based on the enumerated word id.\n",
    "\n",
    "    :param enum_hypothesis_list: enumerated hypothesis list\n",
    "    :type enum_hypothesis_list: list of tuples\n",
    "    :param enum_reference_list: enumerated reference list\n",
    "    :type enum_reference_list: list of 2D tuples\n",
    "    :return: enumerated matched tuples, enumerated unmatched hypothesis tuples,\n",
    "             enumerated unmatched reference tuples\n",
    "    :rtype: list of 2D tuples, list of 2D tuples,  list of 2D tuples\n",
    "    \"\"\"\n",
    "    word_match = []\n",
    "    for i in range(len(enum_hypothesis_list))[::-1]:\n",
    "        for j in range(len(enum_reference_list))[::-1]:\n",
    "            if enum_hypothesis_list[i][1] == enum_reference_list[j][1]:\n",
    "                word_match.append(\n",
    "                    (enum_hypothesis_list[i][0], enum_reference_list[j][0])\n",
    "                )\n",
    "                (enum_hypothesis_list.pop(i)[1], enum_reference_list.pop(j)[1])\n",
    "                break\n",
    "    return word_match, enum_hypothesis_list, enum_reference_list\n",
    "\n",
    "\n",
    "def _enum_stem_match(\n",
    "    enum_hypothesis_list, enum_reference_list, stemmer=PorterStemmer()\n",
    "):\n",
    "    \"\"\"\n",
    "    Stems each word and matches them in hypothesis and reference\n",
    "    and returns a word mapping between enum_hypothesis_list and\n",
    "    enum_reference_list based on the enumerated word id. The function also\n",
    "    returns a enumerated list of unmatched words for hypothesis and reference.\n",
    "\n",
    "    :param enum_hypothesis_list:\n",
    "    :type enum_hypothesis_list:\n",
    "    :param enum_reference_list:\n",
    "    :type enum_reference_list:\n",
    "    :param stemmer: nltk.stem.api.StemmerI object (default PorterStemmer())\n",
    "    :type stemmer: nltk.stem.api.StemmerI or any class that implements a stem method\n",
    "    :return: enumerated matched tuples, enumerated unmatched hypothesis tuples,\n",
    "             enumerated unmatched reference tuples\n",
    "    :rtype: list of 2D tuples, list of 2D tuples,  list of 2D tuples\n",
    "    \"\"\"\n",
    "    stemmed_enum_list1 = [\n",
    "        (word_pair[0], stemmer.stem(word_pair[1])) for word_pair in enum_hypothesis_list\n",
    "    ]\n",
    "\n",
    "    stemmed_enum_list2 = [\n",
    "        (word_pair[0], stemmer.stem(word_pair[1])) for word_pair in enum_reference_list\n",
    "    ]\n",
    "\n",
    "    word_match, enum_unmat_hypo_list, enum_unmat_ref_list = _match_enums(\n",
    "        stemmed_enum_list1, stemmed_enum_list2\n",
    "    )\n",
    "\n",
    "    enum_unmat_hypo_list = (\n",
    "        list(zip(*enum_unmat_hypo_list)) if len(enum_unmat_hypo_list) > 0 else []\n",
    "    )\n",
    "\n",
    "    enum_unmat_ref_list = (\n",
    "        list(zip(*enum_unmat_ref_list)) if len(enum_unmat_ref_list) > 0 else []\n",
    "    )\n",
    "\n",
    "    enum_hypothesis_list = list(\n",
    "        filter(lambda x: x[0] not in enum_unmat_hypo_list, enum_hypothesis_list)\n",
    "    )\n",
    "\n",
    "    enum_reference_list = list(\n",
    "        filter(lambda x: x[0] not in enum_unmat_ref_list, enum_reference_list)\n",
    "    )\n",
    "\n",
    "    return word_match, enum_hypothesis_list, enum_reference_list\n",
    "\n",
    "\n",
    "def stem_match(hypothesis, reference, stemmer=PorterStemmer()):\n",
    "    \"\"\"\n",
    "    Stems each word and matches them in hypothesis and reference\n",
    "    and returns a word mapping between hypothesis and reference\n",
    "\n",
    "    :param hypothesis:\n",
    "    :type hypothesis:\n",
    "    :param reference:\n",
    "    :type reference:\n",
    "    :param stemmer: nltk.stem.api.StemmerI object (default PorterStemmer())\n",
    "    :type stemmer: nltk.stem.api.StemmerI or any class that\n",
    "                   implements a stem method\n",
    "    :return: enumerated matched tuples, enumerated unmatched hypothesis tuples,\n",
    "             enumerated unmatched reference tuples\n",
    "    :rtype: list of 2D tuples, list of 2D tuples,  list of 2D tuples\n",
    "    \"\"\"\n",
    "    enum_hypothesis_list, enum_reference_list = _generate_enums(hypothesis, reference)\n",
    "    return _enum_stem_match(enum_hypothesis_list, enum_reference_list, stemmer=stemmer)\n",
    "\n",
    "\n",
    "\n",
    "def _enum_wordnetsyn_match(enum_hypothesis_list, enum_reference_list, wordnet=wordnet):\n",
    "    \"\"\"\n",
    "    Matches each word in reference to a word in hypothesis\n",
    "    if any synonym of a hypothesis word is the exact match\n",
    "    to the reference word.\n",
    "\n",
    "    :param enum_hypothesis_list: enumerated hypothesis list\n",
    "    :param enum_reference_list: enumerated reference list\n",
    "    :param wordnet: a wordnet corpus reader object (default nltk.corpus.wordnet)\n",
    "    :type wordnet: WordNetCorpusReader\n",
    "    :return: list of matched tuples, unmatched hypothesis list, unmatched reference list\n",
    "    :rtype:  list of tuples, list of tuples, list of tuples\n",
    "\n",
    "    \"\"\"\n",
    "    word_match = []\n",
    "    for i in range(len(enum_hypothesis_list))[::-1]:\n",
    "        hypothesis_syns = set(\n",
    "            chain.from_iterable(\n",
    "                (\n",
    "                    lemma.name()\n",
    "                    for lemma in synset.lemmas()\n",
    "                    if lemma.name().find(\"_\") < 0\n",
    "                )\n",
    "                for synset in wordnet.synsets(enum_hypothesis_list[i][1])\n",
    "            )\n",
    "        ).union({enum_hypothesis_list[i][1]})\n",
    "        for j in range(len(enum_reference_list))[::-1]:\n",
    "            if enum_reference_list[j][1] in hypothesis_syns:\n",
    "                word_match.append(\n",
    "                    (enum_hypothesis_list[i][0], enum_reference_list[j][0])\n",
    "                )\n",
    "                enum_hypothesis_list.pop(i), enum_reference_list.pop(j)\n",
    "                break\n",
    "    return word_match, enum_hypothesis_list, enum_reference_list\n",
    "\n",
    "\n",
    "def wordnetsyn_match(hypothesis, reference, wordnet=wordnet):\n",
    "    \"\"\"\n",
    "    Matches each word in reference to a word in hypothesis if any synonym\n",
    "    of a hypothesis word is the exact match to the reference word.\n",
    "\n",
    "    :param hypothesis: hypothesis string\n",
    "    :param reference: reference string\n",
    "    :param wordnet: a wordnet corpus reader object (default nltk.corpus.wordnet)\n",
    "    :type wordnet: WordNetCorpusReader\n",
    "    :return: list of mapped tuples\n",
    "    :rtype: list of tuples\n",
    "    \"\"\"\n",
    "    enum_hypothesis_list, enum_reference_list = _generate_enums(hypothesis, reference)\n",
    "    return _enum_wordnetsyn_match(\n",
    "        enum_hypothesis_list, enum_reference_list, wordnet=wordnet\n",
    "    )\n",
    "\n",
    "\n",
    "\n",
    "def _enum_allign_words(\n",
    "    enum_hypothesis_list, enum_reference_list, stemmer=PorterStemmer(), wordnet=wordnet\n",
    "):\n",
    "    \"\"\"\n",
    "    Aligns/matches words in the hypothesis to reference by sequentially\n",
    "    applying exact match, stemmed match and wordnet based synonym match.\n",
    "    in case there are multiple matches the match which has the least number\n",
    "    of crossing is chosen. Takes enumerated list as input instead of\n",
    "    string input\n",
    "\n",
    "    :param enum_hypothesis_list: enumerated hypothesis list\n",
    "    :param enum_reference_list: enumerated reference list\n",
    "    :param stemmer: nltk.stem.api.StemmerI object (default PorterStemmer())\n",
    "    :type stemmer: nltk.stem.api.StemmerI or any class that implements a stem method\n",
    "    :param wordnet: a wordnet corpus reader object (default nltk.corpus.wordnet)\n",
    "    :type wordnet: WordNetCorpusReader\n",
    "    :return: sorted list of matched tuples, unmatched hypothesis list,\n",
    "             unmatched reference list\n",
    "    :rtype: list of tuples, list of tuples, list of tuples\n",
    "    \"\"\"\n",
    "    exact_matches, enum_hypothesis_list, enum_reference_list = _match_enums(\n",
    "        enum_hypothesis_list, enum_reference_list\n",
    "    )\n",
    "\n",
    "    stem_matches, enum_hypothesis_list, enum_reference_list = _enum_stem_match(\n",
    "        enum_hypothesis_list, enum_reference_list, stemmer=stemmer\n",
    "    )\n",
    "\n",
    "    wns_matches, enum_hypothesis_list, enum_reference_list = _enum_wordnetsyn_match(\n",
    "        enum_hypothesis_list, enum_reference_list, wordnet=wordnet\n",
    "    )\n",
    "\n",
    "    return (\n",
    "        sorted(\n",
    "            exact_matches + stem_matches + wns_matches, key=lambda wordpair: wordpair[0]\n",
    "        ),\n",
    "        enum_hypothesis_list,\n",
    "        enum_reference_list,\n",
    "    )\n",
    "\n",
    "\n",
    "def allign_words(hypothesis, reference, stemmer=PorterStemmer(), wordnet=wordnet):\n",
    "    \"\"\"\n",
    "    Aligns/matches words in the hypothesis to reference by sequentially\n",
    "    applying exact match, stemmed match and wordnet based synonym match.\n",
    "    In case there are multiple matches the match which has the least number\n",
    "    of crossing is chosen.\n",
    "\n",
    "    :param hypothesis: hypothesis string\n",
    "    :param reference: reference string\n",
    "    :param stemmer: nltk.stem.api.StemmerI object (default PorterStemmer())\n",
    "    :type stemmer: nltk.stem.api.StemmerI or any class that implements a stem method\n",
    "    :param wordnet: a wordnet corpus reader object (default nltk.corpus.wordnet)\n",
    "    :type wordnet: WordNetCorpusReader\n",
    "    :return: sorted list of matched tuples, unmatched hypothesis list, unmatched reference list\n",
    "    :rtype: list of tuples, list of tuples, list of tuples\n",
    "    \"\"\"\n",
    "    enum_hypothesis_list, enum_reference_list = _generate_enums(hypothesis, reference)\n",
    "    return _enum_allign_words(\n",
    "        enum_hypothesis_list, enum_reference_list, stemmer=stemmer, wordnet=wordnet\n",
    "    )\n",
    "\n",
    "\n",
    "\n",
    "def _count_chunks(matches):\n",
    "    \"\"\"\n",
    "    Counts the fewest possible number of chunks such that matched unigrams\n",
    "    of each chunk are adjacent to each other. This is used to caluclate the\n",
    "    fragmentation part of the metric.\n",
    "\n",
    "    :param matches: list containing a mapping of matched words (output of allign_words)\n",
    "    :return: Number of chunks a sentence is divided into post allignment\n",
    "    :rtype: int\n",
    "    \"\"\"\n",
    "    i = 0\n",
    "    chunks = 1\n",
    "    while i < len(matches) - 1:\n",
    "        if (matches[i + 1][0] == matches[i][0] + 1) and (\n",
    "            matches[i + 1][1] == matches[i][1] + 1\n",
    "        ):\n",
    "            i += 1\n",
    "            continue\n",
    "        i += 1\n",
    "        chunks += 1\n",
    "    return chunks\n",
    "\n",
    "\n",
    "def single_meteor_score(\n",
    "    reference,\n",
    "    hypothesis,\n",
    "    preprocess=str.lower,\n",
    "    stemmer=PorterStemmer(),\n",
    "    wordnet=wordnet,\n",
    "    alpha=0.85,\n",
    "    beta=2.35,\n",
    "    gamma=0.45,\n",
    "    w_1=1,\n",
    "    w_2=0.8,\n",
    "    w_3=0.6\n",
    "):\n",
    "    \"\"\"\n",
    "    Calculates METEOR score for single hypothesis and reference as per\n",
    "    \"Meteor: An Automatic Metric for MT Evaluation with HighLevels of\n",
    "    Correlation with Human Judgments\" by Alon Lavie and Abhaya Agarwal,\n",
    "    in Proceedings of ACL.\n",
    "    http://www.cs.cmu.edu/~alavie/METEOR/pdf/Lavie-Agarwal-2007-METEOR.pdf\n",
    "\n",
    "\n",
    "    >>> hypothesis1 = 'It is a guide to action which ensures that the military always obeys the commands of the party'\n",
    "\n",
    "    >>> reference1 = 'It is a guide to action that ensures that the military will forever heed Party commands'\n",
    "\n",
    "\n",
    "    >>> round(single_meteor_score(reference1, hypothesis1),4)\n",
    "    0.7398\n",
    "\n",
    "        If there is no words match during the alignment the method returns the\n",
    "        score as 0. We can safely  return a zero instead of raising a\n",
    "        division by zero error as no match usually implies a bad translation.\n",
    "\n",
    "    >>> round(meteor_score('this is a cat', 'non matching hypothesis'),4)\n",
    "    0.0\n",
    "\n",
    "    :param references: reference sentences\n",
    "    :type references: list(str)\n",
    "    :param hypothesis: a hypothesis sentence\n",
    "    :type hypothesis: str\n",
    "    :param preprocess: preprocessing function (default str.lower)\n",
    "    :type preprocess: method\n",
    "    :param stemmer: nltk.stem.api.StemmerI object (default PorterStemmer())\n",
    "    :type stemmer: nltk.stem.api.StemmerI or any class that implements a stem method\n",
    "    :param wordnet: a wordnet corpus reader object (default nltk.corpus.wordnet)\n",
    "    :type wordnet: WordNetCorpusReader\n",
    "    :param alpha: parameter for controlling relative weights of precision and recall.\n",
    "    :type alpha: float\n",
    "    :param beta: parameter for controlling shape of penalty as a\n",
    "                 function of as a function of fragmentation.\n",
    "    :type beta: float\n",
    "    :param gamma: relative weight assigned to fragmentation penality.\n",
    "    :type gamma: float\n",
    "    :return: The sentence-level METEOR score.\n",
    "    :rtype: float\n",
    "    \"\"\"\n",
    "    enum_hypothesis, enum_reference = _generate_enums(\n",
    "        hypothesis, reference, preprocess=preprocess\n",
    "    )\n",
    "    translation_length = len(enum_hypothesis)\n",
    "    reference_length = len(enum_reference)\n",
    "    matches, _, _ = _enum_allign_words(enum_hypothesis, enum_reference, stemmer=stemmer)\n",
    "    print(matches)\n",
    "    exact_m,_,_ = exact_match(hypothesis, reference)\n",
    "    print(exact_m)\n",
    "    stem_m,_,_ = _enum_stem_match(enum_hypothesis, enum_reference, stemmer=PorterStemmer())\n",
    "    print(stem_m)\n",
    "    syn_m,_,_ = _enum_wordnetsyn_match(enum_hypothesis, enum_reference, wordnet=wordnet)\n",
    "    print(syn_m)\n",
    "    \n",
    "    \n",
    "    \n",
    "    exact_count = len(exact_m)\n",
    "    stem_count = len((set(stem_m).difference(exact_m)))\n",
    "    syn_count = len((set(syn_m).difference(stem_m)))\n",
    "    print(exact_count)\n",
    "    matches_count = len(matches)\n",
    "    print(matches_count)\n",
    "    try:\n",
    "        precision = float(w_1*exact_count + w_2*stem_count + w_3*syn_count) / translation_length\n",
    "        recall = float(w_1*exact_count + w_2*stem_count + w_3*syn_count) / reference_length\n",
    "        #precision = float(matches_count) / translation_length\n",
    "        #recall = float(matches_count) / reference_length\n",
    "        fmean = (precision * recall) / (alpha * precision + (1 - alpha) * recall)\n",
    "        chunk_count = float(_count_chunks(matches))\n",
    "        frag_frac = chunk_count / matches_count\n",
    "    except ZeroDivisionError:\n",
    "        return 0.0\n",
    "    penalty = gamma * frag_frac ** beta\n",
    "    return (1 - penalty) * fmean\n",
    "\n",
    "\n",
    "\n",
    "def meteorn_score_wss(\n",
    "    references,\n",
    "    hypothesis,\n",
    "    preprocess=str.lower,\n",
    "    stemmer=PorterStemmer(),\n",
    "    wordnet=wordnet,\n",
    "    alpha=0.85,\n",
    "    beta=2.35,\n",
    "    gamma=0.45,\n",
    "    w_1=1,\n",
    "    w_2=0.8,\n",
    "    w_3=0.6\n",
    "):\n",
    "    \"\"\"\n",
    "    Calculates METEOR score for hypothesis with multiple references as\n",
    "    described in \"Meteor: An Automatic Metric for MT Evaluation with\n",
    "    HighLevels of Correlation with Human Judgments\" by Alon Lavie and\n",
    "    Abhaya Agarwal, in Proceedings of ACL.\n",
    "    http://www.cs.cmu.edu/~alavie/METEOR/pdf/Lavie-Agarwal-2007-METEOR.pdf\n",
    "\n",
    "\n",
    "    In case of multiple references the best score is chosen. This method\n",
    "    iterates over single_meteor_score and picks the best pair among all\n",
    "    the references for a given hypothesis\n",
    "\n",
    "    >>> hypothesis1 = 'It is a guide to action which ensures that the military always obeys the commands of the party'\n",
    "    >>> hypothesis2 = 'It is to insure the troops forever hearing the activity guidebook that party direct'\n",
    "\n",
    "    >>> reference1 = 'It is a guide to action that ensures that the military will forever heed Party commands'\n",
    "    >>> reference2 = 'It is the guiding principle which guarantees the military forces always being under the command of the Party'\n",
    "    >>> reference3 = 'It is the practical guide for the army always to heed the directions of the party'\n",
    "\n",
    "    >>> round(meteor_score([reference1, reference2, reference3], hypothesis1),4)\n",
    "    0.7398\n",
    "\n",
    "        If there is no words match during the alignment the method returns the\n",
    "        score as 0. We can safely  return a zero instead of raising a\n",
    "        division by zero error as no match usually implies a bad translation.\n",
    "\n",
    "    >>> round(meteor_score(['this is a cat'], 'non matching hypothesis'),4)\n",
    "    0.0\n",
    "\n",
    "    :param references: reference sentences\n",
    "    :type references: list(str)\n",
    "    :param hypothesis: a hypothesis sentence\n",
    "    :type hypothesis: str\n",
    "    :param preprocess: preprocessing function (default str.lower)\n",
    "    :type preprocess: method\n",
    "    :param stemmer: nltk.stem.api.StemmerI object (default PorterStemmer())\n",
    "    :type stemmer: nltk.stem.api.StemmerI or any class that implements a stem method\n",
    "    :param wordnet: a wordnet corpus reader object (default nltk.corpus.wordnet)\n",
    "    :type wordnet: WordNetCorpusReader\n",
    "    :param alpha: parameter for controlling relative weights of precision and recall.\n",
    "    :type alpha: float\n",
    "    :param beta: parameter for controlling shape of penalty as a function\n",
    "                 of as a function of fragmentation.\n",
    "    :type beta: float\n",
    "    :param gamma: relative weight assigned to fragmentation penality.\n",
    "    :type gamma: float\n",
    "    :return: The sentence-level METEOR score.\n",
    "    :rtype: float\n",
    "    \"\"\"\n",
    "    return max(\n",
    "        [\n",
    "            single_meteor_score(\n",
    "                reference,\n",
    "                hypothesis,\n",
    "                preprocess=preprocess,\n",
    "                stemmer=stemmer,\n",
    "                wordnet=wordnet,\n",
    "                alpha=alpha,\n",
    "                beta=beta,\n",
    "                gamma=gamma,\n",
    "            )\n",
    "            for reference in references\n",
    "        ]\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "d1b60f25",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[(0, 0)]\n",
      "[(0, 0)]\n",
      "[]\n",
      "[]\n",
      "1\n",
      "1\n",
      "[(0, 0)]\n",
      "[(0, 0)]\n",
      "[]\n",
      "[]\n",
      "1\n",
      "1\n",
      "[(0, 0), (1, 1), (2, 2)]\n",
      "[(2, 2), (1, 1), (0, 0)]\n",
      "[]\n",
      "[]\n",
      "3\n",
      "3\n",
      "[(0, 0), (1, 1), (2, 2)]\n",
      "[(2, 2), (1, 1), (0, 0)]\n",
      "[]\n",
      "[]\n",
      "3\n",
      "3\n",
      "[(0, 0), (1, 1)]\n",
      "[(1, 1), (0, 0)]\n",
      "[]\n",
      "[]\n",
      "2\n",
      "2\n",
      "[(0, 0), (1, 1)]\n",
      "[(1, 1), (0, 0)]\n",
      "[]\n",
      "[]\n",
      "2\n",
      "2\n",
      "[(0, 0), (1, 1)]\n",
      "[(1, 1), (0, 0)]\n",
      "[]\n",
      "[]\n",
      "2\n",
      "2\n",
      "[(1, 2)]\n",
      "[(1, 2)]\n",
      "[]\n",
      "[]\n",
      "1\n",
      "1\n",
      "[(0, 0), (0, 0), (1, 1)]\n",
      "[(1, 1)]\n",
      "[(0, 0)]\n",
      "[(0, 0)]\n",
      "1\n",
      "3\n",
      "[(0, 0), (1, 1)]\n",
      "[(1, 1), (0, 0)]\n",
      "[]\n",
      "[]\n",
      "2\n",
      "2\n",
      "[(0, 0), (1, 1)]\n",
      "[(1, 1), (0, 0)]\n",
      "[]\n",
      "[]\n",
      "2\n",
      "2\n",
      "[(0, 0), (1, 1), (2, 2)]\n",
      "[(2, 2), (1, 1), (0, 0)]\n",
      "[]\n",
      "[]\n",
      "3\n",
      "3\n",
      "[(0, 0), (1, 1), (2, 2), (3, 3)]\n",
      "[(3, 3), (2, 2), (1, 1), (0, 0)]\n",
      "[]\n",
      "[]\n",
      "4\n",
      "4\n",
      "[(0, 0)]\n",
      "[(0, 0)]\n",
      "[]\n",
      "[]\n",
      "1\n",
      "1\n",
      "[(0, 0), (1, 2)]\n",
      "[(1, 2), (0, 0)]\n",
      "[]\n",
      "[]\n",
      "2\n",
      "2\n",
      "[(0, 0)]\n",
      "[(0, 0)]\n",
      "[]\n",
      "[]\n",
      "1\n",
      "1\n",
      "[(0, 0), (1, 1)]\n",
      "[(1, 1), (0, 0)]\n",
      "[]\n",
      "[]\n",
      "2\n",
      "2\n",
      "[(0, 0), (1, 1), (3, 2)]\n",
      "[(3, 2), (1, 1), (0, 0)]\n",
      "[]\n",
      "[]\n",
      "3\n",
      "3\n",
      "[(0, 0), (1, 1), (3, 2)]\n",
      "[(3, 2), (1, 1), (0, 0)]\n",
      "[]\n",
      "[]\n",
      "3\n",
      "3\n",
      "[(1, 2)]\n",
      "[(1, 2)]\n",
      "[]\n",
      "[]\n",
      "1\n",
      "1\n",
      "[(0, 0), (2, 2), (3, 3)]\n",
      "[(3, 3), (2, 2), (0, 0)]\n",
      "[]\n",
      "[]\n",
      "3\n",
      "3\n",
      "[(0, 0), (1, 1)]\n",
      "[(1, 1), (0, 0)]\n",
      "[]\n",
      "[]\n",
      "2\n",
      "2\n",
      "[(1, 0), (3, 1), (4, 2), (5, 3)]\n",
      "[(5, 3), (4, 2), (3, 1), (1, 0)]\n",
      "[]\n",
      "[]\n",
      "4\n",
      "4\n",
      "[(0, 0), (1, 1), (2, 2)]\n",
      "[(2, 2), (1, 1), (0, 0)]\n",
      "[]\n",
      "[]\n",
      "3\n",
      "3\n",
      "[(0, 0), (1, 3), (3, 2)]\n",
      "[(3, 2), (1, 3), (0, 0)]\n",
      "[]\n",
      "[]\n",
      "3\n",
      "3\n",
      "[(0, 0), (1, 1), (2, 2)]\n",
      "[(2, 2), (1, 1), (0, 0)]\n",
      "[]\n",
      "[]\n",
      "3\n",
      "3\n",
      "[(1, 3)]\n",
      "[(1, 3)]\n",
      "[]\n",
      "[]\n",
      "1\n",
      "1\n",
      "[(0, 0)]\n",
      "[(0, 0)]\n",
      "[]\n",
      "[]\n",
      "1\n",
      "1\n",
      "[(0, 0), (1, 1), (2, 2), (3, 5)]\n",
      "[(3, 5), (2, 2), (1, 1), (0, 0)]\n",
      "[]\n",
      "[]\n",
      "4\n",
      "4\n",
      "[(0, 0), (1, 1), (2, 2), (5, 4), (6, 7)]\n",
      "[(6, 7), (5, 4), (2, 2), (1, 1), (0, 0)]\n",
      "[]\n",
      "[]\n",
      "5\n",
      "5\n",
      "[(0, 2), (1, 4), (2, 5)]\n",
      "[(2, 5), (1, 4), (0, 2)]\n",
      "[]\n",
      "[]\n",
      "3\n",
      "3\n",
      "[(0, 2), (1, 3), (2, 4), (3, 5)]\n",
      "[(3, 5), (2, 4), (1, 3), (0, 2)]\n",
      "[]\n",
      "[]\n",
      "4\n",
      "4\n",
      "[(0, 1)]\n",
      "[(0, 1)]\n",
      "[]\n",
      "[]\n",
      "1\n",
      "1\n",
      "[(0, 0), (1, 1), (2, 5)]\n",
      "[(2, 5), (1, 1), (0, 0)]\n",
      "[]\n",
      "[]\n",
      "3\n",
      "3\n",
      "[(0, 0), (1, 1), (4, 2), (5, 3)]\n",
      "[(5, 3), (4, 2), (1, 1), (0, 0)]\n",
      "[]\n",
      "[]\n",
      "4\n",
      "4\n",
      "[(8, 1)]\n",
      "[(8, 1)]\n",
      "[]\n",
      "[]\n",
      "1\n",
      "1\n",
      "[(0, 0), (2, 2), (3, 3), (4, 4), (5, 5), (6, 6), (7, 8), (8, 1)]\n",
      "[(8, 1), (7, 8), (6, 6), (5, 5), (4, 4), (2, 2), (0, 0)]\n",
      "[(3, 3)]\n",
      "[]\n",
      "7\n",
      "8\n",
      "[(0, 0), (1, 1), (2, 4), (2, 4)]\n",
      "[(1, 1), (0, 0)]\n",
      "[(2, 4)]\n",
      "[(2, 4)]\n",
      "2\n",
      "4\n",
      "[(0, 0), (2, 2), (3, 3), (4, 4), (7, 1)]\n",
      "[(7, 1), (4, 4), (2, 2), (0, 0)]\n",
      "[(3, 3)]\n",
      "[]\n",
      "4\n",
      "5\n",
      "[(0, 0), (3, 4), (4, 5)]\n",
      "[(4, 5), (3, 4), (0, 0)]\n",
      "[]\n",
      "[]\n",
      "3\n",
      "3\n",
      "[(1, 4)]\n",
      "[(1, 4)]\n",
      "[]\n",
      "[]\n",
      "1\n",
      "1\n",
      "[(0, 0), (1, 1), (4, 2)]\n",
      "[(4, 2), (1, 1), (0, 0)]\n",
      "[]\n",
      "[]\n",
      "3\n",
      "3\n",
      "[(0, 0), (1, 1), (2, 2), (3, 3), (4, 4), (5, 5), (6, 6), (7, 7), (8, 8), (9, 9)]\n",
      "[(9, 9), (8, 8), (7, 7), (6, 6), (5, 5), (4, 4), (3, 3), (2, 2), (1, 1), (0, 0)]\n",
      "[]\n",
      "[]\n",
      "10\n",
      "10\n",
      "[(0, 0), (1, 1)]\n",
      "[(1, 1), (0, 0)]\n",
      "[]\n",
      "[]\n",
      "2\n",
      "2\n",
      "[(1, 0), (4, 2)]\n",
      "[(4, 2), (1, 0)]\n",
      "[]\n",
      "[]\n",
      "2\n",
      "2\n",
      "[(0, 0), (1, 1), (2, 2), (3, 5), (4, 9), (5, 3), (8, 10)]\n",
      "[(8, 10), (5, 3), (4, 9), (3, 5), (2, 2), (1, 1), (0, 0)]\n",
      "[]\n",
      "[]\n",
      "7\n",
      "7\n",
      "[(0, 0), (1, 1)]\n",
      "[(1, 1), (0, 0)]\n",
      "[]\n",
      "[]\n",
      "2\n",
      "2\n",
      "[(3, 4)]\n",
      "[(3, 4)]\n",
      "[]\n",
      "[]\n",
      "1\n",
      "1\n",
      "[(6, 1), (7, 2), (8, 3)]\n",
      "[(8, 3), (7, 2), (6, 1)]\n",
      "[]\n",
      "[]\n",
      "3\n",
      "3\n",
      "[(3, 2), (7, 6), (8, 7), (9, 8)]\n",
      "[(9, 8), (8, 7), (7, 6), (3, 2)]\n",
      "[]\n",
      "[]\n",
      "4\n",
      "4\n",
      "[(0, 0), (1, 1), (6, 5)]\n",
      "[(6, 5), (1, 1), (0, 0)]\n",
      "[]\n",
      "[]\n",
      "3\n",
      "3\n",
      "[(0, 0), (1, 1), (2, 2), (4, 3), (5, 4), (6, 5), (7, 6), (8, 7), (9, 8)]\n",
      "[(9, 8), (8, 7), (7, 6), (6, 5), (5, 4), (4, 3), (2, 2), (1, 1), (0, 0)]\n",
      "[]\n",
      "[]\n",
      "9\n",
      "9\n",
      "[(1, 1)]\n",
      "[(1, 1)]\n",
      "[]\n",
      "[]\n",
      "1\n",
      "1\n",
      "[(0, 0)]\n",
      "[(0, 0)]\n",
      "[]\n",
      "[]\n",
      "1\n",
      "1\n",
      "[(4, 1)]\n",
      "[(4, 1)]\n",
      "[]\n",
      "[]\n",
      "1\n",
      "1\n",
      "[(4, 4)]\n",
      "[(4, 4)]\n",
      "[]\n",
      "[]\n",
      "1\n",
      "1\n",
      "[(11, 0), (13, 2), (14, 1), (15, 4), (16, 5), (17, 6), (19, 3)]\n",
      "[(19, 3), (17, 6), (16, 5), (15, 4), (14, 1), (13, 2), (11, 0)]\n",
      "[]\n",
      "[]\n",
      "7\n",
      "7\n",
      "[(0, 0), (4, 3)]\n",
      "[(4, 3), (0, 0)]\n",
      "[]\n",
      "[]\n",
      "2\n",
      "2\n",
      "[(2, 3), (3, 4), (4, 5)]\n",
      "[(4, 5), (3, 4), (2, 3)]\n",
      "[]\n",
      "[]\n",
      "3\n",
      "3\n",
      "[(1, 5), (4, 3), (5, 6)]\n",
      "[(5, 6), (4, 3), (1, 5)]\n",
      "[]\n",
      "[]\n",
      "3\n",
      "3\n",
      "[(0, 0), (1, 6), (1, 6), (2, 2), (8, 1)]\n",
      "[(8, 1), (2, 2), (0, 0)]\n",
      "[(1, 6)]\n",
      "[(1, 6)]\n",
      "3\n",
      "5\n",
      "[(5, 2), (8, 1), (10, 5), (11, 6), (12, 7)]\n",
      "[(12, 7), (11, 6), (10, 5), (8, 1), (5, 2)]\n",
      "[]\n",
      "[]\n",
      "5\n",
      "5\n",
      "[(0, 0), (1, 1), (2, 2), (3, 11), (9, 3)]\n",
      "[(9, 3), (2, 2), (1, 1), (0, 0)]\n",
      "[(3, 11)]\n",
      "[]\n",
      "4\n",
      "5\n",
      "[(0, 0), (1, 1), (2, 2), (3, 5), (4, 10), (6, 8), (7, 9), (8, 16)]\n",
      "[(8, 16), (7, 9), (6, 8), (4, 10), (3, 5), (2, 2), (1, 1), (0, 0)]\n",
      "[]\n",
      "[]\n",
      "8\n",
      "8\n",
      "[(2, 1), (4, 3)]\n",
      "[(4, 3), (2, 1)]\n",
      "[]\n",
      "[]\n",
      "2\n",
      "2\n",
      "[(0, 0), (3, 3)]\n",
      "[(3, 3), (0, 0)]\n",
      "[]\n",
      "[]\n",
      "2\n",
      "2\n",
      "[(0, 0)]\n",
      "[(0, 0)]\n",
      "[]\n",
      "[]\n",
      "1\n",
      "1\n",
      "[(0, 0), (4, 4), (5, 5), (7, 7), (9, 10), (10, 13), (12, 1), (15, 2)]\n",
      "[(15, 2), (12, 1), (10, 13), (9, 10), (7, 7), (5, 5), (4, 4), (0, 0)]\n",
      "[]\n",
      "[]\n",
      "8\n",
      "8\n",
      "[(0, 0), (1, 1), (2, 2), (4, 3), (9, 4), (10, 5)]\n",
      "[(10, 5), (9, 4), (4, 3), (2, 2), (1, 1), (0, 0)]\n",
      "[]\n",
      "[]\n",
      "6\n",
      "6\n",
      "[(2, 0)]\n",
      "[(2, 0)]\n",
      "[]\n",
      "[]\n",
      "1\n",
      "1\n",
      "[(0, 0)]\n",
      "[(0, 0)]\n",
      "[]\n",
      "[]\n",
      "1\n",
      "1\n",
      "[(4, 1)]\n",
      "[(4, 1)]\n",
      "[]\n",
      "[]\n",
      "1\n",
      "1\n",
      "[(4, 2)]\n",
      "[(4, 2)]\n",
      "[]\n",
      "[]\n",
      "1\n",
      "1\n",
      "[(0, 0)]\n",
      "[(0, 0)]\n",
      "[]\n",
      "[]\n",
      "1\n",
      "1\n",
      "[(0, 0), (3, 2)]\n",
      "[(0, 0)]\n",
      "[(3, 2)]\n",
      "[]\n",
      "1\n",
      "2\n",
      "[(1, 0)]\n",
      "[(1, 0)]\n",
      "[]\n",
      "[]\n",
      "1\n",
      "1\n",
      "[(1, 0), (6, 2)]\n",
      "[(6, 2), (1, 0)]\n",
      "[]\n",
      "[]\n",
      "2\n",
      "2\n",
      "[(0, 0), (2, 2), (6, 7), (7, 9), (8, 1)]\n",
      "[(8, 1), (7, 9), (6, 7), (2, 2), (0, 0)]\n",
      "[]\n",
      "[]\n",
      "5\n",
      "5\n",
      "[(0, 0), (4, 2), (6, 3)]\n",
      "[(6, 3), (4, 2)]\n",
      "[]\n",
      "[(0, 0)]\n",
      "2\n",
      "3\n",
      "[(0, 0), (2, 1)]\n",
      "[(2, 1), (0, 0)]\n",
      "[]\n",
      "[]\n",
      "2\n",
      "2\n",
      "[(2, 0), (8, 3), (9, 4), (11, 1), (13, 2)]\n",
      "[(13, 2), (11, 1), (9, 4), (8, 3)]\n",
      "[(2, 0)]\n",
      "[]\n",
      "4\n",
      "5\n",
      "[(0, 0), (2, 3), (5, 7)]\n",
      "[(5, 7), (2, 3), (0, 0)]\n",
      "[]\n",
      "[]\n",
      "3\n",
      "3\n",
      "[(0, 0), (4, 5), (6, 6), (7, 7), (9, 2), (10, 3)]\n",
      "[(10, 3), (9, 2), (7, 7), (6, 6), (4, 5), (0, 0)]\n",
      "[]\n",
      "[]\n",
      "6\n",
      "6\n",
      "[(0, 0), (3, 2), (5, 5)]\n",
      "[(5, 5), (3, 2), (0, 0)]\n",
      "[]\n",
      "[]\n",
      "3\n",
      "3\n",
      "[(3, 0), (4, 10), (5, 2), (11, 1)]\n",
      "[(5, 2), (4, 10), (3, 0)]\n",
      "[(11, 1)]\n",
      "[]\n",
      "3\n",
      "4\n",
      "[(3, 0), (4, 8), (5, 2)]\n",
      "[(5, 2), (4, 8), (3, 0)]\n",
      "[]\n",
      "[]\n",
      "3\n",
      "3\n",
      "[(0, 0), (1, 1), (2, 2)]\n",
      "[(2, 2), (1, 1), (0, 0)]\n",
      "[]\n",
      "[]\n",
      "3\n",
      "3\n",
      "[(0, 0), (1, 1)]\n",
      "[(1, 1), (0, 0)]\n",
      "[]\n",
      "[]\n",
      "2\n",
      "2\n",
      "[(5, 11), (8, 1), (10, 13), (11, 14), (12, 15)]\n",
      "[(12, 15), (11, 14), (10, 13), (8, 1), (5, 11)]\n",
      "[]\n",
      "[]\n",
      "5\n",
      "5\n",
      "[(1, 0), (4, 7)]\n",
      "[(4, 7), (1, 0)]\n",
      "[]\n",
      "[]\n",
      "2\n",
      "2\n",
      "[(3, 5), (7, 8)]\n",
      "[(7, 8), (3, 5)]\n",
      "[]\n",
      "[]\n",
      "2\n",
      "2\n",
      "[(6, 4), (7, 6)]\n",
      "[(7, 6), (6, 4)]\n",
      "[]\n",
      "[]\n",
      "2\n",
      "2\n",
      "[(0, 0), (2, 2), (3, 3)]\n",
      "[(3, 3), (2, 2), (0, 0)]\n",
      "[]\n",
      "[]\n",
      "3\n",
      "3\n",
      "[(1, 1), (3, 0)]\n",
      "[(3, 0), (1, 1)]\n",
      "[]\n",
      "[]\n",
      "2\n",
      "2\n",
      "[(0, 0), (2, 2), (8, 3)]\n",
      "[(8, 3), (2, 2), (0, 0)]\n",
      "[]\n",
      "[]\n",
      "3\n",
      "3\n",
      "[(0, 0), (1, 2), (8, 3)]\n",
      "[(8, 3), (1, 2), (0, 0)]\n",
      "[]\n",
      "[]\n",
      "3\n",
      "3\n",
      "[(0, 0)]\n",
      "[(0, 0)]\n",
      "[]\n",
      "[]\n",
      "1\n",
      "1\n",
      "[(6, 4)]\n",
      "[(6, 4)]\n",
      "[]\n",
      "[]\n",
      "1\n",
      "1\n",
      "[(6, 1)]\n",
      "[(6, 1)]\n",
      "[]\n",
      "[]\n",
      "1\n",
      "1\n",
      "[(1, 1)]\n",
      "[(1, 1)]\n",
      "[]\n",
      "[]\n",
      "1\n",
      "1\n",
      "[0.55, 0.55, 0.97, 0.97, 0.91, 0.91, 0.91, 0.12, 0.74, 0.61, 0.61, 0.72, 0.69, 0.3, 0.39, 0.18, 0.49, 0.75, 0.75, 0.18, 0.62, 0.46, 0.85, 0.5, 0.41, 0.67, 0.14, 0.15, 0.56, 0.55, 0.45, 0.48, 0.17, 0.45, 0.85, 0.12, 0.66, 0.43, 0.66, 0.42, 0.12, 0.72, 1.0, 0.34, 0.33, 0.52, 0.35, 0.11, 0.61, 0.4, 0.4, 0.97, 0.1, 0.17, 0.22, 0.11, 0.62, 0.26, 0.34, 0.24, 0.22, 0.49, 0.35, 0.43, 0.16, 0.16, 0.09, 0.37, 0.34, 0.13, 0.16, 0.13, 0.16, 0.16, 0.17, 0.08, 0.31, 0.22, 0.2, 0.4, 0.27, 0.21, 0.58, 0.23, 0.18, 0.14, 0.23, 0.25, 0.28, 0.14, 0.12, 0.15, 0.27, 0.15, 0.35, 0.35, 0.2, 0.1, 0.2, 0.12]\n"
     ]
    }
   ],
   "source": [
    "meteorn_wss=[]\n",
    "for i in range(len(auth_1)):\n",
    "    meteorn_wss.append(round(meteorn_score_wss([refs[i]],preds[i]),2))\n",
    "print(meteorn_wss)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "7cf9c075",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0.55, 0.55, 0.97, 0.97, 0.91, 0.91, 0.91, 0.12, 0.74, 0.61, 0.61, 0.72, 0.69, 0.3, 0.39, 0.18, 0.49, 0.75, 0.75, 0.18, 0.62, 0.46, 0.85, 0.5, 0.41, 0.67, 0.14, 0.15, 0.56, 0.55, 0.45, 0.48, 0.17, 0.45, 0.85, 0.12, 0.66, 0.43, 0.66, 0.42, 0.12, 0.72, 1.0, 0.34, 0.33, 0.52, 0.35, 0.11, 0.61, 0.4, 0.4, 0.97, 0.1, 0.17, 0.22, 0.11, 0.62, 0.26, 0.34, 0.24, 0.22, 0.49, 0.35, 0.43, 0.16, 0.16, 0.09, 0.37, 0.34, 0.13, 0.16, 0.13, 0.16, 0.16, 0.17, 0.08, 0.31, 0.22, 0.2, 0.4, 0.27, 0.21, 0.58, 0.23, 0.18, 0.14, 0.23, 0.25, 0.28, 0.14, 0.12, 0.15, 0.27, 0.15, 0.35, 0.35, 0.2, 0.1, 0.2, 0.12]\n"
     ]
    }
   ],
   "source": [
    "norm_mn_wss = []\n",
    "\n",
    "max_mn_wss=max(meteorn_wss)\n",
    "min_mn_wss=min(meteorn_wss)\n",
    "\n",
    "for a in range(len(meteorn_wss)):\n",
    "    norm_mn_wss.append(round(((meteorn_wss[a])/(max_mn_wss)),2))\n",
    "print(norm_mn_wss)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "9f6598c5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Spearmans correlation coefficient: 0.761\n",
      "correlated (reject H0) p=0.000\n"
     ]
    }
   ],
   "source": [
    "from scipy.stats import spearmanr\n",
    "# calculate spearman's correlation\n",
    "coef, p = spearmanr(norm_mn_wss, avg_norm_score)\n",
    "print('Spearmans correlation coefficient: %.3f' % coef)\n",
    "# interpret the significance\n",
    "alpha = 0.05\n",
    "if p > alpha:\n",
    "\tprint('uncorrelated (fail to reject H0) p=%.3f' % p)\n",
    "else:\n",
    "\tprint('correlated (reject H0) p=%.3f' % p)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
